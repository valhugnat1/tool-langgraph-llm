{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader # Importing PDF loader from Langchain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # Importing text splitter from Langchain\n",
    "from langchain.embeddings import OpenAIEmbeddings # Importing OpenAI embeddings from Langchain\n",
    "from langchain.schema import Document # Importing Document schema from Langchain\n",
    "from langchain.vectorstores.chroma import Chroma # Importing Chroma vector store from Langchain\n",
    "from dotenv import load_dotenv # Importing dotenv to get API key from .env file\n",
    "from langchain.chat_models import ChatOpenAI # Import OpenAI LLM\n",
    "import os # Importing os module for operating system functionalities\n",
    "import shutil # Importing shutil module for high-level file operations\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Path to the directory to save Chroma database\n",
    "CHROMA_PATH = \"chroma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'articles/doc_craft.txt'}, page_content='ntroduction\\nYou are a Data Scientist struggling with data, code, and models in your projects.\\n\\nYou are an ML Engineer who has trouble replicating pipelines and monitoring models in production.\\n\\nYou are Head of a Data Science team, who has trouble shipping quickly models to production.\\n\\nYou are a CTO who has difficulty to move Python code into large-scale production and manage the DevOps workload on an AI project.\\n\\nðŸš€ Then, you are a potential user of Craft AI\\'s MLOps platform, which aims to accelerate the deployment and improve the management of your Machine Learning models in production.\\n\\nFind all the information you need\\n Get started\\n\\nLearn the basics and launch your first script on the platform.\\n\\n Getting started\\n\\n User Workflow\\n\\nList of user workflows for creating deployments, using data, customising your code, etc.\\n\\n Introduction user worklflow\\n\\n Step & Pipeline\\n\\nUnderstand the ML process and manage your steps & pipelines.\\n\\n Discover the concept\\n\\n Deployment\\n\\nDeploy models seamlessly with our efficient deployment tools.\\n\\n Discover the concept\\n\\n Environment\\n\\nEasily configure and optimise your ML environment for peak performance.\\n\\n Discover the concept\\n\\n Platform\\n\\nAccess our powerful AI platform and unleash its capabilities.\\n\\n Access to the platform\\n\\nWhat is Craft AI?\\nCraft AI is an MLOps platform for data science teams.\\n\\nTo use Craft AI, the basic workflow with the platform is:\\n\\nChoose a configured environment on the cloud provider of your choice\\nCreate Machine Learning pipelines with your Python code\\nDeploy and execute the pipelines on environments running on Kubernetes\\nMonitor the performance of the models in production and the health of the infrastructure\\nThe platform aims to be an end-to-end MLOps tool that brings together all the MLOps functionalities required for the successful implementation of an AI project. It is therefore composed of the following main features:\\n\\nEnvironments\\nMachine Learning Pipelines\\nDeployments\\nplatform_info\\n\\nMission\\nOur mission is to democratize the use of trustworthy Artificial Intelligence on a day-to-day basis. How do we do this? By empowering Data Science teams to master their AI project from start to finish. Our vision of AI is that every project of Data Science should be in production, responsible and profitable!\\n\\nTo achieve this, we have developed a MLOps platform that allows anyone to put Python code into production, on a large scale, in a few clicks. We allow Data Scientist to deploy their models, choose their environments (development and production) and create pipelines to optimize their ML workflows.\\n\\nHow does it work? We will contain your code in step to allow you to create ready to production ML and DL pipelines in an environment adapted to each projectâ€™s needs.\\n\\nHistory\\nCraft AI is a French company founded in 2015. We originally developed AI solutions for the energy, industry, healthcare, education and retail sectors. Our unique ability to deploy thousands of ML models at large scale with a focus on being always explainable, energy frugal and fair, drove us to develop a MLOps platform as a front end to our expertise. Today, with our MLOps platform, we can share our expertise in large-scale model deployment, at large scale.\\n\\nWhat is MLOps?\\nMachine Learning Operations (MLOps), aims to provide an end-to-end development process to design, build and manage reproducible, testable, and evolvable ML-powered software.\\n\\nBeing an emerging field, MLOps is rapidly gaining momentum amongst Data Scientists, ML Engineers and AI enthusiasts. Following this trend, MLOps differentiates the ML models management from traditional software engineering like DevOps and suggests the following MLOps capabilities:\\n\\nMLOps aims to unify the release/production cycle for ML and software application release.\\nMLOps enables automated testing of machine learning artifacts (e.g. data validation, ML model testing, and ML model integration testing)ML\\nMLOps enables the application of agile principles to machine learning projects.\\nMLOps enables supporting machine learning models and datasets to build these models as first-class citizens within CI/CD systems.\\nMLOps reduces technical debt across machine learning models.\\nMLOps must be a language-, framework-, platform-, and infrastructure-agnostic practice.\\nBenefits of using Craft AI\\nThe main benefits of the platform for the users are:\\n\\nNo longer taking 6 months to deploy ML models in production but only a few clicks!\\nAllowing a Data Science team to be autonomous on AI in production without DevOps skills.\\nEnabling large scale production of Python code without refactoring to Java or C.\\nAutomating the execution of the pipelines to save time for Data Science teams.\\nEnsuring an efficient use of computing resources and reduce the cloud bill.\\nImproving model performance over time by automatically triggering re-training pipelines when performance drops.\\n\\n\\nAdministration\\nThe MLOps - Craft AI Platform is composed of a graphical interface allowing you to visualize, create, manage and monitor the objects necessary for the realization of your AI projects.\\n\\nIt is composed of:\\n\\nHomepage: Visualize and create projects.\\nParameters: Manage the companyâ€™s account and users.\\nProject settings: Manage the configuration of a project.\\nEnvironments: See the environment(s) and their information within a project.\\nExecutions: See the execution(s) and their information within an environment or a deployment.\\nNote: We strongly recommend Google Chrome browser to use the graphical interface of the platform.\\n\\nSummary:\\n\\nUsers\\nProjects\\nToken SDK\\nUsers\\nSummary:\\n\\nManage user\\nLogin\\nGet user with ID\\nManage user\\nThe management of user is available by email for the moment. In a next version, it will be possible to add, edit and delete a user directly on the platform UI.\\n\\nAdd a user\\nSend a message to Craft AI with your request and the following information:\\n\\nFirst and last name of the user\\nEmail of the user\\nNote\\n\\nAdding users will arrive later on the platform.\\n\\nAccess rights\\nEach user has access to one or more defined projects.\\n\\nEach user who has access to a project has access to all the information and actions in it.\\n\\nNote\\n\\nAdvanced access rights will arrive later on the platform.\\n\\nDelete a user\\nSend a message to Craft AI with your request and the following information:\\n\\nFirst and last name of the user\\nEmail of the user\\nNote\\n\\nThe deletion of users will arrive later on the platform.\\n\\nLogin\\nHere is the URL to access the platform: https://mlops-platform.craft.ai\\n\\nFirst connection\\nPrerequisites: The user must be added to the platform (Add a user).\\n\\nThe user connects to the platform: https://mlops-platform.craft.ai The connection will not work, but Craft AI will receive the request and can add the user.\\nThe user receives an email/slack from Craft AI to inform him that he can log in.\\nThe user logs in with the same link and has access to the platform.\\nForgot your password\\nIn the Login popup, click on Donâ€™t remember your password?\\nEnter your email, you will receive an email to modify your password.\\nGet user with ID\\nFunction definition\\nWhile using the SDK you may encounter outputs parameters containing a user when using specific functions. Each user is identified with a unique ID. That is the case for example in the sdk.get_pipeline_execution() function output with the parameter created_by. To match the user ID with the corresponding information (name and email) you can use the get_user() function.\\n\\nCraftAiSdk.get_user(user_id)\\nParameters\\nuser_id (str) â€“ The ID of the user.\\nReturns\\nThe user information in dict type, with the following keys:\\n\\nid (str): ID of the user.\\nname (str): Name of the user.\\nemail (str): Email of the user.\\nProjects\\nA project is a complete use case. From data import, through experimentation, testing and production, to performance monitoring over time.\\n\\nUsers can create as many projects as they want. Each project is completely isolated from the others.\\n\\nTo work on a project, it must include at least one environment. The user can create several environments in a project.\\n\\nSummary:\\n\\nCreate a project\\nManage a project\\nCreate a project\\nFrom the homepage, you can create a new project by clicking on the â€œNew projectâ€ button.\\n\\nA project creation page opens in which you have to fill in the fields :\\n\\n[Mandatory]Project name : Enter the name of your project (in lowercase and with â€œ-â€), you will not be able to modify it later.\\nThe following properties are default settings for the pipelines that will be created in this project, so you won\\'t need to configure them each time you create a pipeline. It will always be possible to override these settings separately for each pipeline.\\n\\n[Mandatory]Python version : Select the version of Python that will be applied to this project. It will be possible to choose a different version when creating each pipeline.\\n\\nRepository URL : The SSH URL of a Git repository that contains code for your pipelines, starting with â€œgit@â€. When creating pipelines, it will also be possible to send code directly to the platform or to choose a different Git repository URL.\\n\\nDeploy key : GitHub / GitLab private key to access a Git repository that contains code for your pipelines.\\n\\nDefault branch : The Git branch where to get pipeline code by default for this project. If this field is empty but a Git repository is used to get pipeline code, the platform will use the default Git branch. It will be possible to choose a different default branch within an environment.\\n\\n[Mandatory]Folders : File(s) or folder(s) that will be included in pipelines, among the ones fetched from Git by the platform or sent by the user directly. By default, it will contain the value /, which means all the files in the folder or repository are included. It will be possible to choose different files or folders within an environment.\\n\\nRequirements.txt : The path to the requirements.txt file with the Python libraries to install automatically on this project, among the pipeline files included in the folders defined above. It will be possible to choose a different file within an environment.\\n\\nSystem dependencies : List of APT packages that you want to install automatically on the Linux system where pipelines will run in this project. It will be possible to add different packages within an environment.\\n\\nNote\\n\\nFor more information on how to link pipelines with a Git repository, the full procedure is available here.\\n\\nClick on â€œCreate projectâ€, you will see the project card displayed, and you can enter in it to create a first environment.\\n\\nThere are no access rights at first, all users of the platform have access to all projects.\\n\\nadministration_3\\n\\nNote: You will be able to modify these elements (except the project name) in the Settings section of your project.\\n\\nNext step: Create an environment in this project.\\n\\nManage a project\\nEdit a project\\nWithin your project, click on Settings to view and edit your project information.\\n\\nadministration_4\\n\\nOn this page, you will find the information you set up when you created the project.\\n\\nYou can change them, except the name of the project.\\n\\nWarning\\n\\nDonâ€™t forget to save and validate the confirmation slider to make the changes effective.\\n\\nThe changes will apply to steps created after this modification. These changes may affect your steps and can make them non-functional.\\n\\nadministration_5\\n\\nUsers in a project\\nInitially, all users of the platform have access to all projects without special access rights.\\n\\nNote\\n\\nThe access rights per user within a project will arrive later on the platform.\\n\\nDelete a project\\nInitially, you cannot delete a project.\\n\\nSubmit an email request to delete a project from the platform.\\n\\nNote\\n\\nThe deletion of a project will arrive later on the platform.\\n\\nAccess the SDK\\nThe Craft AI MLOps Platform is composed of a Python SDK allowing you to use, from your IDE, the functions developed by Craft AI to create your steps, pipelines and deployments.\\n\\nSummary:\\n\\nGet token access\\nConnect to the SDK\\nGet token access\\nIn the header, click on your name and go to the â€œParametersâ€ page.\\n\\nadministration_6\\n\\nAt the bottom of the â€œAccountâ€ page, you will find your SDK token, which will allow you to identify yourself to use the SDK.\\n\\nYou can regenerate it, the old token will be obsolete, and you will have to identify yourself again to the SDK to access it.\\n\\nWarning\\n\\nThe SDK token is strictly personal. You must keep it to yourself.\\n\\nadministration_7\\n\\nConnect to the SDK\\nYou must install the SDK from a Python terminal, with the command :\\n\\npip install craft-ai-sdk\\nRun the following commands in a Python terminal to initialize the SDK.\\n\\nEnter the CRAFT_AI_ENVIRONMENT_URL, you can find on Get environment URL.\\nIt should look like https://brisk-hawk-charcoal-zeta-42.mlops-platform.craft.ai.\\n\\nIf you donâ€™t have any environments in your project yet, you should Create an environment.\\n\\nEnter your personal CRAFT_AI_SDK_TOKEN, you can find it in \"Parameters\" on the platform web UI.\\nexport CRAFT_AI_ENVIRONMENT_URL=\"*your-env-URL*\"\\nexport CRAFT_AI_SDK_TOKEN=\"*your-token-acces*\"\\nExecute the following Python code to set up SDK Python object.\\nThe SDK will automatically take into account your environment variables for the installation of the connection to the platform.\\n\\nimport os\\nfrom craft_ai_sdk import CraftAiSdk\\n\\nsdk = CraftAiSdk()\\nYou can also specify it directly in the constructor, although this method is not recommended.\\n\\nimport os\\nfrom craft_ai_sdk import CraftAiSdk\\n\\nsdk = CraftAiSdk(\\n    environment_url=\"*your-env-URL*\",\\n    sdk_token=\"*your-token-acces*\"\\n)\\n\\n\\n\\nGet Started\\nThe goal of this use case is to accompany you in understanding the mechanics of the platform with a simple and basic machine learning application.\\n\\nThe final goal of this get started will be to be able to generate predictions on the Craft AI platform. The model used to generate predictions will be the iris one, but you can use any Python code.\\n\\nTo achieve this goal, we will go through several parts:\\n\\nPart 0 : Setup\\nThis page is about the setup of the platform in your Python code and in the Craft AI platform UI.\\n\\nThere are 2 ways to access the platform:\\n\\nWith the Python SDK, in line of code\\nWith the web interface, in a browser\\nPrerequisites\\n\\nPython 3.8 or higher is required to be installed on your computer.\\nWe strongly recommend the use of Google Chrome browser to use the UI of the platform.\\nsetup3\\n\\nWe already have:\\n\\nA project\\nAn environment setup in the project with datastore and workers\\nSet up the Python SDK\\nThe Python SDK can be installed with pip from a terminal. And, for this use case, you also need NumPy.\\n\\npip install craft-ai-sdk numpy\\nInitialization of Python SDK\\nCopy and paste the following commands into a Python terminal or script to initialize the SDK:\\n\\nfrom craft_ai_sdk import CraftAiSdk\\n\\nsdk = CraftAiSdk(\\n    environment_url=\"MY_ENVIRONMENT_URL\",\\n    sdk_token=\"MY_ACCESS_TOKEN\"\\n)\\nSet the value of environment_url to your environment URL instead of MY_ENVIRONMENT_URL. You can find this URL on the Environments page in the UI. The Environment URL is NOT the address of the page in your browser.\\n\\nSetup Step 6\\n\\nSet the value of sdk_token to your access token instead of MY_ACCESS_TOKEN. On the UI, click on your name in the top right corner, then go to the â€œParametersâ€ page. There, you can generate and find your SDK token at the bottom. The SDK token is strictly personal. You must keep it confidential.\\n\\nSetup Step 5\\n\\nFinally, run your code.\\n\\n[Recommended] Setup your credential with a .env file\\nSuccess\\n\\nðŸŽ‰ Well done! Youâ€™re ready to execute your first code on the platform!\\n\\nWhat\\'s next ?\\nNow that we have configured the platform, we can create our first objects and run a â€œHello Worldâ€ on the platform.\\n\\nNext step: Part 1: Deploy a simple pipeline\\n\\nPart 1: Execute a simple pipeline\\nThe main goal of the Craft AI platform is to allow to deploy easily your machine learning pipelines.\\n\\nIn this part we will use the platform to build a simple â€œhello worldâ€ application by showing you how to execute a basic Python code that prints â€œHello worldâ€ and displays the number of days until 2025.\\n\\nYou will learn how to:\\n\\nPackage your application code into a step on the platform\\nEmbed it in a pipeline\\nExecute it on the platform\\nCheck the logs of the executions on the web interface\\nstep1_00\\n\\nCreate a step with the SDK\\nThe first thing to do to build an application on the Craft AI platform is to create a step.\\n\\nA Step is the equivalent of a Python function in the Craft AI platform. Like a regular function, a step is defined by the inputs it ingests, the code it runs, and the outputs it returns. For this â€œhello worldâ€ use case, we are focusing on the code part so we will ignore inputs and outputs for now.\\n\\nA step can be created from any Python function, using the create_step() method of thesdk object.\\n\\nAll of the code in this example can also be found on GitHub here.\\n\\nFor this example, we will use the following code:\\n\\nimport datetime\\n\\ndef helloWorld() -> None:\\n\\n    # Count the number of days between January 1, 2000, and today\\n    start_date = datetime.datetime(2000, 1, 1)\\n    now = datetime.datetime.now()\\n\\n    difference = now - start_date\\n\\n    print(f\\'Hello world! Number of days since January 1, 2000: {difference.days}\\')\\nCreate a file with the content above named src/part-1-helloWorld.py in a new folder that will contain the step\\'s files. So, the helloWorld function is located in src/part-1-helloWorld.py.\\nWe can now create the step by running the following command in a Python terminal:\\n\\nsdk.create_step(\\n    step_name=\\'part-1-hello-world\\',\\n    function_path=\\'src/part-1-helloWorld.py\\',\\n    function_name=\\'helloWorld\\',\\n    container_config={\\n        \"local_folder\": \".../get_started\", # Enter the path to your local folder here, the one that contains `src/part-1-helloWorld.py`\\n    }\\n)\\nIts main arguments are:\\n\\nThe step_name is the name of the step that will be created. This is the identifier you will use later to refer to this step.\\nThe function_path argument is the path of the Python module containing the function that you want to execute for this step. This path must be relative to the local_folder specified in the container_config.\\nThe function_name argument is the name of the function that you want to execute for this step.\\nThe container_config is the configuration of the container that will be used to execute the function.\\nNote\\n\\nOne of the container_config parameters is the local_folder parameter, which is the path to the folder we want to retrieve, containing the function to execute. We will explain in a later part how to do this differently, but for now, we focus on deploying steps from local code.\\n\\nThe above code should give you the following output:\\n\\n>>> Please wait while step is being created. This may take a while...\\n>>> Steps creation succeeded\\n>>> {\\'name\\': \\'part-1-hello-world\\'}\\nYou can view the list of steps that you created in the platform with the list_steps() function of the SDK.\\n\\nstep_list = sdk.list_steps()\\nprint(step_list)\\n>>> [{\\'step_name\\': \\'part-1-hello-world\\',\\n>>>   \\'created_at\\': \\'xxxx-xx-xxTxx:xx:xx.xxxZ\\',\\n>>>   \\'updated_at\\': \\'xxxx-xx-xxTxx:xx:xx.xxxZ\\',\\n>>>   \\'status\\': \\'Ready\\',\\n>>>   \\'origin\\': \\'local\\'}]\\nYou can see your step and its status of creation at Ready.\\n\\nYou can also get the information of a specific step with the get_step() function of the SDK.\\n\\nstep_info = sdk.get_step(\\'part-1-hello-world\\')\\nprint(step_info)\\n>>> {\\n>>>   \\'parameters\\': {\\n>>>     \\'step_name\\': \\'part-1-hello-world\\',\\n>>>     \\'function_path\\': \\'src/part-1-helloWorld.py\\',\\n>>>     \\'function_name\\': \\'helloWorld\\',\\n>>>     \\'description\\': None,\\n>>>     \\'container_config\\': {\\n>>>       \\'language\\': \\'python:3.X-slim\\',\\n>>>       \\'requirements_path\\': \\'requirements.txt\\',\\n>>>       \\'dockerfile_path\\': None\\n>>>     },\\n>>>     \\'inputs\\': [],\\n>>>     \\'outputs\\': []\\n>>>   },\\n>>>   \\'creation_info\\': {\\n>>>     \\'created_at\\': \\'xxxx-xx-xxTxx:xx:xx.xxxZ\\',\\n>>>     \\'updated_at\\': \\'xxxx-xx-xxTxx:xx:xx.xxxZ\\',\\n>>>     \\'created_by\\': \\'xxxxxxxx-xxxx-xxxx-xxxxx-xxxxxxxxxxx\\',\\n>>>     \\'updated_by\\': \\'xxxxxxxx-xxxx-xxxx-xxxxx-xxxxxxxxxxx\\',\\n>>>     \\'status\\': \\'Ready\\'\\n>>>     \\'origin\\': \\'local\\',\\n>>>   }\\n>>> }\\nSuccess\\n\\nðŸŽ‰ Now your step has been created. You can now create your Pipeline (and after that, youâ€™ll execute it on the platform).\\n\\nCreate a pipeline with the SDK\\nstep1_2\\n\\nThe step part-1-hello-world containing our helloWorld code is now created in the platform and ready to be used in a pipeline that we will then execute.\\n\\nA pipeline is a machine learning workflow, consisting of one or more steps, that can be easily deployed on the Craft AI platform. This way, you can create a full pipeline formed with a directed acyclic graph (DAG) by specifying the output of one step as the input of another step.\\n\\nIn the future, it will be possible to assemble multiple steps into a complex machine learning pipeline. For now, the platform only allows single step pipelines.\\n\\nTo create a pipeline consisting of the previous step, you must use the create_pipeline() function of the SDK.\\n\\nsdk.create_pipeline(\\n    pipeline_name=\\'part-1-hello-world\\',\\n    step_name=\\'part-1-hello-world\\',\\n)\\nThis function has two arguments:\\n\\nThe pipeline_name is the name of the pipeline you have just created. As for the step_name you will then refer to the pipeline using this name\\nThe step_name is the name of the step used in the pipeline.\\nAfter executing this function, you should see the following output :\\n\\n>>> Pipeline creation succeeded\\n>>> {\\'pipeline_name\\': \\'part-1-hello-world\\',\\n>>> \\'created_at\\': \\'xxxx-xx-xxTxx:xx:xx.xxxZ\\',\\n>>> \\'steps\\': [\\'part-1-hello-world\\'],\\n>>> \\'open_inputs\\': [],\\n>>> \\'open_outputs\\': []}\\nSuccess\\n\\nðŸŽ‰ Now that our pipeline is created (around our step), we want to execute it. To do this, we will run the pipeline with the sdk function, run_pipeline(), and it will execute the code contained in the step.\\n\\nExecute your pipeline (run)\\nYou can execute a pipeline on the platform directly with the run_pipeline() function.\\n\\nThis function has two arguments:\\n\\nThe name of the existing pipeline to execute (pipeline_name)\\nOptional (only if you have inputs): a dict of inputs to pass to the pipeline with input names as dict keys and corresponding values as dict values.\\nsdk.run_pipeline(pipeline_name=\\'part-1-hello-world\\')\\n>>> The pipeline execution may take a while, you can check its status and get information on the Executions page of the front-end.\\n>>> Its execution ID is \\'part-1-hello-world-xxxxx\\'.\\n>>> Pipeline execution results retrieval succeeded\\n>>> Pipeline execution startup succeeded\\nSuccess\\n\\nðŸŽ‰ Now, you have created a step for the helloWorld function, included it in a pipeline and execute it on the platform! Our hello world application is built and ready to be executed again!\\n\\nGet information about an execution\\nNow, we have executed the pipeline. The return of the function allows us to see that the pipeline has been successfully executed; however, it does not provide the logs of the execution (we can receive outputs with the return of the run pipeline, but we did not put any here).\\n\\nTo find the list of executions along with the information and associated logs, you can use the user interface as follows:\\n\\nConnect to https://mlops-platform.craft.ai\\n\\nClick on your project:\\n\\nstep1_3\\n\\nClick on the Execution page and on â€œSelect an executionâ€: this displays the list of environments:\\n\\nstep1_4\\n\\nSelect your environment to get the list of runs and deployments:\\n\\nstep1_6\\n\\nFinally, click on a run name to get its executions:\\n\\nstep1_7\\n\\nYou have the â€œGeneralâ€ tab to get general information about your execution and the â€œLogsâ€ tab where you can see and download the execution logs:\\n\\nstep1_8\\n\\nUsing the SDK\\nSuccess\\n\\nðŸŽ‰ You can now get your execution\\'s logs.\\n\\nWhat we have learned\\nIn this part we learned how to easily build, deploy and use a simple application with the Craft AI platform with the following workflow:\\n\\nstep1_9\\n\\nThese 3 main steps are the fundamental workflow to work with the platform and we will see them over and over throughout this tutorial.\\n\\nNow that we know how to run our code on the platform, it is time to create more complex steps to have a real ML use case.\\n\\nNext step : Part 2: Execute a simple ML model\\n\\n\\nPart 2: Execute a simple ML model\\nThe previous part showed the main concepts of the platform and how to use the basics of the SDK. With what you already know you are able to execute really simple pipelines. But in order to build more realistic applications, using more complex code on your data with dependencies such as Python libraries, it is needed to learn more advanced functionnalities and especially how to configure the execution context of a step and how to retrieve data stored on the platform.\\n\\nThis page will present the same commands as the previous ones going through more available functionalities offered by the platform, with a real Machine Learning use case. We will improve this Machine Learning application later in Part 3 and Part 4.\\n\\nYou can find all the code used in this part and its structure here.\\n\\nPrerequisites\\n\\nPython 3.8 or higher is required to be installed on your computer.\\nHave done the Part 1: Execute a simple pipeline.\\nHave scikit-learn, numPy and pandas libraries installed. If not, use these commands in your terminal:\\npip install scikit-learn\\npip install pandas\\npip install numpy\\nOverview of the use case\\nWe will build a pipeline to train and store a simple ML model with the iris dataset. The iris dataset describes four features (petal length, petal width, sepal length, sepal width) from three different types of irises (Setosa, Veriscolour, Virginica).\\n\\nstep2_0\\n\\nThe goal of our application is to classify flower type based on the previous four features. In this part we will start our new use case by retrieving the iris dataset from the Data Store (that we will introduce just below), building a pipeline to train a simple ML model on the dataset and store it on the Data Store.\\n\\nstep2_1\\n\\nStoring data on the platform\\nThe Data Store is a file storage on which you can upload and download unlimited files and organize them as you want using the SDK. All your steps can download and upload files from and to the Data Store.\\n\\nPushing the iris dataset to the Data Store: In our case the first thing we want to do is to upload the iris dataset to the Data Store. You can do so with the upload_data_store_object function from the SDK like so:\\n\\nfrom io import BytesIO\\nfrom sklearn import datasets\\nimport pandas as pd\\n\\niris = datasets.load_iris(as_frame=True)\\niris_df = pd.concat([iris.data, iris.target], axis=1)\\n\\nfile_buffer = BytesIO(iris_df.to_parquet())\\nsdk.upload_data_store_object(\\n   filepath_or_buffer=file_buffer,\\n   object_path_in_datastore=\"get_started/dataset/iris.parquet\"\\n)\\nThe argument filepath_or_buffer can be a string or a file-like object. If a string, it is the path to the file to be uploaded, if a file-like object you have to pass an IO object (something you don\\'t write to the disk but stay in the memory). Here we choose to use a BytesIO object.\\n\\nSource code for model training\\nWe will use the following code that trains a sklearn KNN classifier on the iris dataset from the Data Store and put the trained model on the Data Store.\\n\\nimport joblib\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom craft_ai_sdk import CraftAiSdk\\n\\ndef trainIris():\\n\\n   sdk = CraftAiSdk()\\n\\n   sdk.download_data_store_object(\\n      object_path_in_datastore=\"get_started/dataset/iris.parquet\",\\n      filepath_or_buffer=\"iris.parquet\",\\n   )\\n   dataset_df = pd.read_parquet(\"iris.parquet\")\\n\\n   X = dataset_df.loc[:, dataset_df.columns != \"target\"].values\\n   y = dataset_df.loc[:, \"target\"].values\\n\\n   np.random.seed(0)\\n   indices = np.random.permutation(len(X))\\n\\n   n_train_samples = int(0.8 * len(X))\\n   train_indices = indices[:n_train_samples]\\n   val_indices = indices[n_train_samples:]\\n\\n   X_train = X[train_indices]\\n   y_train = y[train_indices]\\n   X_val = X[val_indices]\\n   y_val = y[val_indices]\\n\\n   knn = KNeighborsClassifier()\\n   knn.fit(X_train, y_train)\\n\\n   mean_accuracy = knn.score(X_val, y_val)\\n   print(\"Mean accuracy:\", mean_accuracy)\\n\\n   joblib.dump(knn, \"iris_knn_model.joblib\")\\n\\n   sdk.upload_data_store_object(\\n      \"iris_knn_model.joblib\", \"get_started/models/iris_knn_model.joblib\"\\n   )\\nDelete objects\\nBefore we really start building our new use case, we might want to clean the platform from the objects we created in Part 1. To do this, we need to use the functions associated with each object, here the pipeline and the step.\\n\\nWarning\\n\\nThese objects have dependencies on each other, we have to delete them in a certain order. First the pipeline, then the step.\\n\\nsdk.delete_pipeline(pipeline_name=\"part-1-hello-world\")\\n\\nsdk.delete_step(step_name=\"part-1-hello-world\")\\nAdvanced step configuration\\nIn the rest of this part we will follow the same workflow as in the previous one:\\n\\nstep2_2\\n\\nNow it is time to use the create_step() method of craft-ai-sdk object to create step like before. This time, we will define a bit more the step and its execution context. We are going to focus on two parameters.\\n\\nPython libraries\\nAs you might have noticed, the code above uses external Python libraries (craft_ai_sdk, joblib, numpy, pandas and scikit learn). In the previous step we built an application that didnâ€™t require any external dependency. This time if we want this code to work on the platform we have to inform it that this step requires some Python libraries to run properly.\\n\\nTo do so we create a requirements.txt file, containing the list of Python libraries used in our step function:\\n\\ncraft_ai_sdk==xx.xx.xx\\njoblib==xx.xx.xx\\nnumpy==xx.xx.xx\\nscikit_learn==xx.xx.xx\\npandas==xx.xx.xx\\npyarrow==xx.xx.xx\\nTip\\n\\nExample with up-to-date version numbers available here.\\n\\nIn this case, we place it at the root of the folder with your code, but you can put it somewhere else.\\n\\nYou can set the default path for this file in the Libraries & Packages section of your project settings using the web interface. All steps created in this project will then use this path by default.\\n\\nSuccess\\n\\nðŸŽ‰ Now all the steps created in this project will have the relevant libraries installed\\n\\nCreate a step\\nIn the Project settings, you can see parameters that are applied by default when you create a step. Like the Python version you are using, information on your Git connection (if you want to use a Git repository instead of a local folder, more information here) and the libraries and packages installed on the project.\\n\\nstep2_4\\n\\nBy default, your step will apply those parameters during its creation. However, sometimes you want to define them only at the step level and override the default ones defined at the project level.\\n\\nThis is the role of the create_step() functionâ€™s container_config parameter. You can pass as a dictionary the set of parameters you want to use for the step creation. It allows you to be specific in the configuration of your step.\\n\\nðŸ’¡ For example, if you need to build a specific step with another version of Python, you can specify the new Python version at the step level using language in the container_config parameter.\\n\\nYour project parameters will remain unchanged.\\n\\nTo go further\\nHere we will specify the requirements_path.\\n\\nTo take into account requirements.txt file, we must add it to the container_config parameter with the requirements_path.\\n\\nsdk.create_step(\\n    step_name=\"part-2-iristrain\",\\n    function_path=\"src/part-2-iris-train.py\",\\n    function_name=\"trainIris\",\\n    description=\"This function creates a classifier model for iris\",\\n    container_config = {\\n        \"requirements_path\" : \"requirements.txt\", #put here the path to the requirements.txt\\n        \"local_folder\": \".../get_started\", # Enter the path to your local folder here \\n    }\\n)\\nIt may also be useful to describe precisely the steps created to be able to understand their purpose afterward. To do so, you can fill in the description parameter during the step creation.\\n\\nTo go further with step creation\\n\\nIf you want to create a step based on the code of a Git repository, you can check this page.\\n\\nSuccess\\n\\nðŸŽ‰ Now your step has been created. You can now create your Pipeline.\\n\\nFrom here, we reproduce the same steps as before with the creation of the pipeline and we execute it.\\n\\nCreate a pipeline\\nCreate a pipeline with the create_pipeline() method of the SDK.\\n\\nsdk.create_pipeline(\\n    pipeline_name=\"part-2-iristrain\",\\n        step_name=\"part-2-iristrain\"\\n)\\nExecute your Pipeline and get the execution logs\\nNow you can execute your pipeline as in Part1.\\n\\nsdk.run_pipeline(pipeline_name=\"part-2-iristrain\")\\nYou can find the list of executions with information and logs in the frontend on the Execution tracking page.\\n\\nThe output is a list of iris categories :\\n\\n>> [2 0 2 0 2 2 0 0 2 0 0 0 1 2 2 0 0 0 1 1 0 0 1 0 2 1 2 1 0 2 0 2 0 0 2 0 2\\n1 1 1 2 2 2 1 0 1 2 2 0 1 1 2 1 0 0 0 2 1 2 0]\\nSuccess\\n\\nðŸŽ‰ You can now execute a more realistic Machine Learning pipeline.\\n\\nNow that we can have more complex code in our steps and we know how to parametrize the execution context of our steps, we would like to be able to give it input elements to vary the result and receive the result easily. For this, we can use the input/output feature offered by the platform.\\n\\nNext step: Part 3: Execute with input and output\\n\\n\\nPart 3: Execute a ML use case with inputs and outputs\\nIn Part 2, we have built and run our first ML pipeline to retrieve data from the data store, train a model and store it on the data store.\\n\\nWe will now train our model with new data, by adding an Input to the pipeline and send the predictions to a final user, by adding an Output to the pipeline.\\n\\nWe will first create the code of the predictIris() function so that it can receive data and return predictions.\\nThen, we will see how to create a step, a pipeline and run it on the platform with input data and return the corresponding predictions as an output.\\nBy the end of this part, we will have built a runable pipeline that allows to get the predictions of the iris species on new data with a simple execution.\\n\\nPrerequisites\\n\\nPython 3.8 or higher is required to be installed on your computer.\\nHave done the previous parts of this tutorial (Part 1: Execute a simple pipeline and Part 2: Execute a simple ML model).\\nOverview of the use case\\nWe will build a pipeline to retrieve the trained model stored in the last part and make a prediction on new data.\\n\\nstep3_0\\n\\nThe code we want to execute\\nFirst we have to implement our code to compute predictions with a stored model on the data store on any (correctly prepared) data given as input instead of computing predictions on a test set. Hence, our file src/part-3-iris-predict.py is as follows:\\n\\nfrom io import BytesIO\\nfrom craft_ai_sdk import CraftAiSdk\\nimport joblib\\nimport pandas as pd\\n\\n\\ndef predictIris(input_data: dict, input_model_path:str):\\n\\n   sdk = CraftAiSdk()\\n\\n   f = BytesIO()\\n   sdk.download_data_store_object(input_model_path, f)\\n   model = joblib.load(f)\\n\\n   input_dataframe = pd.DataFrame.from_dict(input_data, orient=\"index\")\\n   predictions = model.predict(input_dataframe)\\n\\n   final_predictions = predictions.tolist()\\n\\n   return {\"predictions\": final_predictions}\\nIn this code:\\n\\nWe add the argument input_data. Here, we choose it to be a dictionary like the one below:\\n{\\n    1: {\\n        \\'sepal length (cm)\\': 6.7,\\n      \\'sepal width (cm)\\': 3.3,\\n      \\'petal length (cm)\\': 5.7,\\n      \\'petal width (cm)\\': 2.1\\n    },\\n  2: {\\n      \\'sepal length (cm)\\': 4.5,\\n      \\'sepal width (cm)\\': 2.3,\\n      \\'petal length (cm)\\': 1.3,\\n      \\'petal width (cm)\\': 0.3\\n  },\\n}\\nIt contains the data on which we want to compute predictions.\\n\\nWe retrieve our trained model with the download_data_store_object() function of the sdk by passing the model path.\\n\\nAt the end, we convert our input_data dictionary into a Pandas dataframe, and we compute predictions with our trained model.\\n\\nAs you can see, the function now returns a Python dict with one field called â€œpredictionsâ€ that contains the predictions value. The platform only accepts step function with one return value of type ``dict``. Each item of this dict will be an output of the step and the key associated with each item will be the name of this output on the platform.\\n\\nMoreover, you can see that we converted our result from a numpy ndarray to a list. That is because the values of the inputs and outputs are restricted to native Python types such as int, float, bool, string, list and dict with elements of those types. More precisely anything that is json-serializable. Later, the platform might handle more complex input and output types such as numpy array or even pandas dataframe.\\n\\nDont forget to update your requirements.txt file, containing the list of Python libraries used in our step function:\\n\\njoblib==xx.xx.xx\\npandas==xx.xx.xx\\ncraft_ai_sdk==xx.xx.xx\\nTip\\n\\nExample with up-to-date version numbers available here.\\n\\nStep creation with Input and Output\\nNow, letâ€™s create our step on the platform. Here, since we have inputs and an output, our step is the combination of three elements: an input, an output and the Python function above. We will first declare the inputs and the output. Then, we will use the function sdk.create_step() as in Part 2 to create the whole step.\\n\\nstep3_1\\n\\nDeclare Input and Output for a new step\\nTo manage inputs and outputs of a step, the platform requires you to declare them using the Input and Output classes from the SDK.\\n\\nFor our Iris application, the inputs and outputs declaration would look like this:\\n\\nfrom craft_ai_sdk.io import Input, Output\\n\\nprediction_input = Input(\\n   name=\"input_data\",\\n   data_type=\"json\"\\n)\\n\\nmodel_input = Input(\\n   name=\"input_model_path\",\\n   data_type=\"string\"\\n)\\n\\nprediction_output = Output(\\n   name=\"predictions\",\\n   data_type=\"json\"\\n)\\nBoth objects have two main attributes:\\n\\nThe name of the Input or Output\\n\\nFor the inputs it corresponds to the names of the arguments of your stepâ€™s function. In our case name=\"input_data\" and \"input_model_path\", as in the first line of function:\\n\\ndef predictIris(input_data: dict, input_model_path:str):\\nFor the output it must be a key in the dictionary returned by your stepâ€™s function. In our case, name=\"predictions\" as in the last line of function:\\n\\nreturn {\"predictions\": final_predictions}\\nThe data_type describes the type of data it can accept. It can be one of: string, number, boolean, json, array, file.\\n\\nFor the inputs we want a dictionary and a string as we specified, which corresponds to data_type=\"json\" and data_type=\"string\".\\n\\nFor the output, we return a dictionary which corresponds to data_type=\"json\".\\nNow, we have everything we need to create the step and the pipeline corresponding to our predictIris() function.\\n\\nCreate step\\nNow as in Part 2, we\\'ll create our step on the platform using the sdk.create_step() function, but this time we specify our inputs and output:\\n\\nsdk.create_step(\\n   step_name=\"part-3-irisio\",\\n   function_path=\"src/part-3-iris-predict.py\",\\n   function_name=\"predictIris\",\\n   description=\"This function retrieves the trained model and classifies the input data by returning the prediction.\",\\n   inputs=[prediction_input, model_input],\\n   outputs=[prediction_output],\\n   container_config={\\n     \"local_folder\": \".../get_started\", # Enter the path to your local folder here \\n     \"requirements_path\": \"requirements.txt\",\\n   },\\n)\\nThis is exactly like in part 2, except for two parameters:\\n\\ninputs containing the list of Input objects we declared above (here, prediction_input and model_input).\\noutputs containing the list of Output objects we declared above (here, prediction_output).\\nWhen step creation is finished, you obtain a return describing your step (including its inputs and outputs) as below:\\n\\n>> Step \"part-3-irisio\" created\\n  Inputs:\\n    - input_data (json)\\n    - input_model_path (string)\\n  Outputs:\\n    - predictions (json)\\n>> Steps creation succeeded\\n>> {\\'name\\': \\'part-3-irisio\\',\\n \\'inputs\\': [{\\'name\\': \\'input_data\\', \\'data_type\\': \\'json\\'}, {\\'name\\': \\'input_model_path\\', \\'data_type\\': \\'string\\'}],\\n \\'outputs\\': [{\\'name\\': \\'predictions\\', \\'data_type\\': \\'json\\'}]}\\nNow that our step is created in the platform, we can embed it in a pipeline and run it.\\n\\nCreate pipeline\\nLetâ€™s create our pipeline here with sdk.create_pipeline() as in Part 2:\\n\\nsdk.create_pipeline(\\n   pipeline_name=\"part-3-irisio\",\\n   step_name=\"part-3-irisio\",\\n)\\nYou quickly obtain this output, which describes the pipeline, its step and its inputs and outputs:\\n\\n>> Pipeline creation succeeded\\n>> {\\'pipeline_name\\': \\'part-3-irisio\\',\\n \\'created_at\\': \\'xxxx-xx-xxTxx:xx:xx.xxxZ\\',\\n \\'steps\\': [\\'part-3-irisio\\'],\\n \\'open_inputs\\': [{\\'input_name\\': \\'input_data\\',\\n   \\'step_name\\': \\'part-3-irisio\\',\\n   \\'data_type\\': \\'json\\'}, {\\'input_name\\': \\'input_model_path\\',\\n   \\'step_name\\': \\'part-3-irisio\\',\\n   \\'data_type\\': \\'string\\'}],\\n \\'open_outputs\\': [{\\'output_name\\': \\'predictions\\',\\n   \\'step_name\\': \\'part-3-irisio\\',\\n   \\'data_type\\': \\'json\\'}]}\\nSuccess\\n\\nðŸŽ‰ Youâ€™ve created your first step & pipeline with inputs and outputs!\\n\\nLetâ€™s run this pipeline.\\n\\nRun a pipeline with new input data\\nPrepare input data\\nNow, our pipeline needs data as input (formatted as we said above â¬†ï¸). Letâ€™s prepare it, simply by choosing some of the rows of iris dataset we did not use when training our model:\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn import datasets\\n\\nnp.random.seed(0)\\nindices = np.random.permutation(150)\\niris_X, iris_y = datasets.load_iris(return_X_y=True, as_frame=True)\\niris_X_test = iris_X.loc[indices[90:120],:]\\n\\nnew_data = iris_X_test.to_dict(orient=\"index\")\\nLetâ€™s check the data we created:\\n\\nprint(new_data)\\nWe get the following output:\\n\\n>> 124: {\\'sepal length (cm)\\': 6.7,\\n\\'sepal width (cm)\\': 3.3,\\n\\'petal length (cm)\\': 5.7,\\n\\'petal width (cm)\\': 2.1\\n},\\n41: {\\'sepal length (cm)\\': 4.5\\n...\\nFinally, we need to encapsulate this dictionary in another one whose key is \"input_data\" (the name of the input of our step, i.e. the name of the argument of our stepâ€™s function). We define also the path to our trained model on the data store with the value associated to the key \"input_model_path\".\\n\\ninputs = {\\n    \"input_data\": new_data,\\n    \"input_model_path\": \"get_started/models/iris_knn_model.joblib\"\\n}\\nIn particular, when your step has several inputs, this dictionary should have as many keys as the number of inputs the step have.\\n\\nExecute the pipeline (RUN)\\nFinally, we can execute our pipeline with the data weâ€™ve just prepared by calling the run_pipeline() function almost as in Part 2 and passing our dictionary inputs to the inputs arguments of the function:\\n\\noutput_predictions = sdk.run_pipeline(\\n                        pipeline_name=\"part-3-irisio\",\\n                        inputs=inputs)\\nFinally, our output can be obtained like this:\\n\\nprint(output_predictions[\"outputs\"][\\'predictions\\'])\\nThis gives the output we want (with the predictions!):\\n\\n>> {\\'predictions\\': [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2]}\\nMoreover, you can check the logs on the web interface, by clicking on the Executions tracking tab of your environment, selecting your pipeline and choosing the last execution.\\n\\nSuccess\\n\\nðŸŽ‰ Congratulations! You have run a pipeline to which we can pass new data, the path to our trained model and get predictions.\\n\\nNext step: Part 4: Deploy a ML use case with inputs and outputs\\n\\n\\nPart 4: Deploy a ML use case with inputs and outputs\\nIntroduction\\nIn Part 3, we have built and run our second ML pipeline to retrieve our trained model from the data store, provide some new data to it as input and retrieve the result as an output of our pipeline execution.\\n\\nWhat if we want to let an external user execute our predict pipeline? Or if we want to schedule the execution of the pipeline that trains our model periodically?\\n\\nâ‡’ We need to deploy one pipeline via an endpoint and another one with a scheduled execution.\\n\\nThis part will show you how to do this with the Craft AI platform:\\n\\nWe will first update the code of the predictIris() function so that it can retrieve directly from the data store the trained model and returns the predictions as a json to the user.\\nWe will also update the code of the trainIris() function so that it re trains the model on a specific dataset (that could be often updated) and uploads the trained model directly to the datastore.\\nThen, we will see how to create a step and a pipeline that we will deploy on the platform in two different ways, and that could be executed periodicly and by a call.\\nPrerequisites\\n\\nPython 3.8 or higher is required to be installed on your computer.\\nHave done the previous parts of this tutorial ( Part 0: Setup, Part 1: Execute a simple pipeline, Part 2: Execute a simple ML model, Part 3: Execute a ML use case with inputs and outputs).\\nMachine Learning application with I/O\\nHere we will build an application based on what we did on the last part. We will expose our service to external users and schedule periodic executions.\\n\\nOverview of the use case\\nTo get the predictions via and endpoint:\\nstep4_0\\n\\nTo retrain the model periodicly (we will focus on this case later):\\nstep4_0_bis\\n\\nThe code we want to execute\\nWe will first focus on the construction of the endpoint the final user will be able to target.\\n\\nFirst we have to update our code to retrieve directly the model from the data store without any call to the sdk in the code and to return a file on the data store with the predictions inside. Hence, our file src/part-4-iris-predict.py is as follows:\\n\\nimport joblib\\nimport pandas as pd\\nimport json\\n\\ndef predictIris(input_data: dict, input_model:dict):\\n\\n   model = joblib.load(input_model[\\'path\\'])\\n\\n   input_dataframe = pd.DataFrame.from_dict(input_data, orient=\"index\")\\n   predictions = model.predict(input_dataframe)\\n\\n   return {\"predictions\": predictions.tolist()}\\nWhat changed are only how we get the trained model.\\n\\nmodel = joblib.load(input_model[\\'path\\'])\\ninput_model is a dictionary in which the key path refers to the file\\'s path where is located the file on the step environnement.\\n\\nThis input is a file data type.\\n\\nDon\\'t forget to update your requirements.txt file, containing the list of Python libraries used in our step function:\\n\\njoblib==xx.xx.xx\\npandas==xx.xx.xx\\nTip\\n\\nExample with up-to-date version numbers available here.\\n\\nStep creation with Input and Output\\nAs we did in part 3, we will first declare the inputs and the output. Then, we will use the function sdk.create_step() to create the whole step.\\n\\nstep4_1\\n\\nDeclare Input and Output of our new step\\nThe only difference now is the data type we will assign to input_model. This is now a file that we want to retrieve from the data store. To do so, we define the inputs and output like below:\\n\\nfrom craft_ai_sdk.io import Input, Output\\n\\nprediction_input = Input(\\n   name=\"input_data\",\\n   data_type=\"json\"\\n)\\n\\nmodel_input = Input(\\n   name=\"input_model\",\\n   data_type=\"file\"\\n)\\n\\nprediction_output = Output(\\n   name=\"predictions\",\\n   data_type=\"json\"\\n)\\nWe have just seen the code of the step has been adapted to handle file objects.\\n\\nNow, we have everything we need to create, as before, the step and the pipeline corresponding to our predictIris() function.\\n\\nCreate your step\\nNow as in Part 3, it is time to create our step on the platform using the sdk.create_step() function, with our inputs and output:\\n\\nsdk.create_step(\\n   step_name=\"part-4-iris-deployment\",\\n   function_path=\"src/part-4-iris-predict.py\",\\n   function_name=\"predictIris\",\\n   description=\"This function retrieves the trained model and classifies the input data by returning the prediction.\",\\n   inputs=[prediction_input, model_input],\\n   outputs=[prediction_output],\\n   container_config={\\n     \"local_folder\": \".../get_started\", # Enter the path to your local folder here \\n     \"requirements_path\": \"requirements.txt\",\\n   },\\n)\\nWhen the step creation is finished, you obtain an output describing your step (including its inputs and outputs) as below:\\n\\n>> Step \"part-4-iris-deployment\" created\\n  Inputs:\\n    - input_data (json)\\n    - input_model (file)\\n  Outputs:\\n    - predictions (json)\\n>> Steps creation succeeded\\n>> {\\'name\\': \\'part-4-iris-deployment\\',\\n \\'inputs\\': [{\\'name\\': \\'input_data\\', \\'data_type\\': \\'json\\'}, {\\'name\\': \\'input_model\\', \\'data_type\\': \\'file\\'}],\\n \\'outputs\\': [{\\'name\\': \\'predictions\\', \\'data_type\\': \\'json\\'}]}\\nNow that our step is created in the platform, we can embed it in a pipeline and deploy it.\\n\\nCreate your pipeline\\nLetâ€™s create our pipeline here with sdk.create_pipeline() as in Part 3:\\n\\nsdk.create_pipeline(\\n   pipeline_name=\"part-4-iris-deployment\",\\n   step_name=\"part-4-iris-deployment\",\\n)\\nYou quickly obtain this output, which describes the pipeline, its step and its inputs and outputs:\\n\\n>> Pipeline creation succeeded\\n>> {\\'pipeline_name\\': \\'part-4-iris-deployment\\',\\n \\'created_at\\': \\'xxxx-xx-xxTxx:xx:xx.xxxZ\\',\\n \\'steps\\': [\\'part-4-iris-deployment\\'],\\n \\'open_inputs\\': [{\\'input_name\\': \\'input_data\\',\\n   \\'step_name\\': \\'part-4-iris-deployment\\',\\n   \\'data_type\\': \\'json\\'}, {\\'input_name\\': \\'input_model\\',\\n   \\'step_name\\': \\'part-4-iris-deployment\\',\\n   \\'data_type\\': \\'file\\'}],\\n \\'open_outputs\\': [{\\'output_name\\': \\'predictions\\',\\n   \\'step_name\\': \\'part-4-iris-deployment\\',\\n   \\'data_type\\': \\'json\\'}]}\\nSuccess\\n\\nðŸŽ‰ Youâ€™ve created your second step & pipeline with inputs and output!\\n\\nCreate your deployments with input and output mappings\\nHere, we want to be able to execute the pipeline, either by launching the execution with an url link or at a certain time, but not by a run anymore.\\n\\nLet\\'s try the first case.\\n\\nWe want the user to be able to:\\n\\nsend the input data directly to the application via an url link\\nretrieve the results directly from the endpoint\\nWe want also to specify the path to the stored model on the data store, so that the service will take this model directly from the data store. The user won\\'t be the one selecting the model used, it\\'s only on the technical side.\\n\\nCreate the endpoint with IO mappings\\nAn endpoint is a publicly accessible URL that launches the execution of the Pipeline.\\n\\nWithout the platform, you would need to write an api with a library like Flask, Fast API or Django and deploy it on a server that you would have to maintain.\\n\\nstep4_2\\n\\nCreate the endpoint\\nTo create an endpoint, go to the UI. Once in your project, go to the Pipelines page and select your environment. On this page select your pipeline part-4-iris-deployment and press Deploy.\\n\\nOn the page, enter the name for your deployment, (here you will use part-4-iris-deployment) and select Elastic mode in Execution Mode and Endpoint in Execution Rule.\\n\\nstep4_6\\n\\nOnce you have done this, click on \\'Next Step\\' where you will be presented with the mapping page to fill in.\\n\\nIO Mappings\\nFor each input and output in your pipeline, you need to define the source or destination:\\n\\nInput input_data: Select Endpoint. You can leave the default name in Exposed name.\\nInput input_model: Select Datastore and enter the path to the file get_started/models/iris_knn_model.joblib in the Datastore.\\nOutput predictions: Select Endpoint and iris_type as the exposed name.\\nstep4_7\\n\\nThen click Create Deployment and the build will begin.\\n\\nTarget the endpoint\\nPrepare the input data\\n\\nNow, our endpoint needs data as input, like we did for last part:\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn import datasets\\n\\nnp.random.seed(0)\\nindices = np.random.permutation(150)\\niris_X, iris_y = datasets.load_iris(return_X_y=True, as_frame=True)\\niris_X_test = iris_X.loc[indices[90:120],:]\\n\\nnew_data = iris_X_test.to_dict(orient=\"index\")\\nWe need to encapsulate this dictionary in another one whose key is \"input_data\" (the name of the input of our step, i.e. the name of the argument of our stepâ€™s function). We don\\'t need to define the path to our trained model because it is already defined with the output mapping we have just done.\\n\\ninputs = {\\n    \"input_data\": new_data\\n}\\nCall the endpoint with the input data\\n\\nendpoint_url = sdk.base_environment_url  + \"/endpoints/part-4-iris-deployment\"\\nendpoint_token = \"MY_ENDPOINT_TOKEN\"\\nrequest = requests.post(endpoint_url, headers={\"Authorization\": f\"EndpointToken {endpoint_token}\"}, json=inputs)\\nrequest.json()\\nWarning\\n\\nDon\\'t forget to include your deployment name (in the URL) and your endpoint token. All this information is available in the deployment information, as shown here:\\n\\nStep 4.5\\n\\nThere is also sample code you can copy-paste to call the endpoint.\\n\\nThe HTTP code 200 indicates that the request has been taken into account. In case of an error, we can expect an error code starting with 4XX or 5XX.\\n\\nIt is a way to call your deployment. But, obviously, you can call it with any other HTTP client (curl command in bash, Postmanâ€¦).\\n\\nWarning\\n\\nNote that you can\\'t directly call your endpoint and receive the output by entering the URL in your web browser, as the request is based on the POST method and requires an authentication header.\\n\\nLet\\'s check we can get the predictions as output of the endpoint:\\n\\nprint(request.json()[\\'outputs\\'][\\'iris_type\\'])\\nMoreover, you can check the logs on the UI, by clicking on the Executions tab of your environment, selecting your pipeline and choosing the last execution.\\n\\nSuccess\\n\\nðŸŽ‰ Youâ€™ve created your first deployment and you\\'ve just called it!\\n\\nRetrain the model periodically\\nLet\\'s imagine that our dataset is frequently updated, for instance we get new labeled iris data every day. In this case we might want to retrain our model by triggering our training pipeline part4-iris-train every day.\\n\\nThe platform can do this automatically using the periodic execution rule in our deployment.\\n\\nA periodic execution rule allows to schedule a pipeline execution at a certain time. For example, every Monday at a certain time, every month, every 5 minutes etc.\\n\\nThe inputs and output have to be defined, with a constant value or a data store mappings. First we will update our trainIris function so that it produces a file output containing our model, that we will then map to the datastore. You can check the entire updated version of this function in src/part-4-iris-predict.py. The only change is done at the return of the function:\\n\\n> return {\"model\": {\"path\": \"iris_knn_model.joblib\"}}\\nWe can then create the step and pipeline as we are used to.\\n\\ntrain_output = Output(\\n   name=\"model\",\\n   data_type=\"file\"\\n)\\n\\nsdk.create_step(\\n   step_name=\"part-4-iristrain\",\\n   function_path=\"src/part-4-iris-predict.py\",\\n   function_name=\"trainIris\",   \\n   container_config={\\n        \"local_folder\": \".../get_started\", # Enter the path to your local folder here \\n   },\\n   outputs=[train_output],\\n)\\n\\nsdk.create_pipeline(\\n   pipeline_name=\"part-4-iristrain\",\\n   step_name=\"part-4-iristrain\"\\n)\\nNow let\\'s create a deployment that executes our pipeline every 5 minutes (In a real case, we would have set it to every day. However, for this example, let\\'s set it to every 5 minutes, so we can see it in action immediately). In our case, we will map the prediction output (which is our only I/O) to the datastore on the same path that is used in the pediction endpoint deployment. This way our prediction pipeline will automatically use the latest version of our model for predictions.\\n\\nstep4_3\\n\\nCreate periodic deployment\\n\\nLet\\'s create a deployment that will schedule our pipeline to be executed every 5 minutes. IO mappings will be the same as in the endpoint, except that this time the input data is a constant and not something that is provided to the endpoint whenever it is called.\\n\\nLet\\'s return to the Pipelines page and deploy our pipeline. Select Elastic for the Execution Mode and Periodic for the Execution Rule. We then need to add a trigger (at the bottom of the page) and specify that the execution should be repeated every 5 minutes.\\n\\nstep4_8\\n\\nOnce the trigger is created you can go to the Next step.\\n\\nAdapt IO mapping\\n\\nThen, you need to define the destination of your output:\\n\\nOutput model: Select Datastore and enter the path to the file get_started/models/iris_knn_model.joblib in the datastore.\\nstep4_10\\n\\nAnd now, you can complete the deployment of your pipeline.\\n\\nOur training pipeline will now be executed every 5 minutes, updating our model with the potential new data. The predict pipeline will then use this updated model automatically.\\n\\nYou can check that you actually have a new execution every 5 minutes using the sdk or via the web interface.\\n\\nSuccess\\n\\nðŸŽ‰ Congrats! Youâ€™ve created your second deployment and planned it to run every 5 minutes!\\n\\nConclusion\\nSuccess\\n\\nðŸŽ‰ After this Get Started, you have learned how to use the basic functionalities of the platform! You know now the entire workflow to create a pipeline and deploy it.\\n\\nstep4_4\\n\\nYou are now able to:\\n\\nDeploy your code through a pipeline in a few lines of code, run it whenever you want and have the logs to analyze the execution.\\nUse the Data Store on the platform to upload and download files, models, images, etc.\\nExecute your pipeline via an endpoint that is accessible from outside with a secured token, or via a periodic execution.\\nMake your inputs flexible: set constant values to avoid users to fill in, let users enter inputs values via the endpoint directly, or use the data store to retrieve or put objects.\\nIf you want to go further\\n\\nOne concept has not been explained to you: the metrics.\\n\\nIf you want to go further and discover this feature, you can read the associated documentation.\\n\\n\\n\\nAccessing your data\\nWhen executing your code within Pipelines on the Craft AI Platform, one crucial aspect is the ability to retrieve and store data. In this context, we will explore two approaches to acquiring data during our Pipeline Executions:\\n\\nUsing the Data Store provided by the Craft AI Platform, which offers a convenient way to store and retrieve objects on the environment.\\nConnecting to external data sources like you are used to. By accessing your own organization database, cloud external database, some open source data, data available via FTP or some data storage such as the classic ones offered by AWS, Azure, or Google Cloud Platform, we can expand the range of data available for our Pipeline Executions.\\nBy combining the capabilities of both the Data Store and external data sources, we can ensure a reliable and efficient data retrieval system for our executions. We will take a closer look at the techniques and practices of retrieving data for pipeline executions.\\n\\nWarning\\n\\nBe aware that when an execution is launched on the platform, what is on its execution context is not persistent (i.e. it does not remain after the execution), all data kept in memory or on the disk is removed at the end of the execution. It is possible to read, write and manipulate data during the execution, but everything in the execution context at the end of the execution is deleted.\\n\\nThis allows you to have an identical and stable execution context for each run, while avoiding needlessly saturating the disk with your executions.\\n\\nTo ensure the persistence of your data, you can use data sources: It can be the Data Store, your own database or external data storage.\\n\\nSummary\\nHow to store data on the Data Store\\nHow to retrieve data from the Data Store\\nHow to access an external Data Source\\nHow to store data on the Data Store\\nHere we will see how to store files created within a step.\\n\\nTo do this, we will see 2 methods:\\n\\nWith the dedicated SDK function upload_data_store_object()\\n\\nAdvantages:\\n\\nMore flexibility at the Data Store path level. This method allows you to upload files to any location on the Data Store, as you can easily change the file path by providing a different input to the code.\\nDrawbacks:\\n\\nNeed to initialize the SDK in the step.\\nNo tracking (the file path values given as inputs are not stored).\\nWith an Output mapping to the Data Store\\n\\nAdvantages: - No need to initialize the SDK on the step. - tracking of the file used in this execution.\\n\\nDrawbacks:\\n\\nNeed to define the Storage location on the Dataupload_data_store_object() Store before creating the deployment and the path canâ€™t be modified afterwards.\\nOnly one possible storage location.\\nWith the dedicated SDK function upload_data_store_object()\\nYou can also access the Data Store directly from the step code by using the SDK function upload_data_store_object().\\n\\nWarning\\n\\nNote that you need to import the craft_ai_sdk library at the beginning of the code, and have it in your requirements.txt.\\n\\nThe SDK connection is initialized without token or environment URL, since the step will already be executed in the environment.\\n\\nExample\\n\\nget_data.py\\nfrom craft_ai_sdk import CraftAiSdk\\n\\ndef preprocess_data():\\n\\n    # SDK initialization\\n    sdk = CraftAiSdk()\\n\\n    # Using an existing function get_data() to get raw data\\n    df_data = get_data()\\n    # Using an existing function preprocess_data() to preprocess\\n    # the previously retrieved data\\n    df_preprocessed_data = preprocess_data(df_data)\\n    # The existing function create_csv()  writes the dataframe \\n    # df_preprocessed_data as a csv in path_to_preprocessed_data\\n    create_csv(df_preprocessed_data, path_to_preprocessed_data)\\n\\n    # Upload into datastore with the SDK function upload_data_store_object()\\n    sdk.upload_data_store_object(\\n        filepath_or_buffer=path_to_preprocessed_data, \\n        object_path_in_datastore=\"data/preprocessed/data_preprocessed.txt\"\\n    )\\nThen, simply create the step and the pipeline to execute this code.\\n\\nWith an Output mapping to the Data Store\\nHere we will see how to store files that were created within a step to the data store.\\n\\nTo do so, we have a few points to follow:\\n\\nAdapt the code that will be executed in the step, especially by specifying the path on which the file is accessible on the step.\\ndef yourFunction() :\\n    ...\\n    return {\"outputfile\" : \\n        {\"path\": **path of the file on the step**}\\n    }\\nBefore creating the step, define the output of the step with a data_type as file, and create the step and the pipeline as we are used to.\\nfrom craft_ai_sdk.io import Output\\n\\nstep_output = Output(\\n    name=step_output_name,\\n    data_type=\"file\", \\n)\\n\\nsdk.create_step(\\n    function_path=**the path to your function**,\\n    function_name=\"my_function_name\", \\n    step_name=**my step name**,\\n    container_config = { \\n        \"local_folder\": \"my_step_folder/\",\\n    },\\n\\n    outputs=[step_output]\\n)\\n\\nsdk.create_pipeline(pipeline_name=**your pipeline name**, \\n    step_name=**your step name**)\\nDefine the output mapping (with the sdk object OutputDestination) that will be used for the execution of your pipeline, in this case, a datastore_path is needed to map the output of the step to a specific path on the Data Store.\\nfrom craft_ai_sdk.io import OutputDestination\\n\\noutput_mapping = OutputDestination(\\n    step_output_name=step_output_name,\\n    datastore_path=**path on which we want to store the file**,\\n)\\n\\nsdk.run_pipeline(\\n    pipeline_name=**your pipeline name**,\\n    outputs_mapping=[output_mapping]\\n)\\nExample\\n\\nIn this part, we will create a deployment with an endpoint execution rule that returns 2 files, 1 file with text in .txt format and another with a fake confusion matrix in format .csv.\\n\\nFirst, we write the code to create the 2 files.To be able to pass the files to the Data Store, we specify the paths of the 2 files on the step.\\n\\nNote\\n\\nNote the step execution context is an isolated container, and it\\'s the platform that will then copy the files from the execution context to the Data Store (thanks to the output weâ€™ve created).\\n\\nDonâ€™t forget to indicate the dependencies into requirements.txt.\\n\\nimport numpy as np\\nimport pandas as pd\\n\\ndef createFiles() :\\n\\n    # Define file into local step contenaire \\n    path_text = \"file_text_output.txt\"\\n    path_matrix = \"confusion_matrix.csv\"\\n\\n    # Create a fake confusion matrix into .csv file\\n    confusion_matrix = np.array([[100, 20, 5],\\n                                [30, 150, 10],\\n                                [10, 5, 200]])\\n    class_labels = [\\'Class A\\', \\'Class B\\', \\'Class C\\'] # Define the class labels\\n    df = pd.DataFrame(confusion_matrix, index=class_labels, columns=class_labels) # Create a DataFrame from the confusion matrix\\n    df.to_csv(path_matrix, index=True, header=True) # Save the DataFrame as a CSV file\\n\\n    # Create .txt file\\n    text_file = open(path_text, \\'wb\\')  # Open the file in binary mode\\n    text_file.write(\"Result of step send in file output :) \".encode(\\'utf-8\\'))  # Encode the string to bytes\\n    text_file.close()\\n\\n    # Return the path of the file in the container of the current step execution.\\n    fileOjb = {\\n        \"txtFile\" : {\"path\": path_text}, \\n        \"csvFile\" : {\"path\": path_matrix}\\n    }\\n    return fileOjb\\nWarning\\n\\nRemember to push step code to a GitHub repository defined in information project into Craft AI platform.\\n\\nAfter the initialization of SDK connection, we can create the 2 outputs and then the step.\\n\\nFor this step, we assume that all the information is already specified in the project settings (language and repository information).\\n\\n# Output creation\\nstep_output_txt = Output(\\n    name=\"txtFile\",\\n    data_type=\"file\", \\n)\\n\\nstep_output_csv = Output(\\n    name=\"csvFile\",\\n    data_type=\"file\", \\n)\\n\\n# Step creation with output (we supose repository is setup in info project)\\nsdk.create_step(\\n    function_path=\"src/createFiles.py\",\\n    function_name=\"createFiles\", \\n    step_name=\"doc-2o-datastore-step\",\\n    container_config = { \\n        \"local_folder\": \"my_step_folder/\",\\n    },\\n\\n    outputs=[step_output_txt, step_output_csv]\\n)\\nNow, we can create the pipeline.\\n\\nsdk.create_pipeline(pipeline_name=\"doc-2o-datastore-pipeline\", \\n    step_name=\"doc-2o-datastore-step\")\\nTo create an accessible endpoint, we need to create a deployment with two output mappings to the Data Store we created earlier.\\n\\nLetâ€™s pretend we want to store the files at \"docExample/resultText.txtâ€ and \"docExample/resultMatrix.csv\" on the Data Store.\\n\\nendpoint_output_txt = OutputDestination(\\n    step_output_name=\"txtFile\",\\n    datastore_path=\"docExample/resultText.txt\",\\n)\\n\\nendpoint_output_csv = OutputDestination(\\n    step_output_name=\"csvFile\",\\n    datastore_path=\"docExample/resultMatrix.csv\",\\n)\\n\\nendpoint = sdk.create_deployment(\\n    execution_rule=\"endpoint\",\\n    pipeline_name=\"doc-2o-datastore-pipeline\",\\n    deployment_name=\"doc-2o-datastore-dep\",\\n    outputs_mapping=[output_mapping_txt, output_mapping_csv]\\n)\\nAfter that, we can trigger the endpoint with the Python code below (we can use any other tool like postman, curl â€¦).\\n\\nimport requests \\n\\nendpoint_url = **your-url-env**+\"/endpoints/doc-2o-datastore-dep\"\\nheaders = {\"Authorization\": \"EndpointToken \"+endpoint[\"endpoint_token\"]}\\n\\nresponse = requests.post(endpoint_url, headers=headers)\\n\\nprint (response.status_code, response.json())\\nHow to retrieve data from the Data Store\\nHere, we will see how to retrieve files already stored on the Data Store within a step.\\n\\nTo do this, we will see 2 methods:\\n\\nWith the dedicated SDK function download_data_store_object()\\n\\nAdvantages: - More flexibility at the Data Store path level. This method allows you to download files from any location on the Data Store, as you can easily change the file path by providing a different input to the code.\\n\\nDrawbacks:\\n\\nNeed to initialize the SDK in the step.\\nNo tracking (the file path values given as inputs are not stored).\\nWith an Input mapping to the Data Store\\nAdvantages: - No need to initialize the SDK on the step. - tracking of the file used in this execution.\\n\\nDrawbacks:\\n\\nNeed to define the Storage location on the Data Store before creating the deployment and the path canâ€™t be modified afterwards.\\nOnly one possible storage location.\\nWith the dedicated SDK function download_data_store_object()\\nYou can access the Data Store directly from the step code by using the SDK function download_data_store_object().\\n\\nNote\\n\\nNote that you need to import the craft_ai_sdk library at the beginning of the code, and have it in your requirements.txt.\\n\\nThe connection is initialized without token or environment URL, since the Step will already be executed in the environment.\\n\\nExample\\n\\nfrom craft_ai_sdk import CraftAiSdk\\n\\ndef retrievePredictions(id_prediction: int):\\n\\n    # SDK initialization\\n    sdk = CraftAiSdk()  \\n\\n    # Download the file containing the predictions \\n    # of the id_prediction at the local path \"predictions.txt\"\\n    sdk.download_data_store_object(\\n        object_path_in_datastore=f\"data/predictions_{id_prediction}.csv\", \\n        filepath_or_buffer=\"predictions.txt\"\\n    )\\n\\n        # Open and print the content of the file now stored locally\\n    with open(\"predictions.txt\") as f:\\n        contents = f.readlines()\\n        print (contents)\\nThen, simply create the step and the pipeline to execute this code.\\n\\nWith an Input mapping to the Data Store\\nWe will need to define the Input as a file for the step, the input mapping that connects the Data Store to the step and therefore the code embedded within the step to correctly read the file. Letâ€™s start with the latter.\\n\\nTo do so, we have a few points to follow:\\n\\nAdapt the code to access and read your file. Indeed, the input that will be passed to the step will have a predefined form as a dictionary with path as key and the file path as a value. You thus need to access it the same way you retrieve the value of a dictionary. The file will be downloaded in the execution environment before the step is executed. You can then use the file as you would use any other file in the execution environment.\\n\\nHere, we have a function readFile that aims to read the input file and print its content.\\n\\ndef read_file (entryFile: dict) :\\n\\n    # Access the file with its local path (entryFile[\"path\"]) on the step\\n    with open(entryFile[\"path\"]) as f:\\n        contents = f.readlines()\\n        print (contents)\\nWarning\\n\\nOne the code is updated, remember to push step code to a GitHub repository defined in information project into Craft AI platform.\\n\\nBefore creating the step, define the input of the step with a data_type as file, and create the step and pipeline as we are used to.\\n\\nNote\\n\\nWe assume that all the information is already specified in the project settings (language, repository and branch information).\\n\\nfrom craft_ai_sdk.io import Input\\n\\n# Define the input of the step \\nstep_input = Input(\\n    name=\"entryFile\",\\n    data_type=\"file\",    \\n)\\n\\n# Create the step \\nsdk.create_step(\\n    function_path=\"src/read_file.py\",\\n    function_name=\"read_file\", \\n    step_name=\"file-datastore-step\",\\n    container_config = { \\n        \"local_folder\": \"my_step_folder/\",\\n    },\\n\\n    inputs=[step_input]\\n)\\n\\n# Create the pipeline\\nsdk.create_pipeline(pipeline_name=\"file-datastore-pipeline\", \\n    step_name=\"file-datastore-step\")\\nOnce the pipeline is created, we define the correct input mapping and run the pipeline with it.\\n\\nDefine the input mapping (with the SDK object InputSource) that will be used for the execution of your pipeline, in this case, a datastore_path is needed to map the input of the step to the file on the Data Store.\\n\\nLetâ€™s pretend the file we want to retrieve is stored at myFolder/text.txt on the Data Store.\\n\\nfrom craft_ai_sdk.io import InputSource\\n\\ninput_mapping = InputSource(\\n    step_input_name=\"entryFile\"\\n    datastore_path=\"myFolder/text.txt\", # Path of the output file in the datastore\\n)\\n\\n# Run pipeline using mapping defined with InputSource object\\nsdk.run_pipeline(\\n    pipeline_name=\"file-datastore-pipeline\", \\n    inputs_mapping=[input_mapping]\\n)\\nYou can check the logs for your file content.\\n\\npipeline_executions = sdk.list_pipeline_executions(\\n    pipeline_name=\"file-datastore-pipeline\"\\n)\\n\\nlogs = sdk.get_pipeline_execution_logs(\\n    pipeline_name=pipeline_name, \\n    execution_id=pipeline_executions[0][\\'execution_id\\']\\n)\\n\\nprint(\\'\\\\n\\'.join(log[\"message\"] for log in logs))\\nHow to access an external Data Source\\nThe connection with an external data source (database or data storage) involves the following steps:\\n\\nUsing the same code you would use without the Craft AI platform to access your data storage. You only have to encapsulate your code within a step and a pipeline, like any code you would like to execute on the Craft AI platform. You also may have to adapt the inputs and outputs of your main function to respect the Craft AI formats.\\n\\nEmbedding your credentials on your platform environment to use them in your step code to access the database.\\n\\nðŸŒŸ To securely use credentials, a common good practice is to define environment variables to store the credentials safely. If you want to do so, the SDK offers you an easy solution: the create_or_update_environment_variable() function.\\n\\nsdk.create_or_update_environment_variable(\"USERNAME\", \"username_db_1\")\\nThen, when you need the credentials, you access it with the following command :\\n\\nimport os\\nimport dotenv\\n\\ndotenv.load_dotenv()\\n\\nusername = os.environ[\"USERNAME\"]\\nFrom an external Database\\nMoreover, if you try to access to an external Database, we may have to whitelist the IP of your Craft AI platform environment(s) if necessary in your data source configuration. The environments IP are available on the page dedicated to your project environments.\\n\\nenv_info\\n\\nHere is an example of the few points to do in order to access a specific external database directly from our Craft AI environment platform by using the usual credentials and be able to filter on some data on the database:\\n\\nTip\\n\\nIf needed, add the Craft AI environment platform URL to the whitelist of the external database you want to access.\\n\\nFirst, we embed the credentials we usually use to access to the database in our environment by setting them as environment variables.\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"DB_HOST\",\\n    environment_variable_value=\"xxx\")\\n\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"DB_ADMIN\",\\n    environment_variable_value=\"xxx\")\\n\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"DB_PASS\",\\n    environment_variable_value=\"xxx\")\\n\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"DB_NAME\",\\n    environment_variable_value=\"xxx\")\\n\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"DB_PORT\",\\n    environment_variable_value=\"xxx\")\\nSecondly, we encapsulate the code we would use without the Craft AI platform in a function, here filter_data_from_database(), that will be executed within a step:\\n\\nBelow, we use an existing function create_db_connection() that creates a connection to the database with the credentials brought on our Craft AI environment platform.\\nThen, we use an existing function filter_data() that retrieves some data in a specific table on the database by filtering with the ids inputs on a specific column.\\nAs input of our filter_data_from_database() function, we have this list of integer, ids, and as output we have a dictionary whose key is filtered_data. ids and filtered_data are respectively input and output of the step we would define after.\\nHere is the corresponding code:\\n\\nimport os\\nimport dotenv\\n\\ndotenv.load_dotenv()\\n\\nDB_HOST = os.environ[\"DB_HOST\"]\\nDB_ADMIN = os.environ[\"DB_ADMIN\"]\\nDB_PASS = os.environ[\"DB_PASS\"]\\nDB_NAME = os.environ[\"DB_NAME\"]\\nDB_PORT = os.environ[\"DB_PORT\"]\\n\\ndef filter_data_from_database(ids: List[int]) :\\n    try:\\n        # With an existing function create_db_connection,\\n        # Creating a connection to the database with the \\n        # credentials brought on our Craft AI environment platform.\\n        conn = create_db_connection(\\n            host=DB_HOST,\\n            user=DB_ADMIN,\\n            password=DB_PASS,\\n            database=DB_NAME,\\n            port=DB_PORT\\n        )\\n\\n        # With an existing function filter_data, \\n        # Retrieving some data in a specific table on the database \\n        # by filtering with the ids inputs on a specific column.\\n        df_data_filtered = filter_data(conn, ids)\\n        df_data_filtered_final = df_data_filtered.tolist()\\n    finally:\\n        conn.close()\\n    return {\"filtered_data\": df_data_filtered_final}\\nFrom an external Data Storage\\nHere is an example of the few points to do in order to access a specific external data storage directly from a Craft AI environment platform by using the usual credentials and be able to retrieve one specific csv file:\\n\\nFirst, we embed the credentials we usually use to access to the data storage in our environment by setting them as environment variables.\\nsdk = CraftAiSdk(\\n    sdk_token=**our-sdk-token**, \\n    environment_url=**our-environment-url**)\\n\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"SERVER_PUBLIC_KEY\",\\n    environment_variable_value=\"xxx\")\\n\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"SERVER_SECRET_KEY\",\\n    environment_variable_value=\"xxx\")\\n\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"REGION_NAME\",\\n    environment_variable_value=\"xxx\")\\nSecondly, we encapsulate the code we would use without the Craft AI platform in a function that will be executed within a step.\\nBelow, we use an existing function configure_client() that configures a client (data storage specific) with the credentials brought on our Craft AI environment platform to access the data storage.\\nThen, we use an existing function get_object_from_bucket() that, with the configured client, retrieves the object key in the bucket bucket on the data storage.\\nimport os\\nimport dotenv\\n\\ndotenv.load_dotenv()\\n\\nSERVER_PUBLIC_KEY = os.environ[\"SERVER_PUBLIC_KEY\"]\\nSERVER_SECRET_KEY = os.environ[\"SERVER_SECRET_KEY\"]\\nREGION_NAME = os.environ[\"REGION_NAME\"]\\n\\ndef filter_data_from_data_storage(bucket: str, key:str)\\n\\n    # With an existing function configure_client, \\n    # Configuring a client (data storage specific) \\n    # with the credentials brought on our Craft AI environment platform.\\n    client = configure_client(\\n        public_key=SERVER_PUBLIC_KEY, \\n        secret_key=SERVER_SECRET_KEY, \\n        region=REGION_NAME)\\n\\n    # With an existing function get_object_from_bucket,\\n    # Retrieving the object key in the bucket bucket on the data storage.\\n    buffer = get_object_from_bucket(\\n        client=client,\\n        bucket_name=bucket,\\n        key=key\\n    )\\n\\n    dataframe = pd.read_csv(buffer)\\n    return dataframe\\n\\n\\nAdapt your code\\nYou have an existing function executing locally or on a docker for example, you would like to execute it on the Craft AI platform.\\n\\nWhether it be a very basic function, one with inputs and/or outputs, or one that uses external data, it may be necessary to adapt the code to make it runnable without error on the platform.\\n\\nðŸ’¡ Best practice, have code ready for production\\n\\nFunctional Application Design:\\nStructure your production code into nested functions to promote modularity, reuse, and I/O tracking.\\nDo not use notebooks for production; use Python scripts and associated functions.\\nIdentify the main inputs and desired outputs of the main function.\\n\\nApplication Evolution Management:\\n\\nUse Git to version your code, monitor development, and track changes throughout the application\\'s lifecycle.\\nCommit and push your code when you\\'re ready to release it.\\n\\nExplicit Dependencies for Stability:\\n\\nChoose and declare the right dependencies in requirements.txt to ensure constant functionality.\\nSpecify the version of Python compatible with your project during commissioning (available directly in the platform, project parameter).\\nPrerequisites\\nBefore using the Craft AI platform, make sure you have completed the following prerequisites:\\n\\nGet access to an environment\\nConnect the SDK\\n\\nIf this is not the case, you can go to this page for more information : Connect to the platform\\n\\nHow can I Encapsulate a basic script ?\\nBefore explaining how to encapsulate, let\\'s first look at what encapsulation is in the Craft AI platform. Encapsulation is the act of taking your python code and putting it into a standard object for the platform that allows it to run with all the specific features it needs.\\n\\nFor example, encapsulation allows you to :\\n\\nHave all pip libraries correctly installed\\nReceive and send data with the platform and your users\\nEtc.\\nWhen your script is encapsulated in the platform, it is called a â€œstepâ€.\\n\\nThe aim of the platform is to enable you to create a step from your code with as little change as possible to your source code. Some adaptations may therefore be necessary, and we\\'ll explain them here.\\n\\nEncapsulate â€œHello worldâ€ case\\nThe platform encapsulates Python scripts in order to execute them. For reasons of ease of use (particularly with regard to input/output), the platform encapsulates Python functions directly.\\n\\nLet\\'s imagine a hello_world.py script:\\n\\nâŒ this script does not work\\n\\nprint (\"Hello world\")\\nâœ… this script works\\n\\ndef fct_hello(): \\n    print (\"Hello world\")\\nOnce you have uploaded your script to the repository linked to your environment. You can use the Python SDK to request encapsulation of your script with just 1 line of code:\\n\\nsdk.create_step(\\n    function_path=\"hello_world.py\", # Path of were is the script file in your repo\\n    function_name=\"fct_hello\", # Name of function to run\\n    step_name=\"step_example\" # Unique name to identify your step in your environment\\n    container_config = { \\n        \"local_folder\": \"my_step_folder/\",\\n    }\\n)\\nFinally, when your step is ready, you can create a pipeline from it that will be ready to be executed at any time by the platform.\\n\\nEncapsulate a script that uses pip to install libraries\\nIn data science projects, it is common to use libraries not native to Python, generally installed with pip. As part of the encapsulation process, this must also be specified when the step is created using a requirement.txt file.\\n\\nTo do this, add the requirement.txt file to the repository where the script to be executed in the pipeline is located, then specify it when the step is created.\\n\\nsdk.create_step(\\n    function_path=\"hello_world.py\", # Path of were is the script file in your repo\\n    function_name=\"fct_hello\", # Name of function to run\\n    step_name=\"step_example\" # Unique name to identify your step in your environment\\n\\n    # arg requirements_path should be in dict in container_config parameter.\\n    container_config = { \\n        \"requirements_path\": \"requirements.txt\",    # requirement path in repository\\n    \"local_folder\": \"my_step_folder/\",\\n    },\\n)\\nInfo\\n\\nIn addition to pip dependencies, you can add Linux-compatible system dependencies (equivalent to an apt-get install).\\n\\nNote that this is cumulative with pip dependencies.\\n\\n\\nExample:\\n\\n```py\\nsdk.create_step(\\n    function_path=\"hello_world.py\", # Path of were is the script file in your repo\\n    function_name=\"fct_hello\", # Name of function to run\\n    step_name=\"step_example\" # Unique name to identify your step in your environment\\n\\n    # arg requirements_path should be in dict in container_config parameter\\n    container_config = { \\n        \"system_dependencies\": [\"python3-opencv\", \"python3-scipy\"],# list of dependancy name\\n    \"local_folder\": \"my_step_folder/\",\\n    },\\n)\\nEncapsulate a script that uses other existing functions of my repository\\nThe selected folders/files are copied from the repository to the step. The default selection is defined in the project parameters. If necessary, this can be changed to include other files from the repository or, on the contrary, to deselect them. The 2 conditions are :\\n\\nThe python script must be part of the selected folders\\nThe content of all the selected folders must be less than 5MB.\\nExample:\\n\\nsdk.create_step(\\n    function_path=\"hello_world.py\", # Path of were is the script file in your repo\\n    function_name=\"fct_hello\", # Name of function to run\\n    step_name=\"step_example\" # Unique name to identify your step in your environment\\n\\n    container_config = { \\n        # import folder \"/src\"  and file \"asset/data.csv\" from repo to step\\n        \"included_folders\": [\"src\", \"asset/data.csv\"] ,\\n    \"local_folder\": \"my_step_folder/\",\\n    },\\n)\\nHow I encapsulate a script with input and output ?\\nWhen I encapsulate the function in my script, it can request values as input parameters and return values as output with return. How do I interact between these elements and the platform?\\n\\nTo do this, we use the input and output system. These are elements that allow us to:\\n\\nTransmit data from the platform to the function parameters using inputs.\\nTransmit the data returned by the function to the platform using outputs.\\nNote that the inputs and outputs of a step are defined when it is created (see example below).\\n\\nInfo\\n\\nTo tell the platform where to fetch data for inputs and where to send data for outputs, we use mappings that we define when we deploy the pipeline. For more information, click here.\\n\\nWhich data types are available ?\\nHere is a list of possible types for inputs and outputs:\\n\\narray\\nboolean\\njson\\nnumber\\nstring\\nfile\\nNote that the input and output system automatically converts data into objects that can be used in Python. For example, JSON become Python dict, arrays become Python lists, etc.\\n\\nInfo\\n\\nThe files work a little differently, and if you need to access data from external Data Sources, more information here.\\n\\nExample :\\n\\nIn this example, we define two inputs of type number and JSON and an output of type array. The step can be represented as follows:\\n\\nstep_schema_workflow\\n\\nInfo\\n\\nIn this diagram, the arrows with \"Adapt process\" represent the automatic conversion of types by the platform mentioned earlier.\\n\\nOur source code, which will be contained in a function with two parameters and which returns a list in its return. Here is an example of python code in a script.py file:\\n\\ndef my_function(int_value, dictionary):\\n    # Print the parameters\\n    print(\"Parameter int:\", int_value)\\n    print(\"Parameter dict:\", dictionary)\\n\\n    # Create the array (list) using the values from the dictionary\\n    result = list(dictionary.values())\\n    result.append(int_value)\\n\\n    # Return the resulting array\\n    return result\\nOnce the script.py file has been sent to the repository, you can run the python code that uses the Craft AI SDK to create the inputs, outputs, and step:\\n\\n# Import and init craft AI SDK before \\n\\nstep_input1 = Input(\\n  name=\"int_value\", # same name as 1er parameter mandatory\\n    data_type=\"number\",\\n)\\n\\nstep_input2 = Input(\\n  name=\"dictionary\",# same name as 2nd parameter mandatory\\n    data_type=\"json\",\\n)\\n\\nstep_output = Output(\\n  name=\"result\",\\n    data_type=\"array\"\\n)\\n\\nsdk.create_step(\\n    function_path=\"script.py\",\\n    function_name=\"my_function\", \\n    step_name=\"step_A\",\\n    container_config={\\n      \"local_folder\": \"my_step_folder/\",\\n    },\\n\\n    inputs=[step_input1, step_input2],\\n    outputs=[step_output]\\n)\\nWarning\\n\\nBeware, for now all existing data types are not available yet (for example pd.DataFrame, pd.Series, etc.). You have to make sure you adapt your functionâ€™s code to be compatible.\\n\\nTo do so, you can choose from various available IO types.\\n\\nHow to have a default input value ?\\nIn order for certain step code to function correctly, it is possible that certain input values are mandatory in order to launch execution. There are 2 solutions for this:\\n\\nis_required: Used to make the input value mandatory in order to launch execution. If this is the case, execution will return an error without executing the code.\\ndefault_value: Used to give a default value to the input when the basic value used is empty.\\nInfo\\n\\nIf the value of an input is required and has a default value, then the default value will be used for execution in the event of a missing value.\\n\\nExample :\\n\\nFor this example, we\\'ll use the source code of the step we defined in the previous example. However, we\\'re going to modify the code calling the Craft AI platform to notify it :\\n\\nA default value for int_value.\\nThat the dictionary input is mandatory for executing the step\\n# Import and init craft AI SDK before \\n\\nstep_input1 = Input(\\n  name=\"int_value\", # same name as 1er parameter mandatory\\n    data_type=\"number\",\\n    default_value=12,\\n)\\n\\nstep_input2 = Input(\\n  name=\"dictionary\",# same name as 2nd parameter mandatory\\n    data_type=\"json\",\\n    is_required=True,\\n)\\n\\nstep_output = Output(\\n  name=\"result\",\\n    data_type=\"array\"\\n)\\n\\nsdk.create_step(\\n    function_path=\"script.py\",\\n    function_name=\"my_function\", \\n    step_name=\"step_A\",\\n    container_config={\\n      \"local_folder\": \"my_step_folder/\",\\n    },\\n\\n    inputs=[step_input1, step_input2],\\n    outputs=[step_output]\\n)\\nFull example\\nIn this example, we will adapt an ML application (image classification) with :\\n\\nPython dependencies\\nSystem dependencies\\nTwo string inputs (the access path to the image and the model in the data store)\\nOne string output (the classification result)\\nPython application source code :\\n\\nimport cv2\\nimport numpy as np\\nfrom tensorflow.keras.models import load_model\\n\\ndef perform_inference(image_path, model_path):\\n    # Load the pre-trained model\\n    model = load_model(model_path)\\n\\n    # Read the image using OpenCV\\n    image = cv2.imread(image_path)\\n\\n    # Preprocess the image (you may need to adjust this based on your model requirements)\\n    # Example: Resize the image to the input size expected by the model\\n    input_size = (224, 224)\\n    preprocessed_image = cv2.resize(image, input_size)\\n    preprocessed_image = preprocessed_image / 255.0  # Normalize pixel values to [0, 1]\\n\\n    # Perform inference using the loaded model\\n    prediction = model.predict(np.expand_dims(preprocessed_image, axis=0))\\n\\n    # Replace this with your post-processing logic based on the model output\\n    # For example, if it\\'s a classification model, you might want to get the class with the highest probability\\n    predicted_class = np.argmax(prediction)\\n\\n    return predicted_class\\n\\n# Example usage\\nimage_path = \\'path/to/your/image.jpg\\'\\nmodel_path = \\'path/to/your/model.h5\\'\\nresult = perform_inference(image_path, model_path)\\n\\nprint(f\"Predicted Class: {result}\")\\nRequirements.txt :\\n\\nnumpy\\ntensorflow\\nIf you want to test this script locally as if it were run in a step by the platform, you could use a bash script like this:\\n\\nBash command\\n\\n# Install OpenCV using apt-get\\n# Note: This assumes you are using a Debian-based system\\nsudo apt-get update\\nsudo apt-get install -y python3-opencv\\n\\n# Upgrade pip and install Python dependencies from requirements.txt\\npip install --upgrade pip\\npip install -r requirements.txt\\n\\n# Run app.py\\npython app.py\\nOr its equivalent with a Dockerfile :\\n\\nDockerfile\\n\\n# Use an official Python image as a parent image\\nFROM python:3.9-slim\\n\\n# Set the working directory to /app\\nWORKDIR /app\\n\\n# Copy the current directory contents into the container at /app\\nCOPY . /app\\n\\n# Install OpenCV using apt-get\\nRUN apt-get update && apt-get install -y \\\\\\n    python3-opencv\\n\\n# Install any Python dependencies from requirements.txt\\nRUN pip install --upgrade pip && \\\\\\n    pip install -r requirements.txt\\n\\n# Make port 80 available to the world outside this container\\nEXPOSE 80\\n\\n# Run app.py when the container launches\\nCMD [\"python\", \"app.py\"]\\nOnce the script app.py and the requirements.txt file are on the Git repository, all you have to do is use the Craft AI SDK to define the inputs and outputs, and create the step with the correct parameters.\\n\\n# Import and init craft AI SDK before \\n\\nstep_input1 = Input(\\n  name=\"image_path\", \\n    data_type=\"string\",\\n    is_required=True,\\n)\\n\\nstep_input2 = Input(\\n  name=\"model_path\",\\n    data_type=\"string\",\\n    is_required=True,\\n)\\n\\nstep_output = Output(\\n  name=\"result\",\\n    data_type=\"string\"\\n)\\n\\nsdk.create_step(\\n  function_path=\"app.py\",\\n  function_name=\"perform_inference\", \\n  step_name=\"step_B\",\\n    container_config = { \\n    \"requirements_path\": \"requirements.txt\",\\n    \"system_dependencies\": [\"python3-opencv\"],\\n    \"local_folder\": \"my_step_folder/\",\\n  },\\n  inputs=[step_input1, step_input2],\\n  outputs=[step_output]\\n)\\nThe values provided to the Python script during function calls (see code below) will be supplied at each execution and can be modified by the user calling the step. More details here.\\n\\n\\nRun and serve your pipelines\\nData scientists have a crucial role in the creation and implementation of machine learning models, converting advanced algorithms into useful applications. In tasks like data processing, training, scheduling, and model deployment, data scientists often face the challenge of initiating pipelines using different methods while efficiently handling various inputs. This adaptability is vital because it allows them to address a wide range of scenarios and demands.\\n\\nTo streamline this process, the MLOps platform provides a robust and user-friendly deployment features, empowering data scientists to execute and serve their pipelines efficiently.\\n\\nLearn about the various ways to execute your pipelines, which include:\\n\\nPipeline run\\nPipeline deployment, with 2 execution rules\\nEndpoint rule\\nPeriodic rule\\nOne of the fundamental elements provided by the MLOps platform to run and serve your models is the ability to customize the sources of the inputs you provide to a pipeline and the ways you want to retrieve your outputs. The main idea is to easily connect different data sources to your inputs (data directly coming from the final user, or coming from the environment for example) and destinations (delivering output to the final user, writing it in the data store ...). This is known as mapping in the platform, and it plays a central role when creating deployments or running pipelines.\\n\\nSummary\\nHow to execute your pipeline\\nHow to define sources and destinations for step inputs and outputs ?\\nPrerequisites\\nBefore using the Craft AI platform, make sure you have completed the following prerequisites:\\n\\nGet access to an environment\\nConnect the SDK\\nCreate a step\\nCreate a pipeline\\nInfo\\n\\nMake sure that your code inside your step with inputs/outputs is able to read inputs from function parameters and send outputs with the return of the function. More information here.\\n\\nHow to execute your pipeline\\nAs a data scientist, I want to be able to execute my Python code contained in my pipelines in various scenarios :\\n\\nI (or data scientist in my team) want to launch my pipeline on the fly via the craft AI SDK, so I can use the run pipeline.\\nI want to be able to make my pipeline accessible to external users, inside or outside my organization, via a Application Programmatic Interface (API), I can use the endpoint deployment feature to accomplish this.\\nI want my pipeline to be automatically execute at regular intervals without manual intervention, the periodic deployment feature can be employed.\\nRun your pipeline\\nThe run pipeline functionality enables you to trigger a pipeline directly from the Craft AI platform\\'s SDK. This option offers the advantage of being user-friendly and efficient.\\n\\nPlease note that the run pipeline feature is exclusively accessible through the Craft AI SDK and requires proper connection to the appropriate platform environment. It is recommended for conducting experiments and conducting internal testing purposes.\\n\\nTo execute a run on a pipeline with no input and no output, simply use this function:\\n\\n# Run pipeline function \\nsdk.run_pipeline(pipeline_name=\"your-pipeline-name\")\\nTo execute a run on a pipeline with inputs and outputs, you can use the same function and add the parameter inputs and give an object with inputs\\' names as keys (should be the same name as defined in input object give at the step creation) and values you want to provide to your step at execution:\\n\\n# Creating an object with predefined input values for the run,\\n# which will be provided as inputs to the run pipeline function\\ninputs_values = {\\n    \"number1\": 9, # Value for Input \"number1\" of the step (defined during step creation)\\n    \"number2\": 6 # Value for Input \"number2\" of the step (defined during step creation) \\n}\\n\\n# Running the Pipeline and Receiving Output\\noutput_values = sdk.run_pipeline(pipeline_name=\"your-pipeline-name\", inputs=inputs_values)\\n\\nprint (outputs_values[\"outputs\"])\\nFor example, you can obtain an output like this :\\n\\n>> {\\n    \\'output_name_1\\': \\'Lorem ipsum dolor sit amet, consectetur adipiscing elit.\\',\\n    \\'output_name_2\\': 42\\n }\\nThis function will return you the output of your pipeline. Like the inputs, itâ€™s represented as an object with the outputs\\' name as keys and the outputs values as values.\\n\\nWarning\\n\\nDonâ€™t forget to adapt your code to get value from input and return output. You must have created inputs and outputs objects at the step creation stage to get values. More information here.\\n\\nIt is also possible to run a pipeline with specific mapping (to connect inputs/outputs to the data store, environment variable, etc.), that will be covered in this section.\\n\\nCreate an endpoint\\nTriggering a pipeline via an endpoint will enable you to make your application available from any programming language/tool (website, mobile application, etc.).\\n\\nTo make your pipeline available via an endpoint, we need to create a deployment using the create_deployment() function which has the execution_rule parameter set as endpoint. In return, we\\'ll get the URL of the endpoint and its authentication token so that we can call it and trigger our pipeline.\\n\\n# Deployment creation with endpoint as execution rule\\nendpoint_info = sdk.create_deployment(\\n    execution_rule=\"endpoint\",\\n    pipeline_name=\"your-pipeline-name\",\\n    deployment_name=\"your-deployment-name\",\\n)\\nNote\\n\\nHere, we focus mainly on the platform\\'s SDK interface. However, it is possible to deploy a pipeline directly from the web interface by going to the Pipelines page, selecting a pipeline and then clicking the Deploy button.\\n\\nYou can trigger this endpoint from anywhere with any programming language if you have:\\n\\nEnvironment URL (Can be found in web UI and itâ€™s the same you have used to initiate your SDK)\\nDeployment name\\nEndpoint token (given as result of the deployment creation, which secures access to the endpoint)\\nTo trigger the deployed pipeline as an endpoint, you have a couple of options:\\n\\nUtilize the dedicated function provided by the Craft AI SDK.\\n\\n# Execution of pipeline using the endpoint\\nsdk.trigger_endpoint(\\n    endpoint_name=\"your-deployment-name\", \\n    endpoint_token=endpoint_info[\"endpoint_token\"] # Token recieve after deployment creation \\n)\\nImplement a HTTP request to the endpoint using any programming language, similar to the example shown with the command curl :\\n\\ncurl -X POST -H \"Authorization: EndpointToken <ENDPOINT_TOKEN>\" \"<CRAFT_AI_ENVIRONMENT_URL>/endpoints/<DEPLOYMENT_NAME>\"\\nAn other example with Javascript syntax (using axios) :\\n\\nconst axios = require(\\'axios\\');\\n\\nconst ENDPOINT_TOKEN = \\'<ENDPOINT_TOKEN>\\';\\nconst CRAFT_AI_ENVIRONMENT_URL = \\'<CRAFT_AI_ENVIRONMENT_URL>\\';\\nconst DEPLOYMENT_NAME = \\'<DEPLOYMENT_NAME>\\';\\n\\nconst headers = {\\n  \\'Authorization\\': `EndpointToken ${ENDPOINT_TOKEN}`\\n};\\n\\nconst url = `${CRAFT_AI_ENVIRONMENT_URL}/endpoints/${DEPLOYMENT_NAME}`;\\n\\naxios.post(url, null, { headers })\\n  .then(response => {\\n    console.log(\\'Response:\\', response.data);\\n  })\\n  .catch(error => {\\n    console.error(\\'Error:\\', error.message);\\n  });\\nInfo\\n\\nThe URL of your endpoint follows the structure : <CRAFT_AI_ENVIRONMENT_URL>/endpoints/<YOUR_DEPLOYMENT_NAME>\\n\\nThe inputs and outputs defined in your step are automatically linked to the deployment and consequently to the endpoint. You can therefore send a JSON within your request where the parameters correspond to the step inputs so that it can be used as a parameter for your function in the step. The deployment will return a similar object with your outputs as parameters.\\n\\nYou can call the endpoint with inputs using the SDK function or with any other programming language (like before) by specifying the inputs in JSON format in the request body.\\n\\nWith Craft AI SDK :\\n\\n# Value of inputs to be given to trigger pipeline function\\ninputs_values = {\\n    \"number1\": 9, # Input \"number1\" defined in step creation \\n    \"number2\": 6 # Input \"number2\" defined in step creation \\n}\\n\\n# Execution of pipeline using the endpoint\\nsdk.trigger_endpoint(\\n    endpoint_name=\"your-deployment-name\", \\n    endpoint_token=endpoint_info[\"endpoint_token\"] # Token recieve after deployment creation \\n    inputs=inputs_values\\n)\\nWith curl :\\n\\ncurl -X POST -H \"Authorization: EndpointToken <ENDPOINT_TOKEN>\" -H \"Content-Type: application/json\" -d \\'{\"number1\": 9, \"number2\": 6}\\' \"<CRAFT_AI_ENVIRONMENT_URL>/endpoints/<YOUR-DEPLOYMENT-NAME>\"\\nWith JavaScript using axios :\\n\\nconst axios = require(\\'axios\\');\\n\\nconst ENDPOINT_TOKEN = \\'<ENDPOINT_TOKEN>\\';\\nconst CRAFT_AI_ENVIRONMENT_URL = \\'<CRAFT_AI_ENVIRONMENT_URL>\\';\\nconst YOUR_DEPLOYMENT_NAME = \\'<YOUR-DEPLOYMENT-NAME>\\';\\n\\nconst requestData = {\\n  number1: 9,\\n  number2: 6\\n};\\n\\nconst config = {\\n  headers: {\\n    \\'Authorization\\': `EndpointToken ${ENDPOINT_TOKEN}`,\\n    \\'Content-Type\\': \\'application/json\\'\\n  }\\n};\\n\\nconst url = `${CRAFT_AI_ENVIRONMENT_URL}/endpoints/${YOUR_DEPLOYMENT_NAME}`;\\n\\naxios.post(url, requestData, config)\\n  .then(response => {\\n    console.log(\\'Response:\\', response.data);\\n  })\\n  .catch(error => {\\n    console.error(\\'Error:\\', error);\\n  });\\nInfo\\n\\nFor inputs and outputs, don\\'t forget to have adapted the step code, to have declared the inputs and outputs at step level and to have used types (string, integer, etc.) compatible with the data you are going to manipulate.\\n\\nPeriodic\\nYou might need to trigger your Python code regularly, whether it\\'s every X minutes, every hour, or at a specific date, automatically. To achieve this, you can create a periodic deployment for your pipeline. This type of deployment uses the CRON format for scheduling its triggers.\\n\\nTo set up a periodic deployment, you employ the same function as you would for endpoints. However, you specify the periodic trigger mode and define when it should trigger by providing a CRON rule in the schedule parameter.\\n\\ndeployment_info = sdk.create_deployment(\\n    execution_rule=\"periodic\",\\n    pipeline_name=\"your-pipeline-name\",\\n    deployment_name=\"your-deployment-name\",\\n    schedule=\"* * * * *\" # Will be executed every minute of every day \\n)\\nMore information and help about CRON here.\\n\\nHow to define sources and destinations for step inputs and outputs ?\\nIn the previous section, we saw that deployments can be used to trigger the execution of a pipeline, but they can also be used to give and receive information via inputs and outputs. When a pipeline is triggered, it may need to receive or send this information to different sources or destinations.\\n\\nExample:\\n\\nIn the diagram below, we assume that we have deployed an endpoint pipeline. An API is therefore available to the user who triggers the execution of the pipeline each time a request is sent to the API. The request can contain information required to execute the pipeline, just as the pipeline can send information back to the user, as shown in the diagram below.\\n\\nschema_sourcedest\\n\\nTo direct these flows to the right place, the platform allows you to map the step inputs and outputs to different sources and destinations using InputSource and OutputDestination objects. We\\'ll look at four different types of mapping:\\n\\nConstant mapping\\nEndpoint mapping (value or file)\\nEnvironment variable mapping\\nNone / void mapping\\nInputs and outputs are not compatible with all types of deployment. To make things clearer, here is a summary table:\\n\\nConstant\\tEndpoint value\\tEndpoint file\\tEnvironment variable\\tData store file\\tNone / void\\nRun\\tâœ…\\tâŒ\\tâŒ\\tâœ… (input only)\\tâœ…\\tâœ…\\nEndpoint\\tâœ…\\tâœ…\\tâœ… (limited to 1 file per call)\\tâœ… (input only)\\tâœ…\\tâœ…\\nPeriodic\\tâœ…\\tâŒ\\tâŒ\\tâœ… (input only)\\tâœ…\\tâœ…\\nConstant\\nIf I want my deployment to always use the same value as input, I can use a mapping to a constant.\\n\\nInfo\\n\\nNote that the same pipeline can have multiple deployments with multiple different constant values for the same input.\\n\\nWe will be using the constant value for an endpoint deployment here, but the process is the same for other types of deployment (periodic).\\n\\nTo do this, we create an InputSource object for each input in the pipeline that we want to deploy, specifying the name of the input for each mapping and the value of the input using the constant_value parameter.\\n\\nExample :\\n\\n# Endpoint Input mapping\\nendpoint_input1 = InputSource(\\n    step_input_name=\"number1\",\\n    constant_value=6,\\n)\\n\\nendpoint_input2 = InputSource(\\n    step_input_name=\"number2\",\\n    constant_value=3,\\n)\\n\\n# Deployment creation using inputSource object \\nendpoint_info = sdk.create_deployment(\\n    execution_rule=\"endpoint\",\\n    pipeline_name=\"your-pipeline-name\",\\n    deployment_name=\"your-deployment-name\",\\n    inputs_mapping=[endpoint_input1, endpoint_input2]\\n)\\nWarning\\n\\nStep must be created with one or more inputs and the code contained in my step must be suitable for receiving a constant. More information about it here.\\n\\nAll input types are compatible, except for file.\\n\\nEndpoint\\nAs explained above, when a deployment is of the endpoint type, the inputs and outputs of the associated steps have as their default source and destination the parameters of the HTTP request.\\n\\nYou can therefore send a JSON within your request where the keys correspond to the step inputs so that it can be used as parameters for your function in the step. The deployment will return a similar object with your outputs as keys.\\n\\nIf you need to change this default behavior, you can do so using InputSource and OutputDestination. You can define new names that will only be seen at the endpoint level for the external user. It allows you to specify under which names should inputs be passed in the request JSON by your final user or for the outputs, under which names they will be returned.\\n\\nThis input mapping works with any type of input or output (integer, file, string, etc.).\\n\\n# Endpoint Input mapping\\nendpoint_input1 = InputSource(\\n    step_input_name=\"number1\",\\n    endpoint_input_name=\"number_a\",\\n)\\n\\nendpoint_input2 = InputSource(\\n    step_input_name=\"number2\",\\n    endpoint_input_name=\"number_b\",\\n) \\n\\nendpoint_output = OutputDestination(\\n    step_output_name=\"number3\",\\n    endpoint_output_name=\"result\",\\n)\\n\\n# Deployment creation using 2 input mapping and 1 ouptut mapping\\nendpoint_info = sdk.create_deployment(\\n    execution_rule=\"endpoint\",\\n    pipeline_name=\"your-pipeline-name\",\\n    deployment_name=\"your-deployment-name\",\\n    inputs_mapping=[endpoint_input1, endpoint_input2],\\n    outputs_mapping=[endpoint_output]\\n)\\nNow that we have created our deployment, we can trigger the pipeline through the endpoint by passing the inputs and reading the outputs with the new mappings.\\n\\n# Value of inputs to be given to trigger pipeline function\\ninputs_values = {\\n    \"number_a\": 9, # Using mapping defined before linked to number1\\n    \"number_b\": 6 # Using mapping defined before linked to number2\\n}\\n\\n# Execution of pipeline using the endpoint\\nendpointOutput = sdk.trigger_endpoint(\\n    endpoint_name=\"your-deployment-name\", \\n    endpoint_token=endpoint_info[\"endpoint_token\"] # Token received after deployment creation \\n    inputs=inputs_values\\n)\\n\\n# Print result, note you should go into \"outputs\" before\\nprint (endpointOutput[\"outputs\"][\"result\"])\\nThese changes are effective for any endpoint trigger method (curl, JS, etc.).\\n\\nInfo\\n\\nObviously, this type of mapping is only available if the deployment is an endpoint.\\n\\nEnvironment variable\\nAs a data scientist, I may need to use data common for my entire environment in my pipeline. This can be achieved by mapping environment variables to inputs.\\n\\nTo do this, we also use the mapping system with the InputSource object. Environment variables are only available for inputs and not for outputs (but it is still possible to define them directly in the step code).\\n\\nFirst, let\\'s look at how to initialize the two environment variables (on the platform environment), we\\'re going to use:\\n\\n# Creation of env variable for input1\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"RECETTE_VAR_ENV_INPUT1\",\\n    environment_variable_value=6\\n)\\n\\n# Creation of env variable for input2\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"RECETTE_VAR_ENV_INPUT2\",\\n    environment_variable_value=4\\n)\\nNow, we can create an InputSource object and use it at the deployment creation.\\n\\n# Creation of InputSource to get env variable into input \\nendpoint_input1 = InputSource(\\n     step_input_name=\"number1\",\\n     environment_variable_name=\"RECETTE_VAR_ENV_INPUT1\",\\n)\\n\\nendpoint_input2 = InputSource(\\n     step_input_name=\"number2\",\\n     environment_variable_name=\"RECETTE_VAR_ENV_INPUT2\",\\n)\\n\\n# Endpoint creation\\nendpoint_info = sdk.create_deployment(\\n    execution_rule=\"endpoint\",\\n    pipeline_name=\"your-pipeline-name\",\\n    deployment_name=\"your-deployment-name\",\\n    inputs_mapping=[endpoint_input1, endpoint_input2]\\n)\\nThe deployment is created, we can trigger it without giving any object into data of HTTP request, it will take current environment variable value.\\n\\n# Value of inputs to be given to execute pipeline function\\ninputs_value = {\\n    \"number_a\": 9, # Using mapping defined before linked to number1\\n    \"number_b\": 6 # Using mapping defined before linked to number2\\n}\\n\\n# Execution of pipeline using the endpoint\\nsdk.trigger_endpoint(\\n    endpoint_name=\"your-deployment-name\", \\n    endpoint_token=endpoint_info[\"endpoint_token\"] # Token recieve after deployment creation \\n    inputs=inputs_value\\n)\\nData store\\nYou may need to transfer files between your data store and your step (in one direction or the other). To do this, you can also use inputs and outputs mapping system. You\\'ll find all the explanations you need on this page.\\n\\nVoid / None\\nAll step inputs and outputs have to be mapped when you deploy a pipeline with inputs and outputs (otherwise it will raise an error when creating the deployment). If you don\\'t really want to give/receive data in these inputs and outputs, you can map them to None. This will create a None object in Python for the inputs and send the outputs into the void.\\n\\n# Endpoint Input mapping\\nendpoint_input1 = InputSource(\\n    step_input_name=\"number1\",\\n    is_null=True,\\n)\\n\\nendpoint_input2 = InputSource(\\n    step_input_name=\"number2\",\\n    is_null=True,\\n) \\n\\nendpoint_output = OutputDestination(\\n     step_output_name=\"number3\",\\n     is_null=True\\n)\\n\\n# Deployment creation using 2 input mappings and 1 ouptut mapping\\nendpoint_info = sdk.create_deployment(\\n    execution_rule=\"endpoint\",\\n    pipeline_name=\"your-pipeline-name\",\\n    deployment_name=\"your-deployment-name\",\\n    inputs_mapping=[endpoint_input1, endpoint_input2],\\n    outputs_mapping=[endpoint_output]\\n)\\nNow that the deployment has been created, it can be triggered without giving or receiving any input or output.\\n\\n# Execution of pipeline using the endpoint who will return any output (object with value)\\nsdk.trigger_endpoint(\\n    endpoint_name=\"your-deployment-name\", \\n    endpoint_token=endpoint_info[\"endpoint_token\"] # Token received after deployment creation\\n)\\n\\n\\nUnderstanding ML executions\\nAs data scientists, the ability to comprehensively understand and monitor machine learning executions is paramount for delivering successful and impactful models.\\n\\nThis page is a resource that equips data scientists with the necessary knowledge and tools to track, monitor, and analyze the executions of their machine learning models on the Craft AI platform.\\n\\nThis page delves into essential topics, including obtaining execution details, tracking input and output, accessing metrics and logs, and comparing multiple executions. By mastering these techniques, data scientists can make informed decisions, optimize their models, and unlock the true potential of the MLOps platform.\\n\\nInfo\\n\\nOn this page, we will focus more on the pages available in the web interface. We\\'ll mention the SDK functions available without going into detail about them.\\n\\nTopics\\n\\nHow to find an execution and obtain the details ?\\nHow to compare multiple execution ?\\nHow to follow a pipeline in production ?\\nPrerequisites\\nBefore using the Craft AI platform, make sure you have completed the following prerequisites:\\n\\nGet access to an environment\\nConnect the SDK\\nCreate a step & pipeline\\nExecute this pipeline\\nWarning\\n\\nThe tensorflow library does not work with Python 3.12 and later versions yet. We strongly recommend you to use an older version of Python (such as 3.11) to complete these examples.\\n\\nOn this page, we\\'re going to focus on execution analysis using the platform\\'s web interface. For the example, I\\'m going to use a basic deep learning use case to visualize the associated information. You can use your own use cases, of course.\\n\\nStep code used in parts 1 and 2 (model training):\\n\\nimport tensorflow as tf\\nfrom tensorflow.keras import layers, models\\nfrom tensorflow.keras.datasets import fashion_mnist\\nfrom craft_ai_sdk import CraftAiSdk\\n\\ndef trainer(learning_rate=0.001, nb_epoch=5, batch_size=64) : \\n\\n    sdk = CraftAiSdk()\\n\\n    # Load and preprocess the MNIST dataset\\n    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\\n    train_images = train_images.reshape((60000, 28, 28, 1)).astype(\\'float32\\') / 255\\n    test_images = test_images.reshape((10000, 28, 28, 1)).astype(\\'float32\\') / 255\\n    train_labels = tf.keras.utils.to_categorical(train_labels)\\n    test_labels = tf.keras.utils.to_categorical(test_labels)\\n\\n    # Build the neural network model\\n    model = models.Sequential()\\n    model.add(layers.Conv2D(32, (3, 3), activation=\\'relu\\', input_shape=(28, 28, 1)))\\n    model.add(layers.MaxPooling2D((2, 2)))\\n    model.add(layers.Conv2D(64, (3, 3), activation=\\'relu\\'))\\n    model.add(layers.MaxPooling2D((2, 2)))\\n    model.add(layers.Conv2D(64, (3, 3), activation=\\'relu\\'))\\n    model.add(layers.Flatten())\\n    model.add(layers.Dense(64, activation=\\'relu\\'))\\n    model.add(layers.Dense(10, activation=\\'softmax\\'))\\n\\n    # Compile the model\\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\\n                loss=\\'categorical_crossentropy\\',\\n                metrics=[\\'accuracy\\'])\\n\\n    # Train the model\\n    for epoch in range(nb_epoch):  # Set the number of epochs\\n        model.fit(train_images, train_labels, epochs=1, batch_size=batch_size, validation_split=0.2)\\n\\n        # Evaluate the model on the test set and log test metrics\\n        test_loss, test_acc = model.evaluate(test_images, test_labels)\\n        print({\\'epoch\\': epoch + 1, \\'test_loss\\': test_loss, \\'test_accuracy\\': test_acc})\\n        sdk.record_list_metric_values(\"test-loss\",  test_loss)\\n        sdk.record_list_metric_values(\"test-accuracy\", test_acc)\\n\\n\\n    # Evaluate the model on the test set\\n    test_loss, test_acc = model.evaluate(test_images, test_labels)\\n    print(f\\'Test accuracy: {test_acc}\\')\\n    sdk.record_metric_value(\"accuracy\", test_acc)\\n    sdk.record_metric_value(\"loss\", test_loss)\\n\\n    # Save the model\\n    model.save(\\'mnist-model.keras\\')\\n    sdk.upload_data_store_object(\\'mnist-model.keras\\', \\'product-doc/mnist-model.keras\\')\\nStep code used in part 3 (inference with the model):\\n\\nimport tensorflow as tf\\nfrom tensorflow.keras.models import load_model\\nfrom PIL import Image\\nimport numpy as np\\nfrom craft_ai_sdk import CraftAiSdk\\nimport tensorflow as tf\\nfrom tensorflow.keras.datasets import fashion_mnist\\n\\ndef inference(image_num, model_path):\\n\\n    sdk = CraftAiSdk()\\n    ima_path = \\'./image_from_fashion_mnist.jpg\\'\\n\\n    # Load the Fashion MNIST dataset\\n    (train_images, train_labels), (vali_images, vali_labels) = fashion_mnist.load_data()\\n\\n    # save 1 image of validation dataset \\n    image_to_save = Image.fromarray(vali_images[image_num]) \\n    image_to_save.save(ima_path)    \\n\\n\\n    # Save model in local context and load it\\n    sdk.download_data_store_object(model_path, \"model.keras\")\\n    model = load_model(\"model.keras\")\\n\\n\\n    # Preprocess the input image\\n    input_image = tf.keras.preprocessing.image.load_img(ima_path, target_size=(28, 28), color_mode=\\'grayscale\\')\\n    input_image = tf.keras.preprocessing.image.img_to_array(input_image)\\n    input_image = np.expand_dims(input_image, axis=0)\\n    input_image = input_image.astype(\\'float32\\') / 255.0\\n\\n    # Make predictions\\n    predictions = model.predict(input_image)\\n\\n    # The predictions are probabilities, convert them to class labels\\n    predicted_class = int(np.argmax(predictions[0]))\\n\\n    if vali_labels[image_num] : \\n        if vali_labels[image_num] == predicted_class:\\n            sdk.record_metric_value(\"score\", 1)\\n        else :\\n            sdk.record_metric_value(\"score\", 0)\\n\\n    return {\"predicted_class\": str(predicted_class)}\\nHow to find an execution and obtain the details ?\\nWhen using the platform for experimentation or production, you can find the list of all your executions on the Execution > Execution Tracking page (remember to select a project first).\\n\\nOn this page you will find the list of all executions in all environments of the selected project. All executions are listed, whether in progress, failed or finished, whether triggered by a run, endpoint or CRON.\\n\\nWarning\\n\\nPlease note that deleting the pipeline or deployment will delete all attached executions.\\n\\nGet general information on an execution\\nOnce you are in the execution tracking page, you need to select an environment using the selector at the top left. Once the mouse is over the environment, you will see another popup on the right with two lists in a row:\\n\\nThe first contains the list of pipelines that have \\'run\\' attached to them.\\nThe second contains the list of deployments that have executed attached to them.\\nOnce a pipeline or deployment has been selected, the list of executions appears in the left-hand column, from the most recent to the oldest.\\n\\nTip\\n\\nYou can click on an environment directly to get all the associated executions.\\n\\nunderstand_ML_exec_1\\n\\nInfo\\n\\nYou can also retrieve all this information using the sdk.get_pipeline_execution(execution_id) function via the SDK.\\n\\nTrack input and output of execution\\nIf you want to see the inputs and outputs of a execution, you can view them in the tab of the same name. The inputs/outputs of the pipeline are displayed in a table with their:\\n\\nName\\nType\\nSource/destination type (where the value entered for this execution comes from)\\nSource/destination value (what is the value entered for this execution)\\nunderstand_ML_exec_2\\n\\nInfo\\n\\nFor the SDK, this information can be obtained using the function mentioned above sdk.get_pipeline_execution(execution_id).\\n\\nMore information can be obtained using the:\\n\\nsdk.get_pipeline_execution_input(execution_id, input_name)\\nsdk.get_pipeline_execution_output(execution_id, output_name)\\nGet metrics and logs of execution\\nIn the metrics tab, you can retrieve the pipeline metrics if you have defined them in your code.\\n\\nNote that the \\'simple metrics\\' are shown in a table, but the \\'lists metrics\\' are shown with graphs so that you can see how they change during execution. For example, here we follow the evolution of loss and accuracy over the epochs of our model training.\\n\\nunderstand_ML_exec_3\\n\\nInfo\\n\\nIt is also possible to retrieve this information from the SDK using the functions sdk.get_metrics(name, pipeline_name, deployment_name, execution_id) and sdk.get_list_metrics(name, pipeline_name, deployment_name, execution_id), more information here.\\n\\nFinally, the execution logs are also available in the associate tab. Note that the logs, like the other information, are not automatically reflected in the web interface, hence the buttons with arrows for refreshing the page.\\n\\nInfo\\n\\nHere again, an SDK function is available with sdk.get_pipeline_execution_logs(pipeline_name, execution_id, from_datetime, to_datetime, limit).\\n\\nHow to compare multiple executions?\\nCompare execution with a table of comparison\\nLet\\'s go back to the source code of the step from the beginning. This code is designed to train a deep learning model. This model has three hyper-parameters (the learning rate, the number of epochs, and the batch-size) which are associated with inputs to the step.\\n\\nWe can also see that in the step code, there are simple metrics and lists to track performance during and at the end of training.\\n\\nHere, we\\'ll vary the hyperparameters over several training sessions to find the best values. We\\'ll vary the learning rate and batch size with these values:\\n\\nLearning rate\\tNumber of epochs\\tBatch size\\n0.01\\t10\\t32\\n0.01\\t10\\t64\\n0.01\\t10\\t128\\n0.001\\t10\\t32\\n0.001\\t10\\t64\\n0.001\\t10\\t128\\n0.0001\\t10\\t32\\n0.0001\\t10\\t64\\n0.0001\\t10\\t128\\nEach line in this table represents an execution with its hyper-parameters and therefore a training run of the model.\\n\\nInstead of looking at the executions one by one in execution tracking, we go to Executions > Execution Comparison. Then select its environment, to finally see the table with all the executions.\\n\\nunderstand_ML_exec_4\\n\\nThere are 3 important elements on this page:\\n\\nThe table, the central element of the page. In this table, each row represents an execution, except for the first row, which is the column header. Each column represents information about the executions.\\nThese selectors can be used to add more or less information to the table, allowing inputs, metrics, etc. to be displayed or not. The more information you select, the more columns the table will have.\\nAnother tab is available on this page for viewing the metrics lists, but we\\'ll come back to this later.\\nTo start with, I\\'m going to select the meta-data, inputs, simple metrics, and list metrics. We don\\'t need the outputs in this case. If I want, I can even select precisely the inputs and metrics I\\'ve used in my executions.\\n\\nThen, in the header of the table, I\\'ll filter the pipeline names so that I only have the executions from the pipeline I\\'ve used.\\n\\nNote\\n\\nAll filter settings are available from the Filters button at the top right of the screen.\\n\\nFinally, I\\'ll sort according to precision by clicking on the little arrow in the column header.\\n\\nThat\\'s it, I\\'ve sorted my executions to find the parameters that give me the best accuracy for my model. In my case, the best result is obtained with a learning rate of 0.001 and a batch size of 128.\\n\\nunderstand_ML_exec_5\\n\\nWe could also have sorted according to metric lists, with the difference that you have to select your calculation mode before sorting. In fact, since we\\'re just displaying a number representing the list (with the average, the last number, the minimum, etc.), you do this by clicking on the tag at the top of the column (here with last in the screenshot below):\\n\\nunderstand_ML_exec_6\\n\\nCCompare the list metrics of several executions\\nIf you want to see all the values in the execution lists, you can also display the list metrics in their entirety to compare them between executions. To do this, select the executions you want to view using the eye on the left of the table, then go to the visualize tab (top right).\\n\\nNote\\n\\nOnly list metrics executions have a selectable eye.\\n\\nOn this screen, there is a graph for each available metrics list, and each execution is represented by a color. So you can compare the evolution of your metrics between each training session.\\n\\nunderstand_ML_exec_7\\n\\nInfo\\n\\nYou can hide executions by clicking on their names in the legend.\\n\\nHow to follow a pipeline in production?\\nPipeline monitoring\\nOnce our model has been trained and selected, we\\'re going to expose it in an endpoint so that it can be used from any application. To do this, we\\'re already going to need source code for an inference pipeline, this is the 2áµ‰ Python code given at the beginning. Note that this code reuses the validation dataset, to do the inference, for simplicity. We can, therefore, score each prediction and put the result in a score metric:\\n\\n1: The prediction is accurate\\n0: The prediction is false\\nWe create the step, the pipeline, and the endpoint deployment with the associated input/output. Our model is now ready to make predictions. For each prediction, we can track executions using the tools we\\'ve already seen.\\n\\nIn addition, you can also monitor our executions more globally over time by going to Monitoring > Pipeline metrics. On this page, you can see the evolution of the single metrics over time for any selected deployment.\\n\\nIn our case, we can track, execution after execution, the prediction score (true or false) of the model:\\n\\nunderstand_ML_exec_8\\n\\nInfo\\n\\nYou can select a date range in the top right-hand corner. You can also zoom in on a selected range in the graph.\\n\\nResource monitoring\\nWhen you have several models in production (or even in training), you may want to have information about the health of your environment\\'s infrastructure. This helps to ensure that the size of the environment corresponds to the computation/storage requirements and also to identify any problems.\\n\\nThe Monitoring > Resource metrics page allows you to see any over-use of resources (with the cards at the top), as well as their evolution over time (with the graphs), once an environment has been selected.\\n\\nunderstand_ML_exec_9\\n\\nAs a reminder, an environment is made up of workers, which are the calculation units within the environment. Each worker, therefore, has a dedicated curve (selectable from the legend) plus a curve for all the cumulative workers.\\n\\nYou can download this data in .csv format using the download button. The data downloaded will be that selected by date and worker (as for the graphs).\\n\\nInfo\\n\\nIt is also possible to retrieve data with the SDK using the sdk.get_resource_metrics(start_date, end_date, csv) function.\\n\\nThe csv (binary) parameter can be used to retrieve data in .csv format, like the button, or in a Python dictionary.\\n\\nConclusion\\nIn this page, we have covered essential aspects of tracking, monitoring, and analyzing machine learning executions on the Craft AI platform. By understanding how to find and obtain details about executions, track input and output, retrieve metrics and logs, and compare multiple executions, data scientists can effectively leverage the platform\\'s capabilities.\\n\\nAdditionally, we explored the process of following a pipeline in production, including pipeline monitoring and resource monitoring. These practices ensure that models are deployed and running smoothly, with the ability to assess performance and resource utilization over time.\\n\\nBy mastering these tools and techniques, data scientists can make informed decisions, optimize models, and achieve the full potential of the MLOps platform provided by Craft AI.\\n\\nDeploy in low-latency\\nSummary\\nWhat is low-latency mode?\\nDeployment Creation Step by Step\\nMonitoring Low Latency\\nPreloading data for low-latency\\nWhat is low-latency mode?\\nIntroduction\\nThe Craft AI platform offers two pipeline deployment modes:\\n\\nElastic: This is the default mode. It emphasizes simplicity of use.\\nLow-latency: This mode is designed to achieve faster pipeline execution times.\\nA pipeline deployment always has a mode, and an execution rule such as \"endpoint\" or \"periodic\". More detail about deployment on this page.\\n\\nBefore delving into how the low-latency mode operates, let\\'s establish some key points about the deployment modes.\\n\\nNote\\n\\nThe new deployment method does not give rise to any additional financial costs, as it remains in the same environment.\\n\\nElastic Mode\\nThis is the default mode for deployments. In this mode, executions are stateless.\\n\\nThis means that executions in Elastic mode are self-contained and independent from each other, since a unique temporary container is created for each execution. In this mode, executions use all the available computing resources automatically, and no resource is used when there is no execution in progress.\\n\\nAdvantages:\\n\\nAutomatic resource management\\nNo memory side effects executions\\nDisadvantage:\\n\\nSlower individual execution time\\nTechnical information\\n\\nIn this context, a \"container\" refers to a pod in Kubernetes. This mode creates a pod with the required dependencies for each execution, executes the code, and then destroys the pod. This approach enables stateless execution but contributes to an increase in execution time.\\n\\nLow-latency Mode\\nWith this mode, an execution container called a pod is initialized ahead of time, ready for executions.\\n\\nAs a result, the execution time in this mode is faster than in elastic mode. It is designed for use cases that require fast response times or have long initialization times.\\n\\nNote\\n\\nThe faster execution time in this mode comes from gains in how executions are started and stopped, it does not mean that computation is faster. The time taken to compute the same code during an execution in both modes remains the same, depends on the computation resources of the environment.\\n\\nAll executions for a low-latency deployment share the same pod, where memory is shared between executions.\\n\\nA low-latency deployment pod uses computation resources from the environment, even without any execution in progress. And all executions for a deployment run on the same pod. So you need to manage the computation resources of the environment.\\n\\nAdvantage:\\n\\nFaster execution time\\nDisadvantages:\\n\\nManual resource management\\nMemory side effects between executions\\nTechnical details\\n\\nWhen creating a low-latency deployment, the associated pod is created before any execution can start. It starts the process where executions will run, with the dependencies required to run your code. A pod can only handle one execution at a time, but Python global variables are shared between executions.\\n\\nInfo\\n\\nThe support for multiple pods per deployment or multiple executions per pod is coming soon.\\n\\nSummary\\nFor real-time response, use low-latency mode. Otherwise, keep the default mode, elastic mode.\\n\\nIt is important to note that selecting low-latency mode results in a shared execution context between executions in the same deployment, and in a continuously active pod, which requires monitoring resource usage throughout the deployment\\'s lifespan.\\n\\nNote\\n\\nThe run_pipeline() function does not create a deployment, but its behavior is similar to that of the elastic deployment mode.\\n\\nschema_low-latency\\n\\nDeployment Creation\\nIn this section, we\\'ll look at the steps involved in creating a low-latency deployment using the Craft AI SDK.\\n\\nNote\\n\\nIf you have already initialized your SDK with your environment and are familiar with the creation and use of elastic deployment, this section is not applicable. Otherwise, please refer to the relevant documentation here.\\n\\nTo achieve our first low-latency deployment, we will utilise a basic Python script that multiplies two numbers:\\n\\nmultipli.py\\ndef entryStepMultipli(number1, number2):\\n    return {\"resultMulti\": number1 * number2}\\nWarning\\n\\nRemember to push this source code to Git so that the platform can access the Python script for execution.\\n\\nThe approach for creating the step and associated pipeline remain the same, regardless of the chosen deployment mode:\\n\\n# IO creation \\nstep_input1 = Input(\\n    name=\"number1\",\\n    data_type=\"number\",\\n)\\n\\nstep_input2 = Input(\\n    name=\"number2\",\\n    data_type=\"number\",\\n)\\n\\nstep_output1 = Output(\\n    name=\"resultMulti\",\\n    data_type=\"number\",\\n)\\n\\n# Step creation\\nsdk.create_step(\\n    function_path=\"src/multipli.py\",\\n    function_name=\"entryStepMultipli\",\\n    step_name=\"multi-number-step\",\\n    container_config = { \\n    \"local_folder\": \"my_step_folder/\",\\n  },\\n\\n    inputs=[step_input1, step_input2],\\n    outputs=[step_output1],\\n)\\n\\n# Pipeline creation \\nsdk.create_pipeline(pipeline_name=\"multi-number-pipl\", step_name=\"multi-number-step\")\\nThe mode parameter is initially set to low_latency upon creation.\\n\\nHowever, it takes a few tens of seconds for the deployment to become active. You can use a loop to wait for it to be ready, as shown here.\\n\\n# Deployment creation \\nendpoint = sdk.create_deployment(\\n    execution_rule=\"endpoint\",\\n    pipeline_name=\"multi-number-pipl\",\\n    deployment_name=\"multi-number-endpt\",\\n    mode=\"low_latency\",\\n)\\n\\n# Waiting loop until deployment is ready \\nstatus = None\\nwhile status != \\'success\\':\\n    status = sdk.get_deployment(\"multi-number-endpt\")[\\'status\\']\\n    if status != \\'success\\':\\n        print(\"waiting endpoint ready...\", sdk.get_deployment(\"multi-number-endpt\")[\\'status\\'])\\n        time.sleep(5)\\ndeploi_info = sdk.get_deployment(\"multi-number-endpt\")\\nprint(deploi_info)\\nAfter deployment, it operates like an elastic deployment. It can be triggered through the SDK or other methods such as Postman, curl, or JavaScript requests:\\n\\n\\nPython SDK\\nCurl\\nJavascript\\nsdk.trigger_endpoint(\\n    endpoint_name=deploi_info[\"name\"],\\n    endpoint_token=deploi_info[\"endpoint_token\"],\\n    inputs={\"number1\": 3, \"number2\": 4},\\n    wait_for_results=True,\\n)\\n\\nTip\\n\\nAs with elastic deployments, low-latency deployments linked to the pipeline can be viewed on the pipeline page of the web interface, along with the relevant information and executions.\\n\\nMonitoring Low Latency\\nAs previously mentioned, deploying with low-latency introduces additional complexity. To effectively monitor deployment activity, the platform offers various information:\\n\\nStatus: Information on the deployment lifecycle at a given point in time.\\nLogs: Historical and detailed information on the deployment lifecycle\\nVersion: Information on the deployment update\\nStatus\\nLow latency deployments have two additional specific statuses:\\n\\ndeployment_health: Represents the potential availability of the deployment. If enabled and this status is set to Ready, then the deployment is ready to receive requests.\\nstatus: Represents the loading of a pod into the deployment. Note that this does not necessarily correlate with the health of the deployment.\\nWarning\\n\\nThese two statuses are different from the \\'is_enabled\\' parameter, which represents the user\\'s chosen deployment availability.\\n\\nThese statuses are available in the return object of the get_deployment() function:\\n\\nsdk.get_deployment(\\n    deployment_name=\"multi-number-endpt\"\\n)\\nNote\\n\\nThe pod has a specific status in addition to that of deployment.\\n\\nDeployment logs\\nDuring its lifetime, a low-latency deployment generates logs that are not specific to any execution but are linked to the deployment itself. You can use the get_deployment_logs() function in the SDK to get them.\\n\\nfrom datetime import datetime, timedelta\\n\\nsdk.get_deployment_logs(\\n    deployment_name=\"multi-number-endpt\",\\n    from_datetime=datetime.now() - timedelta(hours=2), \\n    to_datetime=datetime.now(), \\n    type=\"deployment\",\\n    limit=None\\n)\\nDeployment update\\nA low-latency deployment can be used to reload the associated pod. To do this, you can call the SDK\\'s update_deployment() function:\\n\\nsdk.update_deployment(\\n    deployment_name=\"multi-number-endpt\"\\n)\\nPreloading data for low-latency\\nWarning\\n\\nConfiguration on demand is an incubating feature.\\n\\nConcept\\nWhen using low-latency mode, it is important to note that this implies continuity between executions. This is because the pod that encapsulates the executions remains active throughout the lifetime of the deployment.\\n\\nAs a result, there is memory permeability between executions. Each execution runs in a different thread in the same process. While this feature can be advantageous, it must be used with care. It allows data to be loaded into memory (RAM and VRAM) prior to an execution by using global variables in Python.\\n\\nHow to do that\\nThe code, specified in the function_path property, when the step was created, is imported during the creation of a low-latency deployment. This enables the loading of variables prior to the first deployment run.\\n\\nNote\\n\\nA global variable can also be defined â€œonlyâ€ in the first execution by creating it only in the function.\\n\\nOnce the data has been loaded into a global variable, it can be read in function executions.\\n\\nNote: That this does not require any changes to the creation of platform objects (step, pipeline, etc.) using the SDK.\\n\\nWarning\\n\\nIf the pod is restarted (after a standby, for example), the loaded data is reset as when the deployment was created.\\n\\nExamples\\nSimple example\\n# Import lib \\nfrom craft_ai_sdk import CraftAiSdk\\nimport os, time\\n\\n# Code run at the low latency deployment creation \\ncount = 0 \\nloaded_data = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\\n\\nprint (\"Init count at : \" + count)\\n\\n# Function who will be run at each execution \\ndef my_step_source_code():\\n    global loaded_data, count\\n\\n    count += 1 \\n\\n    print (count, loaded_data)\\nDeployment logs and logs of the first 2 runs :\\n\\nDeployment\\'s logs : \\n\\n> Pod created successfully\\n> Importing step module\\n> Init count at : 0\\n> Step module imported successfully\\n> Execution \"my-pipeline-1\" started successfully\\n> Execution \"my-pipeline-2\" started successfully\\n\\n\"my-pipeline-1\" logs :\\n\\n> 1 [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\\n\\n\"my-pipeline-2\" logs :\\n\\n> 2 [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\\nNote\\n\\nYou can access to deployment logs by using the SDK function sdk.get_deployment_logs().\\n\\nExample of usage:\\n\\nlogs_deploy = sdk.get_deployment_logs(\\n    deployment_name=\"my-deployment\",\\n)\\n\\nprint(\\'\\\\n\\'.join(log[\"message\"] for log in logs_deploy))\\nExample of a step with LLM preloading\\nimport time\\nfrom vllm import LLM, SamplingParams\\nfrom craft_ai_sdk import CraftAiSdk\\nimport os\\nfrom io import StringIO\\n\\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=1024)\\nllm = LLM(\\n    model=\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\",\\n    quantization=\"awq\",\\n    dtype=\"half\",\\n    max_model_len=16384,\\n)\\n\\ndef persitent_data_step(message: str):\\n    global llm\\n\\n    output = llm.generate([message], sampling_params)[0]\\n\\n    return {\\n        \"results\": output.outputs[0].text,\\n    }\\n\\n\\nConnect a Git repository to the platform\\nThe basic method to retrieve the source code of a pipeline is by specifying a local file that contains the script in .py format. However, it is also possible to retrieve the script directly from a Git repository (GitHub or GitLab). Connecting the Git repository to your platform offers several advantages over using local files.\\n\\nThis documentation explains the benefits of this integration and the steps required to set up this connection securely and efficiently.\\n\\nWhy connect a Git repository?\\nLimitations of using local files\\nTypically, the platform is used solely with local files for sharing the source code of pipelines. This approach has several drawbacks:\\n\\nLack of versioning: It is difficult to track changes made to the code.\\nLimited sharing: Sharing code with other team members is complex and inconvenient.\\nPoor practice for production: Working with local files is not recommended for production environments due to the lack of controls and security measures.\\nProposed solution\\nTo overcome these limitations, we propose reading the code directly from a Git repository. This allows for easier versioning and code sharing. This practice is strongly encouraged for production projects, but not exclusively.\\n\\nTo achieve this:\\n\\nThe user must create a deployment key.\\nThe public key must be added to the Git repository.\\nThe private key must be shared with the platform so it can access the repository.\\nLet\\'s see how to do this in detail.\\n\\nHow to connect Git repository ?\\nProviding information to the Git repository\\nTo allow the Git repository to recognize and authorize connections from the platform, it is necessary to provide an SSH key pair:\\n\\nStep 1 : Generating the deploy key\\nFor security reasons, to get access to your Git repository, the platform uses a Deploy Key with the RSA SSH KEY standard. The deploy key is a special key that grants access to a specific repository; it is not the same as personal keys used commonly by users to access their repositories, although they are both SSH keys.\\n\\nThe deploy key has two elements:\\n\\nThe public key, which must be set in the GitHub administration settings for the repository.\\nThe private key, which must be sent to the Craft AI MLOps Platform, so it can access the repository.\\nFirst, you will need to generate an SSH key on your computer:\\n\\nOn Linux and macOS\\nOn window\\nStep 2 : Adding the public key to the Git repository\\nNow, you have to add the public key to the Git repository settings as a deployment key.\\n\\nFor GitHub :\\n\\nHead to the homepage of your repository on GitHub.\\n\\nGo to the Settings page.\\n\\nOnce there, select the tab on the left named Deploy Keys\\n\\nSelect Add deploy key on the Deploy Keys page.\\n\\ndeploy_key\\n\\nInsert the name you want for your deploy key\\n\\nCopy/paste the public key (content of *your-key-filename*.pub) in the second text box.\\n\\nClick on â€œAdd keyâ€ (you donâ€™t need to allow write access)\\n\\nFor GitLab :\\n\\nHead to the homepage of your repository on GitLab.\\nClick on Settings (left bar) then go to Repository\\nClick on Expand in the Deploy keys section\\nInsert the name you want for your deploy key.\\nCopy/paste the public key (content of *your-key-filename*.pub) in the second text box.\\nClick on Add key (you donâ€™t need to Grant write permissions to this key)\\nProviding Information to the platform\\nThere are two methods to integrate the SSH key on the platform side:\\n\\nIn the step creation information\\nIn the project information\\nStep 3a : During step creation\\nParameters can be configured during each step creation just like with a local folder. To do this, you need to add the necessary information (repository URL, branch, private key) directly into the container configuration.\\n\\nLet\\'s take this file structure as an example:\\n\\n.\\nâ”œâ”€â”€ sdk-platform-script.py\\nâ”œâ”€â”€ my-git-repo/\\nâ”‚   â”œâ”€â”€ README.md\\nâ”‚   â”œâ”€â”€ requirements.txt \\nâ”‚   â””â”€â”€ src/\\nâ”‚       â””â”€â”€ my-source-code.py\\nâ””â”€â”€ keys/\\n    â”œâ”€â”€ my-key-filename\\n    â””â”€â”€ my-key-filename.pub\\nThe script sdk_platform_script.py is responsible for creating platform objects using the SDK. The file my_source_code.py contains the Python code that will be executed in the pipeline, and the keys directory contains the previously generated keys.\\n\\nWhen creating a step, you can configure your SDK script in this way to create a step from the code in the Git repo:\\n\\nsdk-platform-script.py\\nwith open(\\'keys/my-key-filename\\', \\'r\\') as file:\\n    private_key_value = file.read().rstrip()\\n\\n\\nsdk.create_step(\\n    function_path=\"src/my-source-code.py\",\\n    function_name=\"my-function-name\", \\n    step_name=\"my-step-name\",\\n    container_config = {\\n        \"repository_url\": \"git@github.com:my-account/my-git-repo.git\",\\n        \"repository_deploy_key\": private_key_value,\\n        \"repository_branch\": \"main\"\\n    }\\n)\\nStep 3b : In project information\\nAlternatively, you can provide the repository information in the project settings. Once configured, if no additional information is provided during the pipeline creation, it will use the default project settings.\\n\\nTo do this, go to your project\\'s settings page and enter these 3 parameters:\\n\\nRepository URL : Enter the SSH URL of your repository.\\n\\nHow to get my repository URL\\nDeploy key : Enter your Github / GitLab private key.\\n\\nWarning\\n\\nRemember to keep the begin and end tags when you copy/paste the key. It should look like this :\\n\\n-----BEGIN RSA PRIVATE KEY-----\\nMIIJKQIBFSKCAgEAwH/zbeYm3M7elJHIjQTiO2+2QdTOh3ebvZotNQNATJ4UIqVN\\nT9P2xN3Xd/27w8/jv9wmGqHzSVyEo53FfnyDm2zlFvqImRZm3znujA9bbp00itB5\\n...\\nBo1gJMJxYJ4npi+0VULc33Ao6FzOfGxSACoTA/gG/q7LHO68c6Zgz+dI/ekDqG7C\\nGx52WhCP26GdneD/EhPgcUh41FzDbgO2BBIboNnrJLQzSQboK8JNrsislPr7\\n-----END RSA PRIVATE KEY-----\\nDefault branch : Enter the Git branch you want as default for this project. If this field is empty, we will use the default Git branch. It will be possible to choose a different default branch within an environment.\\n\\nOnce this project information is saved, you can set up your steps so that the platform will use your Git repository by default:\\n\\nsdk.create_step(\\n    function_path=\"src/my-source-code.py\",\\n    function_name=\"my-function-name\", \\n    step_name=\"my-step-name\"\\n)\\nNote\\n\\nIn the case where the Git repo information has been defined in the environment settings page as well as in the step creation function, the information defined in the step creation function takes priority.\\n\\n\\nParallelism Deployment\\nExplanation\\nParallelism in deployment allows multiple tasks or executions to run concurrently on the same pod, improving the efficiency and speed of operations. This is particularly useful in data processing, machine learning inference, and other computationally intensive tasks where waiting for one task to complete before starting another can lead to significant delays.\\n\\nNote\\n\\nThe platform offers two types of parallelism:\\n\\nSimple Parallelism: This allows deploying Python code in parallel without any changes; however, it is not compatible with all Python libraries.\\nAdvanced Parallelism: More robust, it is compatible with the majority of Python libraries but requires modifications to your source code.\\nWe\\'ll go into more detail later.\\n\\nAdvantages\\nReduced Waiting Time: Users experience less waiting time for deployments to complete.\\nImproved Resource Utilization: Better usage of available computational resources, ensuring that they are not idle while waiting for other tasks to complete.\\nScalability: Parallel executions enable the system to handle larger workloads by distributing tasks across multiple threads or asynchronous operations.\\nWarnings\\nComplexity: Managing parallel executions adds complexity to the deployment process, requiring careful handling of shared resources and potential concurrency issues.\\nThread Safety: Only thread-safe code is guaranteed to run in parallel without errors. Non-thread-safe code may lead to race conditions and other concurrency issues.\\nShared Execution Space: When using parallel executions, all tasks share the same execution space, which can lead to conflicts.\\nEnvironment Variables: Environment variables can have unexpected values if they are modified during execution by another parallel task.\\nResource Sharing: Global variables, GPU memory, and disk files are shared between multiple executions, so you must be mindful of potential race conditions and concurrency issues.\\nLog Management: Using some libraries with async/threaded methods in your code may cause logs to be associated with the wrong running execution. Logs are associated with executions through Python contextvars.\\nHow to Deploy with Parallelism\\nExample of SDK Code\\nTo create a deployment with parallelism enabled, use the create_deployment function from the CraftAiSdk. Below are examples of how to configure this:\\n\\n# Setting a specific number of parallel executions\\nsdk.create_deployment(\\n    execution_rule=\"endpoint\",\\n    pipeline_name=\"my_pipeline_name\",\\n    deployment_name=\"my_deployment_name\",\\n    mode=\"low_latency\",\\n    enable_parallel_executions=True,  # Default is False\\n    max_parallel_executions_per_pod=10  # Default is 6\\n)\\nNote\\n\\nIf the number of concurrent executions exceeds the maximum allowed (max_parallel_executions_per_pod), additional executions will be queued until a slot becomes available.\\n\\nTip\\n\\nYou can also create your deployment in the Web UI and enable parallelism from there (an option available in the advanced settings on the deployment creation page).\\n\\nSimple Parallelism\\nExplanation\\nSimple parallelism utilizes Python\\'s threading capabilities to run multiple threads concurrently. This allows tasks to be executed in parallel without changes to the existing codebase. Some libraries commonly used in data science do not work with this parallelism mode, so be careful to take it into account.\\n\\nAdvantages:\\n\\nMinimal Code Changes: Requires little to no changes to existing code.\\nEase of Implementation: Easy to implement and use for tasks that are I/O bound or where the GIL (Global Interpreter Lock) is not a major concern.\\nDisadvantages:\\n\\nLimited Compatibility: Not all libraries are thread-safe and may not work correctly with threading.\\nGIL Restrictions: Python\\'s GIL can limit the effectiveness of threading for CPU-bound tasks.\\nHello world use case\\nHere is an example of how to use threading for simple parallelism:\\n\\nSource code of the pipeline\\ncode_step.py\\n# This script defines a function `my_fct` which prints the current time,\\n# waits for 5 seconds, and then prints the time again.\\nfrom datetime import datetime\\nimport time\\n\\ndef my_fct():\\n    print(f\"Hello: {datetime.now()}\")\\n    time.sleep(5)\\n    print(f\"World: {datetime.now()}!\")\\nCreation of the pipeline and deployment\\nscript_sdk_create_obj.py\\n# This script creates a pipeline and a deployment using the Craft AI SDK. \\n# It uploads the code defined in `code_step.py` and sets up parallel executions for the deployment.\\nfrom craft_ai_sdk import CraftAiSdk\\n\\n# Initialize the Craft AI SDK with the specified environment URL and token.\\nsdk = CraftAiSdk(environment_url=\"my-environment-url\", sdk_token=\"my-sdk-token\")\\n\\n# Pipeline creation as usual \\nsdk.create_pipeline(\\n    function_path=\"code_step.py\",\\n    function_name=\"my_fct\", \\n    pipeline_name=\"my-pipeline-name\",\\n    container_config = { \\n        \"local_folder\": \"src/\",\\n    },\\n)\\n\\n# Deployment creation with parallelism enabled \\nsdk.create_deployment(\\n    execution_rule=\"endpoint\",\\n    pipeline_name=\"my-pipeline-name\",\\n    deployment_name=\"my-deployment-name\",\\n    mode=\"low_latency\", # Mandatory, parallelism only work on low-latency mode\\n\\n    enable_parallel_executions=True,\\n    max_parallel_executions_per_pod=5\\n)\\nTrigger the deployment\\nscript_call_pipeline.py\\n# This script makes multiple concurrent requests to a deployment on the Craft AI platform.\\nfrom craft_ai_sdk import CraftAiSdk\\n\\ndeployment_name = \"my-deployment-name\" # Define the deployment name and the number of requests to be launched.\\nnum_requests = 20  # Number of concurrent requests to launch.\\n\\n# Initialize the Craft AI SDK with the specified environment URL and token.\\nsdk = CraftAiSdk(environment_url=\"my-environment-url\", sdk_token=\"my-sdk-token\")\\n\\n# Retrieve the endpoint information for the given deployment name using the SDK.\\nendpoint_info = sdk.get_deployment(deployment_name)\\n\\nfor _ in range(num_requests):\\n    # Launch executions simultaneously on the platform, results available on the execution tracking page \\n    sdk.trigger_endpoint(deployment_name, endpoint_info[\"endpoint_token\"], wait_for_results=False)\\nNote\\n\\nOnce launched, the executions are visible in execution tracking and will be processed simultaneously in groups of 5.\\n\\nAdvanced Parallelism\\nExplanation\\nAdvanced parallelism leverages Python\\'s AsyncIO library to manage asynchronous tasks. This approach is more suited for tasks that require concurrent I/O operations, such as network requests or reading from multiple sources simultaneously.\\n\\nAsyncIO is a library designed to handle asynchronous operations using the async/await syntax.\\n\\nThe platform automatically detects and implements asynchronous parallelism based on the userâ€™s source code, so you don\\'t need to add anything extra during deployment creation.\\n\\nTo trigger advanced parallelism, you just need to add the async keyword to the entry function of your step code. Then, you can use the await keyword where necessary.\\n\\nExample of usage out of the platform:\\n\\nimport asyncio\\n\\nasync def greet(name):\\n    print(f\"Hello, {name}\")\\n    await asyncio.sleep(1)\\n    print(f\"Goodbye, {name}\")\\n\\nasync def main():\\n    await asyncio.gather(\\n        greet(\"Alice\"),\\n        greet(\"Bob\"),\\n        greet(\"Charlie\")\\n    )\\n\\n# Running the main function on a local device\\nasyncio.run(main())\\n>>> Hello, Alice\\n>>> Hello, Bob\\n>>> Hello, Charlie\\n>>> Goodbye, Alice\\n>>> Goodbye, Bob\\n>>> Goodbye, Charlie\\nAdvantages:\\n\\nBroad Library Support: Works with a wide range of libraries and frameworks.\\nEfficient I/O Handling: Ideal for tasks that involve I/O operations.\\nDisadvantages\\n\\nCode Adaptation: Requires changes to the code to use async/await syntax.\\nLearning Curve: May have a steeper learning curve compared to threading.\\nUse Cases hello world\\nLet\\'s take the previous example of a \"hello world\" and adapt it to the asyncio library.\\n\\nSource code of the pipeline\\ncode_step.py\\n# This script defines an asynchronous function `my_fct` which prints the current time,\\n# waits for 5 seconds asynchronously, and then prints the time again.\\nfrom datetime import datetime\\nimport time\\nimport asyncio\\n\\nasync def my_fct():\\n    print(f\"Hello: {datetime.now()}\")\\n    await asyncio.sleep(5)\\n    print(f\"World: {datetime.now()}!\")\\nWarning\\n\\nIt\\'s the only code that needs to be changed compared to the simple parallelism example.\\n\\nThree changes took place:\\n\\nAddition of the import statement for the asyncio library.\\nAddition of the async keyword before the function definition.\\nReplacement of the time.sleep(5) function with asyncio.sleep(5). Be careful to include the await keyword before the function.\\nCreation of the pipeline and deployment\\nTip\\n\\nIt\\'s exactly the same code as the simple parallelism example.\\n\\nscript_sdk_create_obj.py\\n# This script creates a pipeline and a deployment using the Craft AI SDK. \\n# It uploads the code defined in `code_step.py` and sets up parallel executions for the deployment.\\nfrom craft_ai_sdk import CraftAiSdk\\n\\n# Initialize the Craft AI SDK with the specified environment URL and token.\\nsdk = CraftAiSdk(environment_url=\"my-environment-url\", sdk_token=\"my-sdk-token\")\\n\\n# Pipeline creation as usual \\nsdk.create_pipeline(\\n    function_path=\"code_step.py\",\\n    function_name=\"my_fct\", \\n    pipeline_name=\"my-pipeline-name\",\\n    container_config = { \\n        \"local_folder\": \"src/\",\\n    },\\n)\\n\\n# Deployment creation with parallelism enabled \\nsdk.create_deployment(\\n    execution_rule=\"endpoint\",\\n    pipeline_name=\"my-pipeline-name\",\\n    deployment_name=\"my-deployment-name\",\\n    mode=\"low_latency\", # Mandatory, parallelism only work on low-latency mode\\n\\n    enable_parallel_executions=True,\\n    max_parallel_executions_per_pod=5\\n)\\nTrigger the deployment\\nTip\\n\\nIt\\'s exactly the same code as the simple parallelism example.\\n\\nscript_call_pipeline.py\\n# This script makes multiple concurrent requests to a deployment on the Craft AI platform.\\nfrom craft_ai_sdk import CraftAiSdk\\n\\ndeployment_name = \"my-deployment-name\" # Define the deployment name and the number of requests to be launched.\\nnum_requests = 20  # Number of concurrent requests to launch.\\n\\n# Initialize the Craft AI SDK with the specified environment URL and token.\\nsdk = CraftAiSdk(environment_url=\"my-environment-url\", sdk_token=\"my-sdk-token\")\\n\\n# Retrieve the endpoint information for the given deployment name using the SDK.\\nendpoint_info = sdk.get_deployment(deployment_name)\\n\\nfor _ in range(num_requests):\\n    # Launch executions simultaneously on the platform, results available on the execution tracking page \\n    sdk.trigger_endpoint(deployment_name, endpoint_info[\"endpoint_token\"], wait_for_results=False)\\nUse case vLLM text generation\\nLet\\'s now look at an example of an LLM that generates text (including with multiple executions in parallel on the platform). For this, we use the vLLM library, which allows managing concurrent text generation on the same GPU with asynchronous executions on the platform.\\n\\nSource code of the pipeline\\nFor this use case, we need a requirements.txt file like this:\\n\\nrequirements.txt\\nvllm\\nSource code of the step:\\n\\ncode_step_vllm.py\\nimport datetime\\nfrom vllm import SamplingParams, AsyncLLMEngine, AsyncEngineArgs\\n\\nMODEL_NAME = \"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\"\\nengine_args = AsyncEngineArgs(\\n    model=MODEL_NAME,\\n    quantization=\"awq\",\\n    dtype=\"half\",\\n    max_model_len=8000,\\n)\\nllm = AsyncLLMEngine.from_engine_args(engine_args, start_engine_loop=True)\\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=512)\\n\\nid = 0\\n\\nasync def main(prompt):\\n    global id\\n    id += 1\\n\\n    prompt_formatted = f\\'\\'\\'<s>[INST] {prompt} [/INST]\\'\\'\\'\\n\\n    output = None\\n    async for out in llm.generate(prompt_formatted, sampling_params, id):\\n        output = out.outputs[0].text\\n\\n    return {\\'output\\': output}\\nCreation of the pipeline and deployment\\nThe creation of objects is almost the same as before, with the difference being that we add a text input and output.\\n\\nscript_sdk_create_obj.py\\n# This script creates a pipeline and a deployment using the Craft AI SDK. \\n# It uploads the code defined in `code_step_vllm.py` and sets up parallel executions for the deployment.\\nfrom craft_ai_sdk import CraftAiSdk\\nfrom craft_ai_sdk.io import Input, Output\\n\\n# Initialize the Craft AI SDK with the specified environment URL and token.\\nsdk = CraftAiSdk(environment_url=\"my-environment-url\", sdk_token=\"my-sdk-token\")\\n\\n# Pipeline creation as usual \\nsdk.create_pipeline(\\n    function_path=\"code_step_vllm.py\",\\n    function_name=\"main\", \\n    pipeline_name=\"my-pipeline-name\",\\n    container_config = { \\n        \"local_folder\": \"src/\",\\n        \"requirements_path\": \"requirements.txt\"\\n    },\\n    inputs=[Input(name=\"prompt\", data_type=\"string\")],\\n    outputs=[Output(name=\"response\", data_type=\"string\")],\\n)\\n\\n# Deployment creation with parallelism enabled \\nsdk.create_deployment(\\n    execution_rule=\"endpoint\",\\n    pipeline_name=\"my-pipeline-name\",\\n    deployment_name=\"my-deployment-name\",\\n    mode=\"low_latency\", # Mandatory, parallelism only work on low-latency mode\\n\\n    enable_parallel_executions=True,\\n    max_parallel_executions_per_pod=5\\n)\\nTrigger the deployment\\nscript_call_pipeline.py\\n# This script makes multiple concurrent requests to a deployment on the Craft AI platform.\\nfrom craft_ai_sdk import CraftAiSdk\\n\\ndeployment_name = \"my-deployment-name\" # Define the deployment name and the number of requests to be launched.\\nnum_requests = 20  # Number of concurrent requests to launch.\\n\\n# Initialize the Craft AI SDK with the specified environment URL and token.\\nsdk = CraftAiSdk(environment_url=\"my-environment-url\", sdk_token=\"my-sdk-token\")\\n\\n# Retrieve the endpoint information for the given deployment name using the SDK.\\nendpoint_info = sdk.get_deployment(deployment_name)\\n\\nfor _ in range(num_requests):\\n    # Launch executions simultaneously on the platform, results available on the execution tracking page \\n    sdk.trigger_endpoint(deployment_name, endpoint_info[\"endpoint_token\"], inputs={\"prompt\": \"Tell me a story\"}, wait_for_results=False)\\nNote\\n\\nOnce launched, the execution results are visible in execution tracking.\\n\\n\\nWork with environment variables and resource monitoring\\nAn environment is the infrastructure used by the platform to store data and run computation. Once created, you can start working in the environments by creating pipelines and running them in the platform.\\n\\nIn addition, you can create and save environment variables that will allow you to parameterize certain variables in order to call them when running or deploying a pipeline.\\n\\nFunction name\\tMethod\\tReturn type\\tDescription\\ncreate_or_update_environment_variable\\tCraftAiSdk.create_or_update_environment_variable (environment_variable_name, environment_variable_value)\\tdict\\tTo create or update an environment variable available for all pipelines executions.\\nlist_environment_variables\\tCraftAiSdk.list_environment_variables()\\tList of dict\\tGet a list of all environments variables in the current environment.\\ndelete_environment_variable\\tCraftAiSdk.delete_environment_variable(environment_variable_name)\\tdict\\tDelete a specified environment variable.\\nSet up an environment variable\\nAn environment variable is a value that can be passed to your step code. It is used to store information that may be needed by the operating system or by applications that run on the platform (for example, by endpoints you deploy on the platform).\\n\\nTip\\n\\nOn this page, we will detail the usage of the SDK, but environment variables can be modified from a page in the web interface by going to: Environments > click on the three dots of the desired environment > Settings > Environment variables.\\n\\nCreate and update an environment variable\\nTo create or update an environment variable available for all pipelines executions.\\n\\nCraftAiSdk.create_or_update_environment_variable(environment_variable_name,\\nenvironment_variable_value)\\nParameters\\nenvironment_variable_name (str) â€“ Name of the environment variable to create.\\nenvironment_variable_value (str) â€“ Value of the environment variable to create.\\nReturns\\nA dict object containing the ID of environment variable (with keys â€œidâ€)\\n\\nGet the list of environment variables\\nGet the list of all environment variables in the current environment.\\n\\nCraftAiSdk.list_environment_variables()\\nParameter\\nNo parameter\\n\\nReturns\\nList of dicts of environment variables (with keys â€œnameâ€ and â€œvalueâ€)\\n\\nDelete an environment variable\\nDelete a specified environment variable.\\n\\nCraftAiSdk.delete_environment_variable(environment_variable_name)\\nParameter\\nenvironment_variable_name (str) â€“ Name of the environment variable to delete.\\nReturns\\nDict (with keys â€œnameâ€ and â€œvalueâ€) of the deleted environment variable\\n\\nGet resource metrics\\nGet resource metrics of the environment into dict format (by default) or in a .csv file. You can view these metrics in graph format using the Resource Metrics web page.\\n\\nCraftAiSdk.get_resource_metrics(start_date, end_date, csv=False)\\nParameter\\nstart_date (datetime.datetime) - The beginning of the period.\\nend_date (datetime.datetime) - The end of the period.\\ncsv (bool) - If True, it will return a csv file as bytes.\\nReturns\\nIf csv is True, it will return bytes. Otherwise, dict with:\\n\\nThe resource metrics, with the following keys:\\n\\nadditional_data (dict): Additional data with the following keys:\\ntotal_disk (int): Total disk size in bytes.\\ntotal_ram (int): Total RAM size in bytes.\\ntotal_vram (int): Total VRAM size in bytes if there is a GPU.\\ncpu_usage (list of dict): The CPU usage in percent.\\ndisk_usage (list of dict): The disk usage in percent.\\nram_usage (list of dict): The RAM usage in percent.\\nvram_usage (list of dict): The VRAM usage in percent if there is a GPU.\\ngpu_usage (list of dict): The GPU usage in percent if there is a GPU.\\nnetwork_input_usage (list of dict): The network input usage in bytes.\\nnetwork_output_usage (list of dict): The network output usage in bytes.\\nEach element of the lists is a dict with the following keys:\\n\\nmetric (dict): Dictionary with the following key:\\nworker (str): The worker name.\\nvalues (list of list): The values of the metrics in the following format: [[timestamp, value], ...].\\nExample with dict object\\nfrom datetime import datetime, timedelta\\n\\n# Get the current time\\ncurrent_time = datetime.now()\\n\\n# Calculate the beginning of the period (last 2 hours)\\nbegin_date = current_time - timedelta(minutes=2)\\n\\n# Set end_date to the current time\\nend_date = current_time\\n\\n# Print the results\\nprint(\"begin_date:\", begin_date)\\nprint(\"end_date:\", end_date)\\n\\nres = sdk.get_resource_metrics(begin_date, end_date)\\n\\nprint (res)\\nExample of return object:\\n\\n{\\'metrics\\': {\\'cpu_usage\\': [{\\'metric\\': {\\'worker\\': \\'Worker-58d65a25\\'},\\n    \\'values\\': [[1707132918930, 0.00464583333348214],\\n     [1707132948930, 0.004708333333450027],\\n     [1707132978930, 0.004763888888882371],\\n     [1707133008930, 0.0047222222219369045],\\n     [1707133038930, 0.004722222221995642]]},\\n   {\\'metric\\': {\\'worker\\': \\'Worker-6e32691a\\'},\\n    \\'values\\': [[1707132918930, 0.007868055555652481],\\n     [1707132948930, 0.007638888888662494],\\n     [1707132978930, 0.00754861111112385],\\n     [1707133008930, 0.0076597222224098135],\\n     [1707133038930, 0.007930555555419681]]},\\n\\n    ...\\n\\n     [1707132948930, 40546.24444444444],\\n     [1707132978930, 48481.6111111111],\\n     [1707133008930, 40639.65555555555],\\n     [1707133038930, 48189.03333333333]]},\\n   {\\'metric\\': {\\'worker\\': \\'All cumulate\\'},\\n    \\'values\\': [[1707132918930, 63759.26666666666],\\n     [1707132948930, 56443.922222222216],\\n     [1707132978930, 64879.22222222221],\\n     [1707133008930, 46783.055555555555],\\n     [1707133038930, 53954.188888888886]]}]},\\n \\'additional_data\\': {\\'total_ram\\': 16616214528, \\'total_disk\\': 21462233088}}\\nExample with CSV file\\nfrom datetime import datetime, timedelta\\n\\n# Get the current time\\ncurrent_time = datetime.now()\\n\\n# Calculate the beginning of the period (last 2 hours)\\nbegin_date = current_time - timedelta(minutes=2)\\n\\n# Set end_date to the current time\\nend_date = current_time\\n\\n# Print the results\\nprint(\"begin_date:\", begin_date)\\nprint(\"end_date:\", end_date)\\n\\nres = sdk.get_resource_metrics(begin_date, end_date,csv=True)\\n\\nresult_csv = res.decode(\\'utf-8\\')\\n\\n# Save the result to a CSV file\\ncsv_filename = \"resource_metrics.csv\"\\nwith open(csv_filename, \"w\") as csv_file:\\n    csv_file.write(result_csv)\\n\\n\\nSave my data on the store\\nEach environment has its own storage to save and retrieve any amount of data at any time, from anywhere on the web. This storage is an Amazon S3 bucket that you can manage with the Python SDK of the platform.\\n\\nYou can upload any kind of raw data on the store. Furthermore, the results of the pipelines and metrics generated by the platform can also be saved on the data store.\\n\\nInfo: For the moment, it is not possible to copy data from an environment to another. You have to download it and to re-upload it. In future versions, you will be able to transfer your data.\\n\\nInfo\\n\\nThe graphical interface of the data store will arrive later on the platform.\\n\\nSummary\\nGet files\\nGet file info\\nUpload a file\\nDownload a file\\nDelete a file\\nFunction name\\tMethod\\tReturn type\\tDescription\\nlist_data_store_objects\\tCraftAiSdk.list_data_store_objects()\\tlist of dict\\tGet the list of the objects stored in the data store.\\nupload_data_store_object\\tCraftAiSdk.upload_data_store_object(filepath_or_buffer, object_path_in_datastore)\\tNone\\tUpload a file as an object into the data store.\\ndownload_data_store_object\\tCraftAiSdk.download_data_store_object(object_path_in_datastore, filepath_or_buffer)\\tNone\\tDownload an object in the data store and save it into a file.\\ndelete_data_store_object\\tCraftAiSdk.delete_data_store_object(object_path_in_datastore)\\tdict\\tDelete an object on the data store.\\nGet files\\nFunction definition\\nGet the list of the objects stored in the data store.\\n\\nCraftAiSdk.list_data_store_objects()\\nParameter\\nNo parameter\\n\\nReturns\\nList of objects (dict) in the data store, each object being represented as dict with :\\n\\n\"path\" (str): Location of the object in the data store.\\n\"last_modified\" (str): The creation date or last modification date in ISO format.\\n\"size\" (str): The size of the object with units of digital storage measurement (MB, GB, ...).\\nGet file info\\nFunction definition\\nGet information about a single object in the data store.\\n\\nCraftAiSdk.get_data_store_object_information(object_path_in_datastore)\\nParameter\\nobject_path_in_datastore (str) â€“ Location of the object in the data store.\\nReturns\\nObject information, with the following keys:\\n\\n\"path\" (str): Location of the object in the data store.\\n\"last_modified\" (str): The creation date or last modification date in ISO format.\\n\"size\" (str): The size of the object with units of digital storage measurement (MB, GB, ...).\\nUpload a file\\nFunction definition\\nUpload a file as an object into the data store.\\n\\nCraftAiSdk.upload_data_store_object(filepath_or_buffer, object_path_in_datastore)\\nParameters\\nfilepath_or_buffer (str, or file-like object) â€“ String, path to the file to be uploaded ; or file-like object implementing a read() method (e.g. via buildin open function). The file object must be opened in binary mode, not text mode.\\nobject_path_in_datastore (str) â€“ Destination of the uploaded file.\\nReturns\\nThis function returns None.\\n\\nDownload a file\\nFunction definition\\nDownload an object in the data store and save it into a file.\\n\\nCraftAiSdk.download_data_store_object(object_path_in_datastore, filepath_or_buffer)\\nParameters\\nobject_path_in_datastore (str) â€“ Location of the object to download from the data store.\\nfilepath_or_buffer (str or file-like object) â€“ String, file path to save the file to ; or a file-like object implementing a write() method, (e.g. via builtin open function). The file object must be opened in binary mode, not text mode.\\nReturns\\nThis function return an None.\\n\\nDelete a file\\nFunction definition\\nDelete an object on the data store.\\n\\nCraftAiSdk.delete_data_store_object(object_path_in_datastore)\\nParameters\\nobject_path_in_datastore (str) â€“ Location of the object to delete in the data store.\\nReturns\\nDeleted object represented as dict (with key â€œpathâ€).\\n\\nStep & Pipeline\\nA pipeline is a machine learning workflow, consisting of one or more steps, to deploy containerised code. Like a regular function, a step is defined by the input it ingests, the code it runs, and the output it returns. You can then create a full pipeline formed with a computed acyclic graph (DAG) by specifying the output of one step as the input of another step.\\n\\nintroductionStepPipeline_1\\n\\nðŸ’¡ The pipelines are written in Python with SDK calls for an easy authoring experience and executed on Kubernetes for scalability. The pipelinesâ€™ functioning are based on the open-source library Argo. Each step deploy is containerised with Docker.\\n\\nThe main objectives of the steps and pipelines are :\\n\\nOrchestrating end-to-end ML workflows\\nIncreasing reusability of Data Science components from a project to another\\nCollaboratively managing, tracking and viewing pipeline definitions.\\nDeploying in production in few clicks with several methods (endpoint, CRON, manual, â€¦)\\nEnabling large scale production of Python code without refactoring to a more production friendly language\\nEnsuring an efficient use of compute resources, thanks to Kubernetes\\nExample :\\n\\nðŸ–Šï¸ Training pipeline\\n\\nData preparation and preprocessing: In this stage, raw data is collected, cleaned, and transformed into a format that is suitable for training a machine learning model. This may involve tasks such as filtering out missing or invalid data points, normalizing numerical values, and encoding categorical variables.\\nModel training: In this stage, a machine learning model is trained on a prepared dataset. This may involve selecting a model type, tuning hyperparameters, and training the model using an optimization algorithm.\\nModel evaluation: Once the model has been trained, it is important to evaluate its performance to determine how well it generalizes to new data. This may involve tasks such as splitting the dataset into a training set and a test set, evaluating the modelâ€™s performance on the test set, and comparing the results to a baseline model.\\nðŸ–Šï¸ Inference pipeline\\n\\nModel loading: In this stage, a trained machine learning model is loaded from storage and prepared for use. This may involve tasks such as loading the modelâ€™s weights and any associated dependencies.\\nData preparation: In this stage, incoming data is cleaned and transformed into a format that is suitable for the model. This may involve tasks such as normalizing numerical values and encoding categorical variables.\\nInference: In this final stage, the model is used to make predictions on the prepared data. This may involve tasks such as passing the data through the model and processing the output to generate a final prediction.\\n\\n\\nCreate a step\\nA step is an atomic component defined by its input and output parameters and by the processing it applies. Steps are the building blocks of pipelines. In practice, a step is a function with inputs and outputs coded in Python. They are assembled to create a complete ML pipeline. The Python code (currently the only available language) used by the step is stored in a folder on your local machine or in a Git repository.\\n\\nAn input of a step is an object you can use inside the code. An output of a step is defined from the results of the step function. You will be able to connect inputs & outputs of a step with another step to compose a complete ML pipeline by using a directed acyclic graph (DAG).\\n\\nEach step is considered as a specific container that is executed on Kubernetes.\\n\\nThe steps are stored in a specific environment, and only people with access to this environment can read and write the steps. By default, each step uses the values defined in the project settings. However, these values can be overridden in the step creation parameters, as detailed below.\\n\\ncreateStep_1\\n\\nSummary\\nPrepare your code\\nDefine step inputs and outputs\\nCreate a step\\nFunction name\\tMethod\\tReturn type\\tDescription\\nInput\\tInput(name, data_type=\"string\", description=\"\", is_required=False, default_value=None)\\tInput SDK Object\\tCreate an Input object to give at create_steps() function for step a step input.\\nOutput\\tOutput(name, data_type=\"string\", description=\"\")\\tOutput SDK Object\\tCreate an Output object to give at create_steps() function for step a step output.\\ncreate_step\\tcreate_step(step_name, function_path, function_name, description=None, timeout_s=180, container_config=None, inputs=None, outputs=None)\\tlist of dict[str, str]\\tCreate pipeline steps from a source code located on a remote repository.\\nPrepare your code\\n[Optionnal] Use a Git repository\\nSetup Project & Environment\\nCurrently you can create a step using the Python SDK, but not using the GUI. However, once you have created the step and associated pipeline, you will be able to see the pipeline on the UI platform.\\n\\nIf itâ€™s not already done, put the code of the step into a single folder, which will be sent to the platform. The file with the entry function of your step can be anywhere in your folder.\\n\\nExample file tree:\\n\\n.\\nâ”œâ”€â”€ requirements.txt\\nâ””â”€â”€ src/\\n    â””â”€â”€ my_entry_function_step.py\\n...\\nExample my_entry_function_step.py:\\n\\nimport numpy as np\\n# and other import\\n\\ndef entryStep(dataX_input, dataY_input) :\\n\\n    # Some machine learning code\\n\\n    return result_output\\nIf you prefer, you can also use a GitHub repository instead of a local folder. More information can be found here.\\n\\nDefine step inputs and outputs\\nA step may need to receive some information or give some result (just like a function). To do that, we use Input and Output object. These objects allow defining the properties of the input or output that will be expected in the step. The input and output objects thus created must be given as a parameter of the step creation. Each input is defined as an Input object and, each Output is defined as an Output object, through a class available in the SDK.\\n\\nInput object definition\\nfrom craft_ai_sdk.io import Input\\n\\nInput(\\n   name=\"*your_input_name*\",\\n   data_type=\"*your_io_data_type*\",\\n   description=\"\",\\n   is_required=True\\n   default_value=\"*default_value*\"\\n)\\nParameters\\nname just a name for identifying the input later.\\n\\ndata_type, one of the following possible types:\\n\\nfile: reference to binary data, equivalent to a fileâ€™s content. If the input/output is not available, an empty stream.\\n\\njson: JSON-serializable Python object. The following sub-types are provided for more precise type checking, but they are all JSON\\n\\nstring\\n\\nnumber\\n\\narray of JSON\\n\\nboolean\\n\\nIf the input/output is not available, None in Python\\n\\ndefault_value (optional) - If the parameter is empty, this value will be set by default. If a deployment receives an empty parameter and already put a default value in the input, the default value of deployment will be keep.\\n\\nis_required (optional, True by default) - Push an error is the input is empty.\\n\\ndescription (optional) - This parameter precise what itâ€™s expected in this input. Itâ€™s not read by the machine, itâ€™s like a comment.\\n\\nReturn\\nNo return\\n\\nOutput object definition\\nfrom craft_ai_sdk.io import Output\\n\\nOutput(\\n   name=\"*your_input_name*\",\\n   data_type=\"*your_io_data_type*\",\\n   description=\"\",\\n)\\nParameters\\nname just a name for identifying the input later.\\n\\ndata_type, one of the following possible types:\\n\\nfile: reference to binary data, equivalent to a fileâ€™s content. If the input/output is not available, an empty stream.\\n\\njson: JSON-serializable Python object. The following sub-types are provided for more precise type checking, but they are all JSON\\nstring\\nnumber\\narray of JSON\\nboolean\\nIf the input/output is not available, None in Python\\n\\ndescription (optional) - This parameter precise what itâ€™s expected in this input. Itâ€™s not read by the machine, itâ€™s like a comment.\\nReturn\\nNo return\\n\\nNote\\n\\nYou can use craft_ai_sdk.INPUT_OUTPUT_TYPES to get all possible types in Input and Output objects.\\n\\nList of all possible types :\\n\\nARRAY = \"array\"\\nBOOLEAN = \"boolean\"\\nFILE = \"file\"\\nJSON = \"json\"\\nNUMBER = \"number\"\\nSTRING = \"string\"\\nExample :\\n\\nfrom craft_ai_sdk.io import Input, INPUT_OUTPUT_TYPES\\n\\nInput(\\n   name=\"inputName\",\\n   data_type=INPUT_OUTPUT_TYPES.JSON,\\n)\\nExample for input and output\\nInput(\\n    name=\"inputName\",\\n    data_type=\"string\",\\n    description=\"A parameter for step input\",\\n    is_required=True,\\n    default_value=\"default_content_here\"\\n)\\n\\nOutput(\\n    name=\"inputName\",\\n    data_type=\"string\",\\n    description=\"A parameter for step input\",\\n)\\nWarning\\n\\nThe size of the I/O must not exceed 0.06MB (except for file type).\\n\\nCreate a step\\nFunction definition\\nCreate pipeline steps from a source code located on a local folder or a Git repository.\\n\\nsdk.create_step(\\n    function_path=\"src/my_reusable_funtion.py\",\\n    function_name=\"my_function\",\\n    inputs=[Input(...)],\\n    outputs=[Output(...)],\\n    name=\"step-name\", # by default its the function name\\n    description=\"text desciption\",\\n    timeout_s=180,\\n    container_config = {\\n        language=\"python:3.8-slim\",\\n        repository_url=\"your-git-url\",\\n        repository_branch=\"*your-git-branch* or *your-git-tag*\",\\n        repository_deploy_key=\"your-private_key\",\\n        requirements_path=\"your-path-to-requirements.txt\",\\n        included_folders=[\"your-list-of-path-to-sources\"],\\n        system_dependencies=[\"package_1\", \"package_2\"],\\n        dockerfile_path=\"dockerfile\",\\n        local_folder=\"*my-local-folder-path*\"\\n    },\\n)\\nParameters\\nfunction_path (str) â€“ Path to access to the file who had the entry function of the step.\\n\\nfunction_name (str) â€“ Function name of entry function step.\\n\\ninputs (list<Input>) â€“ List of step inputs.\\n\\noutputs (list<Output>) â€“ List of step outputs.\\n\\nname (str) â€“ Step name. By default, itâ€™s the function name. The name must be unique inside an environment and without special character ( - _ & / ? â€¦)\\n\\ndescription (str, optional) â€“ Description of the step, itâ€™s no use by the code, itâ€™s only for user.\\n\\ntimeout_s (int, optional) â€“ Maximum time to wait for the step to be created. 3min by default, and must be at least 2min.\\n\\ncontainer_config (dict, optional) â€“ Dict Python object where each key can override default parameter values for this step defined at project level.\\n\\nlanguage (str, optional) â€“ Language and version used for the step. Defaults to falling back on project information. The accepted formats are python:3.X-slim, where 3.X is a supported version of Python, and python-cuda:3.X-Y.Z for GPU environments, where Y.Z is a supported version of CUDA. The list of supported versions is available here.\\n\\nrepository_url (str, optional) â€“ Remote repository URL.\\nrepository_branch (str, optional) â€“ Branch name for Git repository. Defaults to None.\\nrepository_deploy_key (str, optional) â€“ Private SSH key related to the repository.\\nrequirements_path (str, optional) â€“ Path to the file requirement for Python dependency.\\nincluded_folders (list, optional) â€“ List of folders that need to be accessible from step code.\\nsystem_dependencies (list, optional) â€“ List of APT Linux packages to install.\\ndockerfile_path (str, optional) â€“ Path to a docker-file for having a custom config in step. (see the part after for more detail)\\nlocal_folder (str, optional): Path to local folder where the step files are stored, if not on a Git repository.\\nNote\\n\\nThe repository_branch parameters as well as the container_config elements (except dockerfile_path) can take one of the STEP_PARAMETER object\\'s values in addition to theirs.\\n\\nIn fact, STEP_PARAMETER allows us to specify at the step level whether we want to take the project\\'s values (default behavior) or define a null value:\\n\\nSTEP_PARAMETER.FALLBACK_PROJECT : Allows to take the value defined in the project parameters (default behavior if the field is not defined).\\nSTEP_PARAMETER.NULL : Allows to set the field to null value and not to take the value defined in the project.\\nExample with a code step that does not need a requirement.txt and does not take the one defined in the project settings:\\n\\nfrom craft_ai_sdk import STEP_PARAMETER\\n\\n# Code for init SDK here ...\\n\\nsdk.create_step(\\n  function_path=\"src/helloWorld.py\",\\n  function_name=\"helloWorld\",\\n  step_name=\"my_step_name\",\\n  container_config = {\\n      \"requirements_path\": STEP_PARAMETER.NULL,\\n   }\\n)\\nWarning\\n\\nThe size of the embedded code from your folder / Git repository must not exceed 5MB. You can select the part of your folder / Git repository to import using the included_folders parameter.\\n\\nIf the data you want to import is larger than 5MB, you can use the data store to store it and then import it into your step.\\n\\nReturns\\nThe return type is a dict with the following keys :\\n\\nparameters (dict): Information used to create the step with the following keys:\\nstep_name (str): Name of the step.\\nfunction_path (str): Path to the file that contains the function.\\nfunction_name (str): Name of the function in that file.\\ndescription (str): Description.\\ninputs (list of dict): List of inputs represented as a dict with the following keys:\\nname (str): Input name.\\ndata_type (str): Input data type.\\nis_required (bool): Whether the input is required.\\ndefault_value (str): Input default value.\\noutputs (list of dict): List of outputs represented as a dict with the following keys:\\nname (str): Output name.\\ndata_type (str): Output data type.\\ndescription (str): Output description.\\ncontainer_config (dict[str, str]): Some step configuration, with the following optional keys:\\nlanguage (str): Language and version used for the step. Defaults to falling back on project information. The accepted formats are python:3.X-slim, where 3.X is a supported version of Python, and python-cuda:3.X-Y.Z for GPU environments, where Y.Z is a supported version of CUDA. The list of supported versions is available here.\\nrepository_url (str): Remote repository url.\\nrepository_branch (str): Branch name.\\nincluded_folders (list[str]): List of folders and files in the repository required for the step execution.\\nsystem_dependencies (list[str]): List of system dependencies.\\ndockerfile_path (str): Path to the Dockerfile.\\nrequirements_path (str): Path to the requirements.txt file.\\ncreation_info (dict): Information about the step creation:\\ncreated_at (str): The creation date in ISO format.\\nupdated_at (str): The last update date in ISO format.\\ncommit_id (str): The commit id on which the step was built.\\nstatus (str): The step status, if the step creation process is under 2m40s (most of the time it is), is always Ready when this function returns.\\norigin (str): The origin of the step, can be git_repository or local.\\nNote\\n\\nOnce our step is created, we need to create the pipeline that wraps the step. It is mandatory to create a pipeline once the step is created to be able to use it later. This technical choice was made in anticipation of future multistep functionality. This forces the use of a pipeline to contain the steps.\\n\\nListe of language available\\nWhen using a CPU environment, the language parameter must be :\\n\\npython:3.8-slim.\\npython:3.9-slim\\npython:3.10-slim\\nWhen using a GPU environment, the language parameter must be :\\n\\nFor cuda v11.8\\npython-cuda:3.8-11.8\\npython-cuda:3.9-11.8\\npython-cuda:3.10-11.8\\n\\nFor cuda v12.1\\n\\npython-cuda:3.9-12.1\\npython-cuda:3.10-12.1\\nYou can also use the CPU image in a GPU environment if you don\\'t need access to the GPU.\\n\\nExample: Create step from scratch\\nFunction usage\\n\\nfrom craft_ai_sdk import Input, Output\\n\\ninput1 = Input(\\n    name=\"input1\",\\n    data_type=\"string\",\\n    description=\"A parameter named input1, its type is a string\",\\n    is_required=True,\\n)\\n\\ninput2 = Input(\\n    name=\"input2\",\\n    data_type=\"file\",\\n    description=\"A parameter named input2, its type is a file\"\\n)\\n\\ninput3 = Input(\\n    name=\"input3\",\\n    data_type=\"number\",\\n)\\n\\nprediction_output = Output(\\n    name=\"prediction\",\\n    data_type=\"file\",\\n    default_value=\"default,content,here\",\\n)\\n\\nstep = sdk.create_step(\\n    function_path=\"src/my_reusable_funtion.py\",\\n    function_name=\"my_function\",\\n    description=\"Apply the model to the sea\",\\n    container_config = { \\n        \"local_folder\": \"my/path/\",\\n    },\\n    inputs_list=[input1, input2, input3],\\n    outputs_list=[prediction_output],\\n        ## ...\\n)\\nNote\\n\\nIf you need to create a step with a more specific configuration, you can do this with a custom Dockerfile. More detail about that here.\\n\\nDownload step local code\\nFunction definition\\nDownload a step local folder as a .tgz file. Only available if step origin is local_folder.\\n\\nCraftAiSdk.download_step_local_folder(\\n    step_name=\"my_step_name\", \\n    folder=\"my/path/\"\\n)\\nParameters\\nstep_name (str) â€“ Name of the step to be downloaded.\\nfolder (str) â€“ Path to the folder where the file will be saved.\\nReturns\\nNone\\n\\nManage a step\\nSummary:\\n\\nFind and get information about steps\\nDelete steps\\nFunction Name\\tMethod\\tReturn Type\\tDescription\\nget_step\\tget_step (step_name)\\tdict\\tGet information about a step.\\nlist_steps\\tlist_steps()\\tlist of dict\\tGet a list of available steps.\\ndelete_step\\tdelete_step (step_name)\\tdict[str, str]\\tDelete one step.\\nâ“ For step update and deletion, you need the name (the name you provide when creating the step) of the step you want to update/delete. You can find it with function list_steps() (see the previous part).\\n\\nFind and get information about steps\\nTo get information about a step, we need its name in the environment. You can search its name in the list of stepâ€™s name of the environment.\\n\\nGet list of steps\\nFunction definition\\nTo get all steps available in the current environment, you can get a list of step name with this function:\\n\\nCraftAiSdk.list_steps()\\nReturns\\nList of steps represented as dict with the following keys:\\n\\nname (str): Name of the step.\\nstatus (str): either Pending or Ready.\\ncreated_at (str): The creation date in ISO format.\\nupdated_at (str): The last update date in ISO format.\\nrepository_branch (str): The branch of the repository where the step was built.\\nrepository_url (str): The url of the repository where the step was built.\\ncommit_id (str): The commit id on which the step was built.\\norigin (str): The origin of the step, can be git_repository or local.\\nGet information about one step\\nFunction definition\\nGet all information (repository, dependency, â€¦) about one step in the current environment with its name.\\n\\nCraftAiSdk.get_step(step_name)\\nParameters\\nstep_name (str) â€“ The name of the step to get.\\nReturns\\ndict: None if the step does not exist; otherwise the step information, with the following keys:\\n\\nparameters (dict): Information used to create the step with the following keys:\\nstep_name (str): Name of the step.\\nfunction_path (str): Path to the file that contains the function.\\nfunction_name (str): Name of the function in that file.\\ndescription (str): Description.\\ninputs (list of dict): List of inputs represented as a dict with the following keys:\\nname (str): Input name.\\ndata_type (str): Input data type.\\nis_required (bool): Whether the input is required.\\ndefault_value (str): Input default value.\\noutputs (list of dict): List of outputs represented as a dict with the following keys:\\nname (str): Output name.\\ndata_type (str): Output data type.\\ndescription (str): Output description.\\ncontainer_config (dict[str, str]): Some step configuration, with the following optional keys:\\nlanguage (str): Language and version used for the step.\\nrepository_url (str): Remote repository url.\\nrepository_branch (str): Branch name.\\nincluded_folders (list[str]): List of folders and files in the repository required for the step execution.\\nsystem_dependencies (list[str]): List of system dependencies.\\ndockerfile_path (str): Path to the Dockerfile.\\nrequirements_path (str): Path to the requirements.txt file.\\ncreation_info (dict): Information about the step creation:\\ncreated_at (str): The creation date in ISO format.\\nupdated_at (str): The last update date in ISO format.\\ncommit_id (str): The commit id on which the step was built.\\nstatus (str): Either \"Pending\" or \"Ready\".\\norigin (str): The origin of the step, can be git_repository or local.\\nDelete steps\\nDelete steps function\\nFunction definition\\nDelete step in the environment with his name.\\n\\nCraftAiSdk.delete_step(step_name, force_dependents_deletion=False)\\nParameters\\nstep_name (str) â€“ Name of the step to delete as defined in the create step function.\\nforce_dependents_deletion (bool, optional) â€“ if True the associated stepâ€™s dependencies will be deleted too (pipeline, pipeline executions, deployments). Defaults to False.\\nReturns\\nDeleted step represented as dict (with key â€œnameâ€). The return type is a dict [str, str].\\n\\nWarning\\n\\nYou can\\'t delete a step that is used in a pipeline. You must delete the pipeline before or use the force_dependents_deletion parameter during step deletion.\\n\\n\\nCompose a pipeline\\nA pipeline is a machine learning workflow, consisting of one or more steps, to deploy code using Docker containers. By specifying the output of one step as the input of another step, the user can create a full pipeline formed with a computed acyclic graph (DAG).\\n\\nInfo\\n\\nFor the moment, the pipelines are only single-step. We are actively working on the integration of multi-steps for the next versions of the platform.\\n\\nTo deploy ML code in production, you need to go through a pipeline. So you have to go through a single-step pipeline in any case to perform an endpoint or another type of deployment.\\n\\nSummary:\\n\\nCompose a mono-step pipeline\\nDelete a pipeline\\nFind and get pipeline information\\nFunction name\\tMethod\\tReturn type\\tDescription\\ncreate_pipeline\\tcreate_pipeline(pipeline_name, step_name)\\tdict[str, str]\\tCreate a pipeline containing a single step.\\nget_pipeline\\tget_pipeline(pipeline_name)\\tdict\\tGet a single pipeline if it exists.\\ndelete_pipeline\\tdelete_pipeline(pipeline_name, force_deployments_deletion=False)\\tdict\\tDelete a pipeline identified by its name and ID.\\nlist_pipelines\\tlist_pipelines()\\tlist of dict\\tGet the list of all pipelines.\\nCreate a mono-step pipeline\\nBefore creating a pipeline, the step creation must be finished. You can check this by checking that the step\\'s status are equal to Ready using the get_step() function.\\n\\nSDK function pipeline\\nFunction definition\\npipeline = sdk.create_pipeline(\\n   pipeline_name=\"my_pipeline\",\\n   step_name=\"my_step\",\\n)\\nParameters\\npipeline_name (str) â€“ Name of the pipeline to create.\\n\\nstep_name (str) â€“ Name of the step to include in the pipeline.\\n\\n!!! note The step should have the status â€œReadyâ€ before being used to create the pipeline.\\n\\nReturns\\nCreated pipeline represented as dict using this keys:\\n\\npipeline_name (str): Pipeline name.\\ncreated_at (str): Pipeline date of creation.\\nsteps (list[str]): List of step names.\\nopen_inputs (list[dist]): List of all input of step.\\ninput_name (str): Name of the open input.\\nstep_name (str): Name of the step that provides the open input.\\ndata_type (str): Data type of the open input.\\ndescription (str): Description of the open input.\\ndefault_value (str): Default value of the open input.\\nis_required (bool): Whether the open input is required or not.\\nopen_outputs (list[dist]): List of all output of step.\\noutput_name (str): Name of the open output.\\nstep_name (str): Name of the step that provides the open output.\\ndata_type (str): Data type of the open output.\\ndescription (str): Description of the open output.\\nInformation about pipeline store\\nPipeline can have multiple inputs and outputs, or no one. In fact, itâ€™s dependent on the inputs and outputs of step inside. All input and output will be the same as the step inside. So, you have nothing to configure on pipeline creation for input and output.\\n\\nGet information pipeline\\nGet information about one pipeline\\nFunction definition\\nGet all information about one pipeline, referred by its name.\\n\\nCraftAiSdk.get_pipeline(pipeline_name)\\nParameters\\npipeline_name (str) â€“ Name of the pipeline to get.\\nReturns\\nThe pipeline information in a dict (or None if the pipeline does not exist), with the following keys:\\n\\npipeline_name (str): Pipeline name.\\ncreated_at (str): Pipeline date of creation.\\ncreated_by (str): ID of the user who created the deployment.\\nlast_execution_id (str): ID of the last execution of the pipeline.\\nsteps (list[str]): List of step names.\\nopen_inputs (list[dist]): List of all input of step.\\ninput_name (str): Name of the open input.\\nstep_name (str): Name of the step that provides the open input.\\ndata_type (str): Data type of the open input.\\ndescription (str): Description of the open input.\\ndefault_value (str): Default value of the open input.\\nis_required (bool): Whether the open input is required or not.\\nopen_outputs (list[dist]): List of all output of step.\\noutput_name (str): Name of the open output.\\nstep_name (str): Name of the step that provides the open output.\\ndata_type (str): Data type of the open output.\\ndescription (str): Description of the open output.\\nGet all pipelines\\nFunction definition\\nGet the list of all pipelines on your environment.\\n\\nCraftAiSdk.list_pipelines()\\nReturns\\nList of pipelines represented as dict with keys :\\n\\n\"pipeline_name\" (str): Name of pipeline\\n\"created_at\" (str): Create date of the pipeline.\\n\"status\" (str): Status of the pipeline.\\nDelete a pipeline\\nFunction definition\\nDelete a pipeline from the current environment.\\n\\nCraftAiSdk.delete_pipeline(pipeline_name, force_deployments_deletion=False)\\nParameters\\npipeline_name (str) â€“ Name of the pipeline.\\nforce_deployments_deletion (bool, optional) â€“ if True, the associated endpoints will be deleted too. Defaults to False.\\nReturns\\nThe deleted pipeline and its associated deleted deployments represented as a dict with the following keys:\\n\\npipeline (dict): Deleted pipeline represented as dict with the following keys:\\nname (str): Name of the deleted pipeline.\\ndeployments (list): List of deleted deployments represented as dict with the following keys:\\nname (str): Name of the deleted deployments.\\nexecution_rule (str): Execution rule of the deleted deployments.\\nWarning\\n\\nYou can\\'t delete a pipeline that is used in a Deployment\\n\\nRun a pipeline\\nNow that your pipeline is created you can run it directly from the SDK. Or you can configure a deployment.\\n\\n\\nDeployment\\nA deployment is a way to trigger an execution of a Machine Learning pipeline in a repeatable and automated way. Each pipeline can be associated with multiple deployments.\\n\\nFor each deployment, you can use 2 distinct execution rules :\\n\\nby endpoint (web API)\\nby periodic trigger (CRON)\\nIn addition, for each deployment, you will need to connect the pipeline inputs and outputs with the desired sources and destinations. When one of the deployment conditions is met, the pipeline is executed by using the computing resources available in its environment.\\n\\nThe results of the execution (predictions, metrics, data, ...) can be stored in the data store of the environment and can be easily retrieved by the users. You can find all the information about the executions in the execution tracking section.\\n\\n\\n\\nIn addition to the deployment functionality, it is possible to run a pipeline directly without deploying it. This allows you to run your pipeline on the fly without having to create a specific deployment, which is very useful during the experimentation phase.\\n\\nThe main objectives of the deployments are :\\n\\nNo longer take 6 months to deploy ML models in production but a few clicks!\\nAutomating the execution of the pipelines to save time for Data Science teams\\nCreating secured web API to deliver pipeline results to external users without any DevOps skills\\nAutomatically triggering re-training pipelines when model performance drops\\nVisualizing pipeline executions, experiment tracking, and ML artifacts\\n\\n\\nDefine the pipeline sources and destinations\\nA deployment is a way to run a Machine Learning pipeline in a repeatable and automated way. First, you have to choose one of the 2 deployments methods.\\n\\nThen, you need to connect the pipeline inputs and outputs with the desired sources and destinations.\\n\\nSources : This is the origin of the data that you want to connect to the pipeline inputs. The data can come from the data store, from environment variables, from constants or from the endpoint (if this deployment method has been chosen).\\n\\nDestinations : This is the data drop point that you want to connect to the pipeline outputs. The data can go to the data store or to the endpoint (if this deployment method has been chosen).\\n\\n\\n\\nNote\\n\\nOn this page, we will mainly focus on the platform\\'s SDK interface.\\n\\nHowever, it is possible to deploy a pipeline directly from the web interface by going to the Pipelines page, selecting a pipeline and then clicking the Deploy button.\\n\\nSummary\\nGeneral function of I/O mapping\\nCreate input mapping\\nCreate output mapping\\nObjects name\\tConstructor\\tReturn type\\tDescription\\nInputSource\\tInputSource(step_input_name, required=False, default=None)\\tInputSource Object\\tCreate a mapping source object for deployment.\\nOutputDestination\\tOutputDestination(step_output_name, endpoint_output_name, required=False, default=None)\\tOutputDestination Object\\tCreate a mapping destination object for deployment.\\nGeneral function of the I/O mapping\\nMapping rules\\nWhen you start a new deployment, the data flow is configured with a mapping. You can create this mapping in two ways: auto mapping (only available with endpoint trigger) or manual mapping in the SDK or UI.\\n\\nAuto mapping automatically maps all inputs to endpoint variables for sources. If you need a different mapping or another trigger, you must map your inputs and outputs manually.\\n\\nAuto mapping Example\\n\\n\\n\\nFor each input / output, you had defined a data type in step. This data type will be the same as the mapped step input / output. You need to map the good source and destination with the good data type :\\n\\nEndpoint (input and output) â†’ Variable (string, number, ...) and file\\nData store â†’ Only file\\nConstant â†’ Only variable (string, number, ...)\\nEnvironment variable â†’ Only variable (string, number, ...)\\nNone â†’ Variable (string, number, ...) and file\\nLimitations\\nThe inputs (as outputs) of a step mapped through the endpoint (default mapping) can consist of only a single file or multiple variables. Thus, if you use deployment by triggering an endpoint, you cannot have multiple files as inputs or outputs due to a technical limitation of the API calls.\\n\\nTo have multiple files as the inputs or outputs of an endpoint, you can compress the files into a single file (for example, with the [tar]{.title-ref} command) to be sent in the API call.\\n\\nExamples: You can do\\n\\n\\nWarning\\n\\nThe data store is not yet available for the input/output mapping, but it\\'s coming soon, so stay tuned.\\n\\n\\n\\n\\n\\nExamples: You can\\'t do\\n\\n\\n\\n\\n\\n\\nCreation input mapping\\nImport Dependencies\\nBefore creating a mapping between the input/output of a pipeline and the sources/destinations of an endpoint, you need to import the InputSource and OutputDestination objects from the SDK.\\n\\nfrom craft_ai_sdk.io import InputSource, OutputDestination\\nFunction Definition\\nFor each input of your pipeline, in manual mapping, you need to create a mapping object that will be given to the create_deployment() function.\\n\\nboat_endpoint_input = InputSource(\\n    step_input_name=\"apply_model\",\\n\\n    ## Choose from one of these parameters\\n    endpoint_input_name=\"boat\", ## For mapping with an endpoint\\n    environment_variable_name=None, ## For mapping with an environment variable\\n    constant_value=None, ## For mapping with a constant value\\n    is_null=True ## For mapping with None\\n\\n    is_required=True,\\n    default_value=\"empty\",\\n)\\nParameters\\nstep_input_name (str): name of the input at the step level to which the deployment input will be linked to\\nDifferent possible sources:\\n\\nendpoint_input_name (str, optional): name of the input at the endpoint level to which the step input will be linked to\\nenvironment_variable_name (str, optional): name of the environment variable to which the step input will be linked to\\nconstant_value (Any, optional): a constant value to which the step input will be linked to\\nOther parameters:\\n\\nis_null (True, optional): if specified, the input will not take any value at execution time\\ndefault_value (Any, optional): this parameter can only be specified if the deployment is an endpoint. In this case, if nothing is passed at the endpoint level, the step input will take the default_value\\nis_required (bool, optional): this parameter can only be specified if the deployment is an endpoint.If set to True, the corresponding endpoint input should be provided at execution time.\\nReturn\\nAn InputSource object that can be used in the deployment creation in the dictionary format.\\n\\nAdditional parameters for source definition\\nBy default, the source is configured as an endpoint parameter, but you can configure a different source for your mapping. For each source, you have to add 1 parameter to :\\n\\nDefine the type of source with the name of parameter added\\nPrecise element about the source\\nWe will list all parameters you can have in your input mapping.\\n\\nWarning\\n\\nYou can only add 1 parameter of source definition. By default, it\\'s always endpoint source that is configured.\\n\\nEndpoint source\\n\\nParameter name : endpoint_input_name\\n\\nSource from : Outside through the endpoint\\n\\nValue to put in parameter : name of the input received by the endpoint in the body of the HTTP call\\n\\nExample :\\n\\nendpoint_input = InputSource(\\n    step_input_name=\"seagull\",\\n    endpoint_input_name=\"seagull\",\\n    required=False, #optional \\n    default=\"Eureka\", #optional\\n)\\nData store source\\n\\nParameter name : datastore_path\\n\\nSource from : file content from the data store\\n\\nValue to put in parameter : path to a data store file\\n\\nExample :\\n\\ndata_store_input = InputSource(\\n    step_input_name=\"trainingData\",\\n    datastore_path=\"path/to/trainingData.csv\",\\n)\\nExample step code to read file :\\n\\ndef stepFileIO (trainingData) :\\n   print (trainingData)\\n\\n   with open(trainingData[\"path\"]) as f:\\n      contents = f.readlines()\\n      print (contents)\\nConstant source\\n\\nParameter name : constant_value\\n\\nSource from : static value\\n\\nValue to put in parameter : direct value\\n\\nExample :\\n\\nconstant_input = InputSource(\\n  step_input_name=\"salt\",\\n    constant_value=3,\\n)\\nEnvironment variable\\n\\nParameter name : environment_variable_name\\n\\nSource from : the variables set at the level of an Environment in the platform\\n\\nValue to put in parameter : name of the environment variable\\n\\nExample :\\n\\nenv_var_input = InputSource(\\n  step_input_name=\"fish\",\\n    environment_variable_name=\"nameOfEnvVar\",\\n)\\nNone value\\n\\nParameter name : no_value\\n\\nDestination to : void\\n\\nValue to put in parameter : True\\n\\nExample :\\n\\nnull_input = InputSource(\\n  step_input_name=\"fish\",\\n    is_null=True\\n)\\nCreate output mapping\\nImport dependency\\nBefore creating mapping between input / output of pipeline and sources / destination of endpoint, you have to import InputSource and OutputDestination objects from SDK.\\n\\nfrom craft_ai_sdk.io import InputSource, OutputDestination\\nFunction definition\\nFor each output of your pipeline, in manual mapping, you have to create an object, that will be given to create_deployment() function.\\n\\nendpoint_output = OutputDestination(\\n    step_output_name=\"pred_0\",\\n\\n    ## Choose from one of these parameters\\n    endpoint_output_name=\"pred_0\", ## For mapping with an endpoint\\n    is_null=True ## For mapping with None\\n)\\nParameters\\nstep_output_name (str) - the specific output in the step\\nendpoint_output_name (str, optional) -- Name of the endpoint output to which the output is mapped.\\nis_null (True, optional) -- If specified, the output is not exposed as a deployment output.\\nReturn\\nAn OutputSource object who can be used in the deployment creation.\\n\\nAdditional parameter for destination definition\\nBy default, the destination is configured as an endpoint parameter, but can configure a different source for your mapping. For each source, you have to add 1 parameter to :\\n\\nDefine the type of destination with the name of parameter added\\nPrecise element about the destination\\nWe will list all parameters you can have in your output mapping.\\n\\nWarning\\n\\nYou have to add just 1 parameter of destination definition. If a parameter destination is missing, the function will generate an error (as opposed to input mapping).\\n\\nEndpoint destination\\n\\nParameter name : endpoint_output_name\\n\\nDestination to : Outside through the endpoint\\n\\nValue to put in parameter : name of the output received by the endpoint in the body of the HTTP call\\n\\nExample :\\n\\nprediction_deployment_ouput = OutputDestination(\\n    step_output_name=\"prediction\",\\n    endpoint_output_name=\"beautiful_prediction\",\\n)\\nData store destination\\n\\nParameter name : datastore_path\\n\\nDestination to : write a file into the data store\\n\\nValue to put in parameter : path to a data store folder\\n\\nExample :\\n\\nprediction_deployment_ouput = OutputDestination(\\n    step_output_name=\"history_prediction\",\\n    datastore_path=\"path/to/history/folder.csv\",\\n)\\nExample step code to send file :\\n\\ndef stepFileIO () :\\n    text_file = open(\\'history_prediction.txt\\', \\'wb\\')  # Open the file in binary mode\\n    text_file.write(\"Result of step send in file output :) \".encode(\\'utf-8\\'))  # Encode the string to bytes\\n    text_file.close()\\n    fileOjb = {\"history_prediction\" : {\"path\": \"history_prediction.txt\"}}\\n\\n   return fileOjb \\nDynamic path :\\n\\nYou can also specify a dynamic path for the file to be uploaded by using one of the following patterns in your datastore path:\\n\\n{execution_id}: The execution id of the deployment.\\n{date}: The date of the execution in truncated ISO 8601 (YYYYMMDD) format.\\n{date_time}: The date of the execution in ISO 8601 (YYYYM-MDD_hhmmss) format.\\nExample with a dynamic path :\\n\\nprediction_deployment_ouput = OutputDestination(\\n    step_output_name=\"history_prediction\",\\n    datastore_path=\"path/to/history/exec_{execution_id}.csv\",\\n)\\nVoid destination\\n\\nParameter name : no_destination\\n\\nDestination to : void\\n\\nValue to put in parameter : True\\n\\nExample :\\n\\nprediction_deployment_ouput = OutputDestination(\\n    step_output_name=\"history_prediction\",\\n    is_null=True,\\n)\\n4. Generate new endpoint token\\nIf you need to alter the endpoint token for an endpoint, you can generate a new one with the following SDK function.\\n\\nsdk.generate_new_endpoint_token(endpoint_name=\"*your-endpoint-name*\")\\nWarning\\n\\nThis will permanently deactivate the previous token.\\n\\n\\nChoose an execution rule\\nA deployment is a way to run a Machine Learning pipeline in a repeatable and automated way.\\n\\nFor each deployment, you can configure an execution rule:\\n\\nby endpoint (web API) : the pipeline will be executed by a call to a web API. In addition, this API will allow, if necessary, to retrieve data as input and deliver the result of the pipeline as output. Access to the API can be securely communicated to external users.\\nby periodic trigger (CRON) : rules can be configured to trigger the pipeline periodically.\\nSummary\\nDeploy with execution rule: Endpoint\\nDeploy with execution rule: Periodic\\nFunction name\\tMethod\\tReturn type\\tDescription\\ncreate_deployment\\tcreate_deployment(deployment_name, pipeline_name, execution_rule, mode, outputs_mapping=[], inputs_mapping=[], descriptionenable_parallel_executions=None, max_parallel_executions_per_pod=None)\\tDict\\tFunction that deploys a pipeline by creating a deployment which allows a user to trigger the pipeline execution\\nDeploy with execution rule: Endpoint\\nDefinition function\\nTo create an auto-mapping deployment where all inputs and outputs are based on API calls, you can use the create_deployment function. To create a deployment with manual mapping, you can use the create_deployment function with the additional parameters inputs_mapping to specify the precise mapping between input and source.\\n\\nCraftAiSdk.create_deployment(\\n    pipeline_name, \\n    deployment_name, \\n    execution_rule=\"endpoint\",\\n    mode=DEPLOYMENT_MODES.ELASTIC,\\n    inputs_mapping=None,\\n    outputs_mapping=None, \\n    description=None,\\n    enable_parallel_executions=None,\\n    max_parallel_executions_per_pod=None\\n    )\\nParameters\\ndeployment_name (str) -- Name of endpoint chosen by the user to refer to the endpoint\\npipeline_name (str) -- Name of pipeline that will be run by the deployment / endpoint\\nexecution_rule (str) - Execution rule of the deployment. Must be endpoint or periodic. For convenience, members of the enumeration DEPLOYMENT_EXECUTION_RULES could be used too.\\nmode (str) â€“ Mode of the deployment. Can be \"elastic\" or \"low_latency\". Defaults to \"elastic\". For convenience, members of the enumeration DEPLOYMENT_MODES can be used. This defines how computing resources are allocated for pipeline executions:\\n\\nelastic: Each pipeline execution runs in a new isolated container (â€œpodâ€), with its own memory (RAM, VRAM, disk). No variables or files are shared between executions, and the pod is destroyed when the execution ends. This mode is simple to use because it automatically uses computing resources for running executions, and each execution starts from an identical blank state. However, it takes time to create a new pod at the beginning of each execution (tens of seconds), and computing resources can become saturated when there are many executions.\\n\\nlow_latency: All pipeline executions for the same deployment run in a shared container (â€œpodâ€) with shared memory. The pod is created when the deployment is created, and deleted when the deployment is deleted. Shared memory means that if one execution modifies a global variable or a file, subsequent executions on the same pod will see the modified value. This mode allows executions to respond quickly (less than 0.5 seconds of overhead) because the pod is already up and running when an execution starts, and it is possible to preload or cache data. However, it requires care in the code because of possible interactions between executions. Additionally, computing resources must be managed carefully, as pods use resources continuously even when there is no ongoing execution, and the number of pods does not automatically adapt to the number of executions. During the lifetime of a deployment, a pod may be re-created by the platform for technical reasons (including if it tries to use more memory than available). This mode is not compatible with steps created with a container_config.dockerfile_path property in create_step().\\n\\ndescription (str, optional) -- Text description of usage of pipeline for user only\\n\\noutputs_mapping (List) - List of all OutputDestination objects with information for each output mapping.\\ninputs_mapping (List, optional) - List of input mappings, to map pipeline inputs to different sources (such as constant values, endpoint inputs, data store or environment variables). See InputSource for more details. For endpoint rules, if an input of the step in the pipeline is not explicitly mapped, it will be automatically mapped to an endpoint input with the same name.\\ndescription (str, optional) â€“ Description of the deployment.\\nenable_parallel_executions (bool, optional) â€“ Whether to run several executions at the same time in the same pod, if mode is \"low_latency\". Not applicable if mode is \"elastic\", where each execution always runs in a new pod. This is disabled by default, which means that for a deployment with \"low_latency\" mode, by default only one execution runs at a time on a pod, and other executions are pending while waiting for the running one to finish. Enabling this may be useful for inference batching on a model that takes much memory, so the model is loaded in memory only once and can be used for several inferences at the same time. If this is enabled, then global variables, GPU memory, and disk files are shared between multiple executions, so you must be mindful of potential race conditions and concurrency issues. For each execution running on a pod, the main Python function is run either as an asyncio coroutine with await if the function was defined with async def (recommended), or in a new thread if the function was defined simply with def. Environment variables are updated whenever a new execution starts on the pod. Using some libraries with async/threaded methods in your code may cause logs to be associated with the wrong running execution (logs are associated with executions through Python contextvars).\\nmax_parallel_executions_per_pod (int, optional) â€“ Only applies if enable_parallel_executions is True. The maximum number of executions that can run at the same time on a deploymentâ€™s pod in \"low_latency\" mode where enable_parallel_executions is True: if a greater number of executions are requested at the same time, then only max_parallel_executions_per_pod executions will actually be running on the pod, and the other ones will be pending until a running execution finishes. The default is 6.\\ntimeout_s (int) â€“ Maximum time (in seconds) to wait for the deployment to be ready. 3 minutes (180 seconds) by default, and at least 2 minutes (120 seconds).\\nReturns\\nInformation about the deployment just create in a dict Python format. In this data, you will have :\\n\\nname - Name of the deployment.\\nendpoint_token - Token of the endpoint used to trigger the deployment. Note that this token is only returned if execution_rule is â€œendpointâ€.\\nExample\\nExample auto mapping\\nsdk.create_deployment(\\n   deployment_name=\"my_deployment\",\\n   pipeline_name=\"my_pipeline\",\\n    execution_rule=\"endpoint\",\\n   outputs_mapping=[],\\n   inputs_mapping=[],\\n)\\n\\n> {\\n>   \\'name\\': \\'name-endpoint\\', \\n>   \\'endpoint_token\\': \\'S_xZOKU ... KHs\\'\\n> }\\nExample manual mapping\\nsdk.create_deployment(\\n   deployment_name=\"my_deployment\",\\n   pipeline_name=\"my_pipeline\",\\n    execution_rule=\"endpoint\",\\n    inputs_mapping=[\\n                seagull_endpoint_input,\\n                big_whale_input,\\n                salt_constant_input,\\n        ],\\n   outputs_mapping=[prediction_endpoint_ouput],\\n)\\n\\n\\n> {\\n>   \\'name\\': \\'name-endpoint\\', \\n>   \\'endpoint_token\\': \\'S_xZOkCI ... FIg\\'\\n> }\\nDeploy with execution rule: Periodic\\nDefinition function\\nTo create an auto-mapping deployment where all inputs and outputs are based on periodicity, you can use the create_deployment function. To create a deployment with manual mapping, you can use the create_deployment function with the additional parameters inputs_mapping to specify the precise mapping between input and source.\\n\\nCraftAiSdk.create_deployment(\\n    pipeline_name, \\n    deployment_name, \\n    execution_rule=\"periodic\",\\n    mode=DEPLOYMENT_MODES.ELASTIC,\\n    schedule=None, \\n    inputs_mapping=None,\\n    outputs_mapping=None, \\n    description=None\\n    )\\nWarning\\n\\nInput and output mapping must always be precise. Auto mapping isn\\'t available for periodic deployment.\\n\\nParameters\\ndeployment_name (str) -- Name of the deployment chosen\\n\\npipeline_name (str) -- Name of pipeline that will be run by the deployment\\n\\ndescription (str, optional) -- Text description of usage of pipeline for user only.\\n\\nexecution_rule (str) - Execution rule of the deployment. Must be endpoint or periodic. For convenience, members of the enumeration DEPLOYMENT_EXECUTION_RULES could be used too.\\n\\nmode (str) â€“ Mode of the deployment. Can be \"elastic\" or \"low_latency\". Defaults to \"elastic\". For convenience, members of the enumeration DEPLOYMENT_MODES can be used. This defines how computing resources are allocated for pipeline executions:\\n\\nelastic: Each pipeline execution runs in a new isolated container (â€œpodâ€), with its own memory (RAM, VRAM, disk). No variables or files are shared between executions, and the pod is destroyed when the execution ends. This mode is simple to use because it automatically uses computing resources for running executions, and each execution starts from an identical blank state. However, it takes time to create a new pod at the beginning of each execution (tens of seconds), and computing resources can become saturated when there are many executions.\\n\\nlow_latency: All pipeline executions for the same deployment run in a shared container (â€œpodâ€) with shared memory. The pod is created when the deployment is created, and deleted when the deployment is deleted. Shared memory means that if one execution modifies a global variable or a file, subsequent executions on the same pod will see the modified value. This mode allows executions to respond quickly (less than 0.5 seconds of overhead) because the pod is already up and running when an execution starts, and it is possible to preload or cache data. However, it requires care in the code because of possible interactions between executions. Additionally, computing resources must be managed carefully, as pods use resources continuously even when there is no ongoing execution, and the number of pods does not automatically adapt to the number of executions. During the lifetime of a deployment, a pod may be re-created by the platform for technical reasons (including if it tries to use more memory than available). This mode is not compatible with steps created with a container_config.dockerfile_path property in create_step().\\n\\nschedule (str, optional) - Schedule of the deployment. Only required if execution_rule is \"periodic\". Must be a valid: cron expression. The deployment will be executed periodically according to this schedule. The schedule must follow this format: <minute> <hour> <day of month> <month> <day of week>. Note that the schedule is in UTC time zone. \"*\" means all possible values. Here are some examples:\\n\\n\"0 0 * * *\" will execute the deployment every day at midnight.\\n\"0 0 5 * *\" will execute the deployment every 5th day of the month at midnight.\\ninputs_mapping (List of instances of [InputSource], optional) - List of input mappings, to map pipeline inputs to different : sources (such as constant values, endpoint inputs, or environment variables). See InputSource for more details. For endpoint rules, if an input of the step in the pipeline is not explicitly mapped, it will be automatically mapped to an endpoint input with the same name. For periodic rules, all inputs of the step in the pipeline must be explicitly mapped.\\n\\noutputs_mapping (List of instances of [OutputDestination], optional) - List of output mappings, to map pipeline outputs to different :\\ndestinations. See OutputDestination for more details. For endpoint execution rules, if an output of the step in the pipeline is not explicitly mapped, it will be automatically mapped to an endpoint input with the same name. For other rules, all outputs of the step in the pipeline must be explicitly mapped.\\n\\ndescription (str, optional) â€“ Description of the deployment.\\n\\nenable_parallel_executions (bool, optional) â€“ Whether to run several executions at the same time in the same pod, if mode is \"low_latency\". Not applicable if mode is \"elastic\", where each execution always runs in a new pod. This is disabled by default, which means that for a deployment with \"low_latency\" mode, by default only one execution runs at a time on a pod, and other executions are pending while waiting for the running one to finish. Enabling this may be useful for inference batching on a model that takes much memory, so the model is loaded in memory only once and can be used for several inferences at the same time. If this is enabled, then global variables, GPU memory, and disk files are shared between multiple executions, so you must be mindful of potential race conditions and concurrency issues. For each execution running on a pod, the main Python function is run either as an asyncio coroutine with await if the function was defined with async def (recommended), or in a new thread if the function was defined simply with def. Environment variables are updated whenever a new execution starts on the pod. Using some libraries with async/threaded methods in your code may cause logs to be associated with the wrong running execution (logs are associated with executions through Python contextvars).\\nmax_parallel_executions_per_pod (int, optional) â€“ Only applies if enable_parallel_executions is True. The maximum number of executions that can run at the same time on a deploymentâ€™s pod in \"low_latency\" mode where enable_parallel_executions is True: if a greater number of executions are requested at the same time, then only max_parallel_executions_per_pod executions will actually be running on the pod, and the other ones will be pending until a running execution finishes. The default is 6.\\ntimeout_s (int) â€“ Maximum time (in seconds) to wait for the deployment to be ready. 3 minutes (180 seconds) by default, and at least 2 minutes (120 seconds).\\nReturns\\nInformation about the deployment just create in a dict Python format.\\n\\nname - Name of the deployment.\\nschedule - Schedule of the deployment. Note that this schedule is only returned if execution_rule is â€œperiodicâ€.\\nhuman_readable_schedule - Human readable schedule of the deployment. Note that this schedule is only returned if execution_rule is â€œperiodicâ€.\\nExample\\nSet up deployment to be triggered automatically every 14 days.\\n\\nsdk.create_deployment(\\n   deployment_name=\"my_deployment\",\\n   pipeline_name=\"my_pipeline\",\\n   execution_rule=\"periodic\",\\n   schedule=\"0 14 * * *\"\\n)\\n\\n\\n> {\\n>   \\'name\\': \\'produit-endpoint-periodic\\', \\n>   \\'schedule\\': \\'*/2 * * * *\\', \\n>   \\'human_readable_schedule\\': \\'Every 2 minutes\\'\\n> }\\n\\n\\nExecute a pipeline\\nAn execution of a pipeline creates an execution on the platform. Each execution is associated with a pipeline with the definition of the values of its inputs and outputs. The execution triggers the execution of the pipeline on one or more Kubernetes containers using the computational resources available on the environment. All the results and artifacts of the execution can be retrieved in the Execution Tracking tab.\\n\\nThere are two ways to execute a pipeline:\\n\\nby creating a deployment: the execution will then depend on the selected execution rule and will be performed when the execution condition is met (call for an endpoint, periodicity for a CRON, etc...)\\nby running it instantly with the sdk: It is then necessary to indicate the values for each input of the pipeline.\\nSummary\\nRun a pipeline\\nTrigger a deployment with execution rule by endpoint with SDK Craft AI\\nTrigger a deployment with execution rule by endpoint with request\\nGet result of a past execution\\nFunction name\\tMethod\\tReturn type\\tDescription\\nrun_pipeline\\trun_pipeline(pipeline_name, inputs=None, inputs_mapping=None, outputs_mapping=None)\\tdict\\tExecutes the pipeline on the platform.\\nretrieve_endpoint_results\\tretrieve_endpoint_results(endpoint_name, execution_id, endpoint_token)\\tdict\\tGet result of endpoint execution.\\nRun a pipeline\\nA run is an execution of a pipeline on the platform. SDK function that runs a pipeline to create an execution.\\n\\nrun_pipeline(pipeline_name, inputs=None, inputs_mapping=None, outputs_mapping=None)\\nParameters\\n\\npipeline_name (str) -- Name of an existing pipeline.\\ninputs (dict, optional) - Dictionary of inputs to pass to the pipeline with input names as dict keys and corresponding values as dict values. For files, the value should be the path to the file or a file content as an instance of io.IOBase. Defaults to None.\\ninputs_mapping (list of instances of [InputSource]{.title-ref}) - List of input mappings, to map pipeline inputs to different sources (such as environment variables). See [InputSource]{.title-ref} for more details.\\noutputs_mapping (list of instances of [OutputDestination]{.title-ref}) - List of output mappings, to map pipeline outputs to different destinations (such as datastore). See [OutputDestination]{.title-ref} for more details.\\nReturns\\n\\nCreated pipeline execution represented as dict with execution_id and outputs as keys. The output values will be in the output object represented as dict with output_names as keys and corresponding values as values.\\n\\nExample of return object :\\n\\n{\\n  \"execution_id\": \"my-pipeline-8iud6\",\\n  \"outputs\": {\\n      \"output_number\": 0.117,\\n      \"output_text\": \"This is working fine\",\\n   }\\n}\\nTrigger a deployment with execution rule by endpoint with SDK Craft AI\\nSDK function that triggers the deployment of our pipeline.\\n\\nsdk.trigger_endpoint(endpoint_name, endpoint_token, inputs={},\\nwait_for_results=True)\\nParameters\\n\\nendpoint_name (str) -- Name of the endpoint.\\nendpoint_token (str) -- Token to access endpoint.\\ninputs (dict) - Inputs value for endpoint call.\\nwait_for_results (bool, optional) -- Automatically call retrieve_endpoint_results (True by default)\\nReturns\\n\\nCreated pipeline execution represented as dict.\\n\\nTrigger a deployment with execution rule by endpoint with request\\nFor trigger a deployment who is set up with an endpoint, you can also send request with your element defined in the pipeline input.\\n\\nExamples in Python for variable :\\n\\nimport requests\\n\\nr = requests.post(\\n    \"https://your_environment_url/my_endpoint\",\\n    json={\\n        \"input1\": \"value1\",\\n        \"input2\": [1,2,3]\\n        \"input3\": False\\n    },\\n    headers={\"Authorization\": \"EndpointToken \" + ENDPOINT_TOKEN }\\n)\\nExamples in Python for file (not available with auto mapping) :\\n\\nimport requests\\n\\nr = requests.post(\\n    \"https://your_environment_url/my_multistep_endpoint\",\\n    files={\"data\": open(\"my_file.txt\", \"rb\")},\\n    headers={\"Authorization\": \"EndpointToken \" + ENDPOINT_TOKEN }\\n)\\nNote\\n\\nWe have explained in this documentation how to trigger the endpoint with\\n\\nPython, but you can obviously send a request from any tool (curl, postman, JavaScript, ...).\\n\\nWarning\\n\\nInputs and outputs have size limits. This limit is 0.06MB for cumulative inputs and also 0.06MB for cumulative outputs. This input/output size limit is available for all trigger/deployment types (run, endpoint or CRON). This limit applies regardless of the source or destination of the input/output.\\n\\nOnly file inputs/outputs are not affected by this limit. We recommend that you use this method when transferring large amounts of data.\\n\\nGet result of a past execution\\nGet the results of an endpoint execution.\\n\\nCraftAiSdk.retrieve_endpoint_results(endpoint_name, execution_id, endpoint_token)\\nParameters\\n\\nendpoint_name (str) - Name of the endpoint.\\nexecution_id (str) - Name of the execution returned by trigger_endpoint.\\nendpoint_token (str) - Token to access endpoint.\\nReturns\\n\\nCreated pipeline execution represented as dict with the following keys:\\n\\noutputs (dict): Dictionary of outputs of the pipeline with output names as keys and corresponding values as values.\\nGet all execution of a pipeline\\nGet a list of executions for the given pipeline\\n\\nCraftAiSdk.list_pipeline_executions(pipeline_name)\\nParameters\\n\\npipeline_name (str) - Name of an existing pipeline.\\nReturns\\n\\nA list of information on the pipeline execution represented as dict with the following keys:\\n\\nexecution_id (str): Name of the pipeline execution.\\nstatus (str): Status of the pipeline execution.\\ncreated_at (str): Date of creation of the pipeline execution.\\ncreated_by (str): ID of the user who created the pipeline execution. In the case of a pipeline run, this is the user who triggered the run. In the case of an execution via a deployment, this is the user who created the deployment.\\nend_date (str): Date of completion of the pipeline execution.\\npipeline_name (str): Name of the pipeline used for the execution.\\ndeployment_name (str): Name of the deployment used for the execution.\\nsteps (list of obj): List of the step executions represented as dict with the following keys:\\nname (str): Name of the step.\\nstatus (str): Status of the step.\\nstart_date (str): Date of start of the step execution.\\nend_date (str): Date of completion of the step execution.\\ncommit_id (str): Id of the commit used to build the step.\\nrepository_url (str): Url of the repository used to build the step.\\nrepository_branch (str): Branch of the repository used to build the step.\\nrequirements_path (str): Path of the requirements.txt file.\\norigin (str): The origin of the step, can be git_repository or local.\\n\\n\\nFollow the deployment executions\\nA deployment - is a way to run a Machine Learning pipeline in a repeatable and automated way. Once it is created, you can find the setup of your deployments in the pipeline store. In addition, you can find all the information about the executions of the deployments in the execution tracking.\\n\\nWarning\\n\\nDates provided by the Web UI and SDK are always expressed in Coordinated Universal Time (UTC).\\n\\nSummary\\nGet information about a deployment\\nDelete a deployment or an execution\\nFollow the execution tracking\\nFunction name\\tMethod\\tReturn type\\tDescription\\nlist_deployments\\tlist_deployments()\\tlist of dict\\tGet the list of all deployments.\\nget_deployment\\tget_deployment(deployment_name)\\tdict\\tGet information of a deployment.\\ndelete_deployment\\tdelete_deployment(deployment_name)\\tdict\\tDelete a deployment identified by its name.\\nget_pipeline_execution\\tget_pipeline_execution(execution_id)\\tdict\\tGet the status of one pipeline execution identified by its name.\\ndelete_pipeline_execution\\tdelete_pipeline_execution(execution_id)\\tdict\\tDelete pipeline execution.\\nget_pipeline_execution_input\\tget_pipeline_execution_input(execution_id, input_name)\\tdict\\tGet information about an input of an execution.\\nget_pipeline_execution_output\\tget_pipeline_execution_output(execution_id, output_name)\\tdict\\tGet information about an output of an execution.\\nGet information about a deployment\\nList of deployments\\nGet the list of all deployments.\\n\\nCraftAiSdk.list_deployments()\\nReturns\\n\\nList of deployments represented as dict with the following keys:\\n\\nname (str): Name of the deployment.\\npipeline_name (str): Name of the pipeline associated to the deployment.\\nexecution_rule (str): Execution rule of the deployment. Can be endpoint or periodic.\\nis_enabled (bool): Whether the deployment is enabled.\\ncreated_at (str): Date of creation of the deployment.\\nGet deployment information\\nGet information of a deployment.\\n\\nCraftAiSdk.get_deployment(deployment_name)\\nParameters\\n\\ndeployment_name (str) -- Name of the deployment.\\nReturns\\n\\nDeployment information represented as dict with the following keys:\\n\\nname (str): Name of the deployment.\\nmode (str): The deployment mode. Can be \"elastic\" or \"low_latency\".\\npipeline (dict): Pipeline associated with the deployment represented as dict with the following keys:\\nname (str): Name of the pipeline.\\ninputs_mapping (list of dict): List of inputs mapping represented as dict with the following keys:\\nstep_input_name (str): Name of the step input.\\ndata_type (str): Data type of the step input.\\ndescription (str): Description of the step input.\\nconstant_value (str): Constant value of the step input. Note that this key is only returned if the step input is mapped to a constant value.\\nenvironment_variable_name (str): Name of the environment variable. Note that this key is only returned if the step input is mapped to an environment variable.\\nendpoint_input_name (str): Name of the endpoint input. Note that this key is only returned if the step input is mapped to an endpoint input.\\nis_null (bool): Whether the step input is mapped to null. Note that this key is only returned if the step input is mapped to null.\\ndatastore_path (str): Datastore path of the step input. Note that this key is only returned if the step input is mapped to the datastore.\\nis_required (bool): Whether the step input is required. Note that this key is only returned if the step input is required.\\ndefault_value (str): Default value of the step input. Note that this key is only returned if the step input has a default value.\\noutputs_mapping (list of dict): List of outputs mapping represented as dict with the following keys:\\nstep_output_name (str): Name of the step output.\\ndata_type (str): Data type of the step output.\\ndescription (str): Description of the step output.\\nendpoint_output_name (str): Name of the endpoint output. Note that this key is only returned if the step output is mapped to an endpoint output.\\nis_null (bool): Whether the step output is mapped to null. Note that this key is only returned if the step output is mapped to null.\\ndatastore_path (str): Datastore path of the step output. Note that this key is only returned if the step output is mapped to the datastore.\\nendpoint_token (str): Token of the endpoint. Note that this key is only returned if the deployment is an endpoint.\\nschedule (str): Schedule of the deployment. Note that this key is only returned if the execution rule of the deployment is \"periodic\".\\nhuman_readable_schedule (str): Human readable schedule of the deployment. Note that this key is only returned if the execution rule of the deployment is \"periodic\".\\ncreated_at (str): Date of creation of the deployment.\\ncreated_by (str): ID of the user who created the deployment.\\nupdated_at (str): Date of last update of the deployment.\\nupdated_by (str): ID of the user who last updated the deployment.\\nlast_execution_id (str): ID of the last execution of the deployment.\\nis_enabled (bool): Whether the deployment is enabled.\\ndescription (str): Description of the deployment.\\nexecution_rule (str): Execution rule of the deployment.\\nstatus (str): The deployment status. Can be \"pending\", \"up\", \"failed\" or \"standby\".\\npods (list of dict): List of pods associated with the low latency deployment. Note that this key is only returned if the deployment is in low latency mode. Each pod is represented as dict with the following keys:\\npod_id (str): ID of the pod.\\nstatus (str): Status of the pod.\\nReturn type\\n\\ndict\\n\\nDelete a deployment or an execution\\nDelete a deployment\\nDelete a deployment identified by its name.\\n\\nCraftAiSdk.delete_deployment(deployment_name)\\nParameters\\n\\ndeployment_name (str) - Name of the deployment.\\nReturns\\n\\nDeleted deployment represented as dict (with keys name, execution_rule). The return data type is dict.\\n\\nWarning\\n\\nBe careful, deleting a deployment will delete all its executions.\\n\\nDelete an execution\\nDelete one pipeline execution identified by its execution_id.\\n\\nCraftAiSdk.delete_pipeline_execution(execution_id)\\nParameters\\n\\nexecution_id (str) - Name of the pipeline execution.\\nReturns\\n\\nDeleted pipeline execution represented as dict with the following keys:\\n\\nexecution_id (str): Name of the pipeline execution deleted.\\nFollow the execution tracking\\nGet execution list\\nGet the status of one pipeline execution identified by its name.\\n\\nCraftAiSdk.get_pipeline_execution(execution_id)\\nParameters\\n\\nexecution_id (str) - ID of the pipeline execution.\\nReturns\\n\\nInformation on the pipeline execution with id execution_id represented as dict.\\n\\nexecution_id (str): Name of the pipeline execution.\\nstatus (str): Status of the pipeline execution.\\ncreated_at (str): Date of creation of the pipeline\\ncreated_by (str): ID of the user who created the pipeline execution. In the case of a pipeline run, this is the user who triggered the run. In the case of an execution via a deployment, this is the user who created the deployment.\\nend_date (str): Date of completion of the pipeline execution.\\npipeline_name (str): Name of the pipeline used for the execution.\\ndeployment_name (str): Name of the deployment used for the execution.\\nsteps (list of obj): List of the step executions represented as dict with the following keys:\\nname (str): Name of the step.\\nstatus (str): Status of the step.\\nstart_date (str): Date of start of the step execution.\\nend_date (str): Date of completion of the step execution.\\ncommit_id (str): Id of the commit used to build the step.\\nrepository_url (str): Url of the repository used to build the step.\\nrepository_branch (str): Branch of the repository used to build the step.\\nrequirements_path (str): Path of the requirements.txt file.\\norigin (str): The origin of the step, can be git_repository or local.\\ninputs (list of dict): List of inputs represented as a dict with the following keys:\\nstep_input_name (str): Name of the input.\\n`data_type (str): Data type of the input.\\nsource (str): Source of type of the input. Can be environment_variable, datastore, constant, is_null endpoint or run.\\nendpoint_input_name (str): Name of the input in the endpoint execution if source is endpoint.\\nconstant_value (str): Value of the constant if source is constant.\\nenvironment_variable_name (str): Name of the environment variable if source is environment_variable.\\nis_null (bool): True if source is is_null.\\nvalue: Value of the input.\\noutputs (list of dict): List of outputs represented as a dict with the following keys:\\nstep_output_name (str): Name of the output.\\n`data_type (str): Data type of the output.\\ndestination (str): Destination of type of the output. Can be datastore, is_null endpoint or run.\\nendpoint_output_name (str): Name of the output in the endpoint execution if destination is endpoint.\\nis_null (bool): True if destination is is_null.\\nvalue: Value of the output.\\nGet execution logs\\nGet the logs of an executed pipeline identified by its name.\\n\\nCraftAiSdk.CraftAiSdk.get_pipeline_execution_logs(*pipeline_name*, *execution_id*,\\nfrom_datetime=None, to_datetime=None, limit=None)\\nParameters\\n\\npipeline_name (str) -- Name of an existing pipeline.\\nexecution_id (str) -- ID of the pipeline execution.\\nfrom_datetime (datetime.time, optional) -- Datetime from which the logs are collected.\\nto_datetime (datetime.time, optional) -- Datetime until which the logs are collected.\\nlimit (int, optional) -- Maximum number of logs that are collected.\\nReturns\\n\\nList of collected logs represented as dict (with keys message, timestamp and stream). The return type is a list.\\n\\nGet Input and Output of an execution\\nInput\\nGet the input value of an executed pipeline identified by its execution_id.\\n\\nCraftAiSdk.get_pipeline_execution_input(execution_id, input_name)\\nParameters\\n\\nexecution_id (str) - ID of the pipeline execution.\\ninput_name (str) - Name of the input.\\nReturns\\n\\nInformation on the input represented as a dict with the following keys :\\n\\nstep_input_name (str): Name of the input.\\ndata_type (str): Data type of the input.\\nsource (str): Source of type of the input. Can be environment_variable, datastore, constant, is_null endpoint or run.\\nendpoint_input_name (str): Name of the input in the endpoint execution if source is endpoint.\\nconstant_value (str): Value of the constant if source is constant.\\nenvironment_variable_name (str): Name of the environment variable if source is environment_variable.\\nis_null (bool): True if source is is_null.\\nvalue: Value of the input.\\nOutput\\nGet the output value of an executed pipeline identified by its execution_id.\\n\\nCraftAiSdk.get_pipeline_execution_output(execution_id, output_name)\\nParameters\\n\\nexecution_id (str) - ID of the pipeline execution.\\noutput_name (str) - Name of the output.\\nReturns\\n\\nInformation on the output represented as a dict with the following keys :\\n\\nstep_output_name (str): Name of the output.\\ndata_type (str): Data type of the output.\\ndestination (str): Destination of type of the output. Can bedatastore,is_nullendpointorrun`.\\nendpoint_output_name (str): Name of the output in the endpoint ex- ecution if destination is endpoint.\\nis_null (bool): True if destination is is_null.\\nvalue: Value of the output.\\n\\nMetrics\\nIn the context of MLOps, tracking and monitoring metrics is critical for assessing the performance and progress of machine learning pipelines. The CraftAiSdk platform provides a comprehensive set of features for defining and recording metrics at each pipeline execution.\\n\\nWith measurement capabilities, you can efficiently track and retrieve the metrics associated with each execution in your machine learning pipelines. This enables you to valuable insights and make informed decisions about your models and deployments.\\n\\nPipeline Metrics\\nThe record_metric_value function allows you to create or update a pipeline metric within a step code. This function allows you to store the name and corresponding value of a particular metric.\\n\\nYou do not need to declare anything outside of the step, you can just use record_metrics_value() in your step code. Remember, if you want to use the SDK in your step code, you don\\'t need to specify your environment URL or token in the builder parameters.\\n\\nAfter the execution is finished, you can find all your metric values in the web interface on the Execution page and on the Metrics tab.\\n\\nUpload Metrics\\nCurrently, pipeline metrics can only have one numeric value and one name for each execution metric. If multiple metrics are entered with identical names, only the last metric will be retained.\\n\\nWarning\\n\\nThis function can only be used in the source code of the step running on the platform. When used outside a code step, it doesn\\'t send metrics and displays a warning message.\\n\\nCraftAiSdk.record_metric_value(name, value)\\nParameters\\n\\nname (str) - The name of the metric to store.\\nvalue (float) - The value of the metric to store.\\nReturns\\n\\nTrue if sent, False otherwise\\n\\nExample\\n\\nHere is a very simple example of step code that sends only 2 different metrics.\\n\\nNote\\n\\nDon\\'t forget to import the craft-ai-sdk package in the step code and to list the library in your requirement.txt to install it on the step execution context.\\n\\nfrom craft_ai_sdk import CraftAiSdk\\n\\ndef metricsStep () :\\n\\n    sdk = CraftAiSdk()\\n\\n    # Some code \\n\\n    sdk.record_metric_value(\"accuracy\", 0.1409)\\n    sdk.record_metric_value(\"loss\", 1/3)\\n\\n    print (\"Metrics are sent\")\\nGet metrics\\nThe get_metrics function retrieves a list of pipeline metrics. You can filter the metrics based on the name, pipeline name, deployment name, or execution ID. It\\'s important to note that only one of the parameters (name, pipeline_name, deployment_name, execution_id) can be set at a time.\\n\\nCraftAiSdk.get_metrics(name=None, pipeline_name=None, deployment_name=None, execution_id=None)\\nParameters\\n\\nname (str, optional) - The name of the metric to retrieve.\\npipeline_name (str, optional) - Filter metrics by pipeline. If not specified, all pipelines will be considered.\\ndeployment_name (str, optional) - Filter metrics by deployment. If not specified, all deployments will be considered.\\nexecution_id (str, optional) - Filter metrics by execution. If not specified, all executions will be considered.\\nReturns\\n\\nThe function returns a list of execution metrics as dictionaries. Each metric entry contains the following keys: name, value, created_at, execution_id, deployment_name, pipeline_name.\\n\\nList Metrics\\nThe Craft AI platform provides robust features for defining and recording list metrics during pipeline execution. This functionality allows you to store the name and corresponding list of values for a specific metric.\\n\\nTo create or update a list metric within a step code, you can utilize the record_list_metric_values() function. Afterwards, you can retrieve your metrics outside the step using the get_list_metrics() function. Additionally, you can access all your metric values in the web interface via the Metrics tab on the Execution page.\\n\\nSimilar to pipeline metrics, list metrics can only consist of a list of numbers (integer or float).\\n\\nUpload list metrics\\nThe record_list_metric_values() function enables you to add values to a metric list by specifying the name of the metric list and the corresponding values. There is no need to declare anything outside of the step; simply use record_list_metric_values() in your step code, as you would for pipeline metrics.\\n\\nIt\\'s important to note that when using the record_list_metric_values() function, it can only be utilized within the source code of the step running on the platform. When uploading list metrics, you have the option to either specify a Python list directly or upload values individually, specifying the same metric name (which will automatically accumulate into a list).\\n\\nHere is an example of step code that sends two different lists metrics:\\n\\nWarning\\n\\nThis function can only be used in the source code of the step running on the platform. When used outside of a code step, it doesn\\'t send metrics and displays a warning message.\\n\\nCraftAiSdk.record_list_metric_values(name, values)\\nParameters\\n\\nname (str) - Name of the metric list to add values.\\nvalues (list of float or float) - Values of the metric list to add.\\nReturns\\n\\nThis function returns nothing (None).\\n\\nExample\\n\\nHere is a very simple example of step code that sends only 2 different lists metrics.\\n\\nNote\\n\\nDon\\'t forget to import the craft-ai-sdk package in the step code and to list the library in your requirement.txt to install it on the step execution context.\\n\\nfrom craft_ai_sdk import CraftAiSdk\\nimport math \\n\\ndef metricsStep():\\n    sdk = CraftAiSdk()\\n\\n    # Some code \\n\\n    # Just one list upload\\n    sdk.record_list_metric_values(\"accuracy_list\", [0.89, 0.92, 0.95])\\n\\n    # Tow list upload, the lists will be concatenated in loss_list list metrics \\n    sdk.record_list_metric_values(\"loss_list\", [1.4, 1.2])\\n    sdk.record_list_metric_values(\"loss_list\", [1.1, 1.0])\\n\\n    # Upload multiple values that will be concatenated into 1 metrics list *logx* with all values\\n    for i in range (1, 50) : \\n\\n        sdk.record_list_metric_values(\"logx\", math.log(i))\\n\\n    print(\"List metrics are sent\")\\nWarning\\n\\nA pipeline metrics and a list metrics can have the same name in the same execution. A metrics list is limited to a maximum of 50,000 values per execution.\\n\\nGet list metrics\\nTo retrieve a list of metric lists, you can use the get_list_metrics() function. This function allows you to filter the metric lists based on the name, pipeline name, deployment name, or execution ID.\\n\\nIt\\'s important to note that only one of the parameters (name, pipeline_name, deployment_name, execution_id) can be set at a time.\\n\\nCraftAiSdk.get_list_metrics(name=None, pipeline_name=None, deployment_name=None, execution_id=None)\\nParameters\\n\\nname (str, optional) - Name of the metric list to retrieve.\\npipeline_name (str, optional) - Filter metric lists by pipeline, defaults to all the pipelines.\\ndeployment_name (str, optional) - Filter metric lists by deployment, defaults to all the deployments.\\nexecution_id (str, optional) - Filter metric lists by execution, defaults to all the executions.\\nReturns\\n\\nThe function returns a list of execution metrics as dictionaries. Each metric entry contains the following keys: name, value, created_at, execution_id, deployment_name, pipeline_name.\\n\\nHere is an example of how to use the get_list_metrics() function:\\n\\nlist_metrics = CraftAiSdk.get_list_metrics(name=\"accuracy_list\", pipeline_name=\"my_pipeline\")\\n\\n')]\n"
     ]
    }
   ],
   "source": [
    "# Directory to your pdf files:\n",
    "DATA_PATH = \"/articles/\"\n",
    "def load_documents():\n",
    "  \"\"\"\n",
    "  Load PDF documents from the specified directory using PyPDFDirectoryLoader.\n",
    "  Returns:\n",
    "  List of Document objects: Loaded PDF documents represented as Langchain\n",
    "                                                          Document objects.\n",
    "  \"\"\"\n",
    "  # Initialize PDF loader with specified directory\n",
    "  loader = TextLoader(\"articles/doc_craft.txt\")\n",
    "\n",
    "  return loader.load() \n",
    "\n",
    "documents = load_documents() # Call the function\n",
    "# Inspect the contents of the first document as well as metadata\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(documents: list[Document]):\n",
    "  \"\"\"\n",
    "  Split the text content of the given list of Document objects into smaller chunks.\n",
    "  Args:\n",
    "    documents (list[Document]): List of Document objects containing text content to split.\n",
    "  Returns:\n",
    "    list[Document]: List of Document objects representing the split text chunks.\n",
    "  \"\"\"\n",
    "  # Initialize text splitter with specified parameters\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300, # Size of each chunk in characters\n",
    "    chunk_overlap=100, # Overlap between consecutive chunks\n",
    "    length_function=len, # Function to compute the length of the text\n",
    "    add_start_index=True, # Flag to add start index to each chunk\n",
    "  )\n",
    "\n",
    "  # Split documents into smaller chunks using text splitter\n",
    "  chunks = text_splitter.split_documents(documents)\n",
    "  print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "  # Print example of page content and metadata for a chunk\n",
    "  document = chunks[0]\n",
    "  print(document.page_content)\n",
    "  print(document.metadata)\n",
    "\n",
    "  return chunks # Return the list of split text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_to_chroma(chunks: list[Document]):\n",
    "  \"\"\"\n",
    "  Save the given list of Document objects to a Chroma database.\n",
    "  Args:\n",
    "  chunks (list[Document]): List of Document objects representing text chunks to save.\n",
    "  Returns:\n",
    "  None\n",
    "  \"\"\"\n",
    "\n",
    "  # Clear out the existing database directory if it exists\n",
    "  if os.path.exists(CHROMA_PATH):\n",
    "    shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "  # Create a new Chroma database from the documents using OpenAI embeddings\n",
    "  db = Chroma.from_documents(\n",
    "    chunks,\n",
    "    OpenAIEmbeddings(),\n",
    "    persist_directory=CHROMA_PATH\n",
    "  )\n",
    "\n",
    "  # Persist the database to disk\n",
    "  db.persist()\n",
    "  print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 documents into 1251 chunks.\n",
      "ntroduction\n",
      "You are a Data Scientist struggling with data, code, and models in your projects.\n",
      "\n",
      "You are an ML Engineer who has trouble replicating pipelines and monitoring models in production.\n",
      "\n",
      "You are Head of a Data Science team, who has trouble shipping quickly models to production.\n",
      "{'source': 'articles/doc_craft.txt', 'start_index': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/miniconda3/envs/scw_rag/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1251 chunks to chroma.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/miniconda3/envs/scw_rag/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "def generate_data_store():\n",
    "  \"\"\"\n",
    "  Function to generate vector database in chroma from documents.\n",
    "  \"\"\"\n",
    "  documents = load_documents() # Load documents from a source\n",
    "  chunks = split_text(documents) # Split documents into manageable chunks\n",
    "  save_to_chroma(chunks) # Save the processed data to a data store\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "# Generate the data store\n",
    "generate_data_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    " - -\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def query_rag(query_text):\n",
    "  \"\"\"\n",
    "  Query a Retrieval-Augmented Generation (RAG) system using Chroma database and OpenAI.\n",
    "  Args:\n",
    "    - query_text (str): The text to query the RAG system with.\n",
    "  Returns:\n",
    "    - formatted_response (str): Formatted response including the generated text and sources.\n",
    "    - response_text (str): The generated response text.\n",
    "  \"\"\"\n",
    "  # YOU MUST - Use same embedding function as before\n",
    "  embedding_function = OpenAIEmbeddings()\n",
    "\n",
    "  # Prepare the database\n",
    "  db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "  \n",
    "  # Retrieving the context from the DB using similarity search\n",
    "  results = db.similarity_search_with_relevance_scores(query_text, k=10)\n",
    "\n",
    "  # Check if there are any matching results or if the relevance score is too low\n",
    "  if len(results) == 0 or results[0][1] < 0.7:\n",
    "    print(f\"Unable to find matching results.\")\n",
    "\n",
    "  # Combine context from matching documents\n",
    "  context_text = \"\\n\\n - -\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    " \n",
    "  # Create prompt template using context and query text\n",
    "  prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "  prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "  \n",
    "  # Initialize OpenAI chat model\n",
    "  model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "  # Generate response text based on the prompt\n",
    "  response_text = model.predict(prompt)\n",
    " \n",
    "   # Get sources of the matching documents\n",
    "  sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    " \n",
    "  # Format and return response including generated text and sources\n",
    "  formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "  return formatted_response, response_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/miniconda3/envs/scw_rag/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  warn_deprecated(\n",
      "/home/hugo/miniconda3/envs/scw_rag/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "/home/hugo/miniconda3/envs/scw_rag/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use the `create_pipeline` method, you need to call it with the required parameters: `pipeline_name` and `step_name`. Hereâ€™s an example of how to do it:\n",
      "\n",
      "```python\n",
      "sdk.create_pipeline(\n",
      "    pipeline_name=\"your_pipeline_name\",\n",
      "    step_name=\"your_step_name\"\n",
      ")\n",
      "```\n",
      "\n",
      "Replace `\"your_pipeline_name\"` with the desired name for your pipeline and `\"your_step_name\"` with the name for the step in the pipeline. After executing this function, you will obtain an output that describes the pipeline, its step, and its inputs and outputs.\n"
     ]
    }
   ],
   "source": [
    "query_text = \"How to use create_pipeline ? \"\n",
    "\n",
    "# Let's call our function we have defined\n",
    "formatted_response, response_text = query_rag(query_text)\n",
    "# and finally, inspect our final response!\n",
    "print(response_text)\n",
    "# formatted_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For testing fastaapi server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugo has 10$ on his bank account.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# init client and connect to localhost server\n",
    "client = OpenAI(\n",
    "    api_key=\"fake-api-key\",\n",
    "    base_url=\"http://127.0.0.1:8001\" # change the default port if needed\n",
    ")\n",
    "\n",
    "# call API\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How many $ have Hugo on him bank account ?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-1337-turbo-pro-max\",\n",
    ")\n",
    "\n",
    "# print the top \"choice\" \n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugo has $10 in his bank account.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# init client and connect to localhost server\n",
    "client = OpenAI(\n",
    "    api_key=\"fake-api-key\",\n",
    "    base_url=\"http://127.0.0.1:8000\" # change the default port if needed\n",
    ")\n",
    "\n",
    "# call API\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            # \"content\": \"How to create a pipeline ?\",\n",
    "            \"content\": \"How many $ have Hugo on him bank account ?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-1337-turbo-pro-max\",\n",
    ")\n",
    "\n",
    "# print the top \"choice\" \n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context provided does not contain any information relevant to the message you shared. Therefore, I cannot answer the question based on the given context. "
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"fake-api-key\",\n",
    "    base_url=\"http://localhost:8000\" # change the default port if needed\n",
    ")\n",
    "\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say this is a test, s greater than number of elements in index 6, updating n_results = 6\"}],\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-first-poc-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
